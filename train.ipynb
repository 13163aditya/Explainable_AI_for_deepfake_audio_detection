{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd57ef55f99cab2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:38:54.307683Z",
     "start_time": "2025-04-23T20:32:05.812191Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3600 samples with 76 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9273 - loss: 0.1990 - val_accuracy: 0.9986 - val_loss: 0.0061\n",
      "Epoch 2/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 3/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 4/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.4372e-04 - val_accuracy: 1.0000 - val_loss: 6.0835e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.7483e-04 - val_accuracy: 1.0000 - val_loss: 7.0261e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 4.1706e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.0231e-04 - val_accuracy: 1.0000 - val_loss: 2.9683e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.7543e-05 - val_accuracy: 1.0000 - val_loss: 1.9042e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.2662e-04 - val_accuracy: 1.0000 - val_loss: 1.7113e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.2868e-04 - val_accuracy: 1.0000 - val_loss: 5.5100e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.1137e-04 - val_accuracy: 1.0000 - val_loss: 3.9709e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.3346e-04 - val_accuracy: 1.0000 - val_loss: 3.1076e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.5679e-05 - val_accuracy: 1.0000 - val_loss: 9.3896e-06\n",
      "Epoch 14/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.7461e-05 - val_accuracy: 1.0000 - val_loss: 8.3058e-06\n",
      "Epoch 15/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 8.2356e-05 - val_accuracy: 1.0000 - val_loss: 2.4222e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.7394e-05 - val_accuracy: 1.0000 - val_loss: 2.3616e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.3803e-05 - val_accuracy: 1.0000 - val_loss: 2.5775e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.3505e-05 - val_accuracy: 1.0000 - val_loss: 2.4498e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.0890e-05 - val_accuracy: 1.0000 - val_loss: 1.2158e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.1102e-05 - val_accuracy: 1.0000 - val_loss: 1.0656e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4503e-05 - val_accuracy: 1.0000 - val_loss: 1.5881e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.1999e-06 - val_accuracy: 1.0000 - val_loss: 2.3926e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.4162e-05 - val_accuracy: 1.0000 - val_loss: 1.8157e-05\n",
      "Epoch 24/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 9.4375e-06 - val_accuracy: 1.0000 - val_loss: 1.7399e-05\n",
      "Epoch 25/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.3377e-05 - val_accuracy: 1.0000 - val_loss: 1.7946e-05\n",
      "Epoch 26/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.4254e-05 - val_accuracy: 1.0000 - val_loss: 1.9472e-05\n",
      "Epoch 27/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.2471e-05 - val_accuracy: 1.0000 - val_loss: 2.2525e-05\n",
      "Epoch 28/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.9382e-06 - val_accuracy: 1.0000 - val_loss: 2.1057e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 9.4716e-06 - val_accuracy: 1.0000 - val_loss: 1.6931e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.1662e-05 - val_accuracy: 1.0000 - val_loss: 1.6198e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model and artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration - Update this path\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "SR = 22050\n",
    "N_MFCC = 20\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76  # Must be exactly 76 features\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Feature extractor that returns (1, 76) shaped array\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=N_MFCC,\n",
    "                                   n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # Chroma (2)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                           n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # Spectral Features (6)\n",
    "        features.append(np.mean(librosa.feature.spectral_centroid(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_centroid(y=audio, sr=SR)))\n",
    "        features.append(np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_bandwidth(y=audio, sr=SR)))\n",
    "        features.append(np.mean(librosa.feature.spectral_rolloff(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_rolloff(y=audio, sr=SR)))\n",
    "\n",
    "        # Zero Crossing Rate (2)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # RMS Energy (2)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                                frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # Spectral Contrast (12)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                   n_bands=5,\n",
    "                                                   n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(contrast, axis=1))\n",
    "        features.extend(np.std(contrast, axis=1))\n",
    "\n",
    "        # Tonnetz (12)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Feature mismatch: {len(features)} != {FEATURE_COUNT}\")\n",
    "\n",
    "        return features.reshape(1, -1)  # Ensure 2D array\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset():\n",
    "    X, y = [], []\n",
    "    for label, folder in enumerate([\"real\", \"fake\"]):\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise FileNotFoundError(f\"Missing folder: {folder_path}\")\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                features = extract_features(os.path.join(folder_path, file))\n",
    "                if features is not None:\n",
    "                    X.append(features[0])  # Store as 1D array\n",
    "                    y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Training pipeline\n",
    "X, y = load_dataset()\n",
    "print(f\"Loaded {len(X)} samples with {X.shape[1]} features\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Prepare background data for SHAP\n",
    "background_data = X_scaled[np.random.choice(X_scaled.shape[0], 100, replace=False)]\n",
    "joblib.dump(scaler, \"scaler_nn_0.joblib\")\n",
    "joblib.dump(background_data, \"background_nn_0.joblib\")\n",
    "\n",
    "# Model architecture - MUST have 2 outputs for SHAP\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')  # Critical for SHAP compatibility\n",
    "])\n",
    "model.compile(optimizer=Adam(0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n",
    "model.save(\"audio_model_nn_0.h5\")\n",
    "print(\"Training complete! Model and artifacts saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b815d3c16d7ae032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:41:40.321944Z",
     "start_time": "2025-04-23T22:35:51.497088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 1800 real samples\n",
      "Processed 100/1800 real files\n",
      "Processed 200/1800 real files\n",
      "Processed 300/1800 real files\n",
      "Processed 400/1800 real files\n",
      "Processed 500/1800 real files\n",
      "Processed 600/1800 real files\n",
      "Processed 700/1800 real files\n",
      "Processed 800/1800 real files\n",
      "Processed 900/1800 real files\n",
      "Processed 1000/1800 real files\n",
      "Processed 1100/1800 real files\n",
      "Processed 1200/1800 real files\n",
      "Processed 1300/1800 real files\n",
      "Processed 1400/1800 real files\n",
      "Processed 1500/1800 real files\n",
      "Processed 1600/1800 real files\n",
      "Processed 1700/1800 real files\n",
      "Processed 1800/1800 real files\n",
      "Found 1800 fake samples\n",
      "Processed 100/1800 fake files\n",
      "Processed 200/1800 fake files\n",
      "Processed 300/1800 fake files\n",
      "Processed 400/1800 fake files\n",
      "Processed 500/1800 fake files\n",
      "Processed 600/1800 fake files\n",
      "Processed 700/1800 fake files\n",
      "Processed 800/1800 fake files\n",
      "Processed 900/1800 fake files\n",
      "Processed 1000/1800 fake files\n",
      "Processed 1100/1800 fake files\n",
      "Processed 1200/1800 fake files\n",
      "Processed 1300/1800 fake files\n",
      "Processed 1400/1800 fake files\n",
      "Processed 1500/1800 fake files\n",
      "Processed 1600/1800 fake files\n",
      "Processed 1700/1800 fake files\n",
      "Processed 1800/1800 fake files\n",
      "\n",
      "Total samples loaded: 3600\n",
      "Class distribution: Real=1800, Fake=1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m65/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8373 - loss: 0.3519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.8686 - loss: 0.2926 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
      "Epoch 2/30\n",
      "\u001b[1m86/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 3/30\n",
      "\u001b[1m81/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 5/30\n",
      "\u001b[1m43/90\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 7.2626e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.5829e-04 - val_accuracy: 1.0000 - val_loss: 6.1265e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m76/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.9484e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.7738e-04 - val_accuracy: 1.0000 - val_loss: 4.7438e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 9.3366e-04 - val_accuracy: 0.9986 - val_loss: 0.0015\n",
      "Epoch 9/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.9729e-04 - val_accuracy: 0.9986 - val_loss: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.3203e-04 - val_accuracy: 0.9986 - val_loss: 0.0011\n",
      "Epoch 11/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 7.7214e-05 - val_accuracy: 1.0000 - val_loss: 8.0959e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model and artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76  # Must be exactly 76\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Feature extraction that consistently returns 76 features\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "        feature_counts = {}\n",
    "\n",
    "        # 1. MFCCs (40 features: 20 means + 20 std)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "        feature_counts['mfcc'] = 40\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "        feature_counts['chroma'] = 2\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "\n",
    "        spectral_features = [\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ]\n",
    "        features.extend(spectral_features)\n",
    "        feature_counts['spectral'] = 6\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "        feature_counts['zcr'] = 2\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "        feature_counts['rms'] = 2\n",
    "\n",
    "        # 6. Spectral Contrast (12 features: 6 means + 6 std)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        # Ensure we only get 6 bands\n",
    "        contrast_mean = np.mean(contrast[:6], axis=1)\n",
    "        contrast_std = np.std(contrast[:6], axis=1)\n",
    "        features.extend(contrast_mean)\n",
    "        features.extend(contrast_std)\n",
    "        feature_counts['contrast'] = 12\n",
    "\n",
    "        # 7. Tonnetz (12 features: 6 means + 6 std)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "        feature_counts['tonnetz'] = 12\n",
    "\n",
    "        features = np.array(features)\n",
    "        total_features = sum(feature_counts.values())\n",
    "\n",
    "        if len(features) != FEATURE_COUNT or total_features != FEATURE_COUNT:\n",
    "            print(\"\\nFeature count breakdown:\")\n",
    "            for name, count in feature_counts.items():\n",
    "                print(f\"{name}: {count}\")\n",
    "            print(f\"Total features extracted: {len(features)}\")\n",
    "            print(f\"Expected: {FEATURE_COUNT}\")\n",
    "            raise ValueError(\"Feature count mismatch!\")\n",
    "\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset():\n",
    "    X, y = [], []\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    for label, folder in enumerate([\"real\", \"fake\"]):\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Directory not found: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(files)} {folder} samples\")\n",
    "\n",
    "        for i, file in enumerate(files):\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Processed {i+1}/{len(files)} {folder} files\")\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(label)\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y)==0)}, Fake={sum(np.array(y)==1)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    X, y = load_dataset()\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Prepare SHAP background data\n",
    "    background_samples = min(100, len(X_scaled))\n",
    "    background = X_scaled[np.random.choice(len(X_scaled), background_samples, replace=False)]\n",
    "\n",
    "    # Save artifacts\n",
    "    joblib.dump(scaler, \"scaler_nn_32.joblib\")\n",
    "    joblib.dump(background, \"background_nn_32.joblib\")\n",
    "\n",
    "    # Model architecture\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             batch_size=32,\n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=callbacks,\n",
    "             verbose=1)\n",
    "\n",
    "    # Save final modela\n",
    "    model.save(\"audio_model_nn_32.h5\")\n",
    "    print(\"Training complete! Model and artifacts saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adfd3553a28b68",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T05:37:33.140734Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 1800 real samples\n",
      "Found 1800 fake samples\n",
      "\n",
      "Total samples loaded: 3600\n",
      "Class distribution: Real=1800, Fake=1800\n",
      "\n",
      "Starting 10×10-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 1: Accuracy = 1.0000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 3: Accuracy = 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 1, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Repeat 1, Fold 10: Accuracy = 1.0000\n",
      "Repeat 1 complete. Mean accuracy: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 2, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 2, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Repeat 2, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 2, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 2, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 2, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 2, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 2, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 10: Accuracy = 1.0000\n",
      "Repeat 2 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 3, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 3, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 3, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 3, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 3, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 3, Fold 10: Accuracy = 1.0000\n",
      "Repeat 3 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 4, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Repeat 4, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 4, Fold 10: Accuracy = 1.0000\n",
      "Repeat 4 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 5, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 5, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 10: Accuracy = 1.0000\n",
      "Repeat 5 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 6, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  \n",
      "Repeat 6, Fold 10: Accuracy = 1.0000\n",
      "Repeat 6 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 7, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 7, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 7, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step  \n",
      "Repeat 7, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Repeat 7, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 10: Accuracy = 1.0000\n",
      "Repeat 7 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 8, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 8, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 8, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 8, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 8, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "Repeat 8, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Repeat 8, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Repeat 8, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Repeat 8, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step  \n",
      "Repeat 8, Fold 10: Accuracy = 1.0000\n",
      "Repeat 8 complete. Mean accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Repeat 9, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Repeat 9, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 9, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Repeat 9, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Repeat 9, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Repeat 9, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 9, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Repeat 9, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Repeat 9, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "SR = 22050  # Sample rate\n",
    "N_FFT = 2048  # FFT window size\n",
    "HOP_LENGTH = 512  # Sliding window for FFT\n",
    "FEATURE_COUNT = 76  # Total number of audio features\n",
    "N_SPLITS = 10  # Number of folds per cross-validation\n",
    "N_REPEATS = 10  # Number of times to repeat CV\n",
    "RESULTS_DIR = \"model_results\"  # Output directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Create results directory\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract 76 audio features from WAV file\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # 1. MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        features.extend([\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ])\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # 6. Spectral Contrast (12 features)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(contrast[:6], axis=1))\n",
    "        features.extend(np.std(contrast[:6], axis=1))\n",
    "\n",
    "        # 7. Tonnetz (12 features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load dataset from real and fake folders\"\"\"\n",
    "    X, y = [], []\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    for label, folder in enumerate([\"real\", \"fake\"]):\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Directory not found: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(files)} {folder} samples\")\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(label)\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y)==0)}, Fake={sum(np.array(y)==1)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_evaluation_metrics(y_true, y_pred, y_probs, repeat, fold):\n",
    "    \"\"\"Save evaluation metrics for each fold\"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title(f'Confusion Matrix (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/confusion_matrix_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'ROC Curve (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/roc_curve_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Prepare background data for SHAP\n",
    "    background = X_scaled[np.random.choice(len(X_scaled), min(100, len(X_scaled)), replace=False)]\n",
    "    joblib.dump(background, f\"{RESULTS_DIR}/background.joblib\")\n",
    "\n",
    "    # Cross-validation setup\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train model\n",
    "            model = create_model()\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=30,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Save metrics\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "            save_evaluation_metrics(y_test, y_pred, y_probs, repeat, fold)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model.h5')\n",
    "                joblib.dump(scaler, f'{RESULTS_DIR}/scaler.joblib')\n",
    "\n",
    "            print(f\"Repeat {repeat+1}, Fold {fold+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Repeat statistics\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Final results\n",
    "    joblib.dump(all_accuracies, f\"{RESULTS_DIR}/all_accuracies.joblib\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"10×10-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a8f42689db9d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T08:12:18.632073Z",
     "start_time": "2025-04-29T07:55:36.866586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 fake test samples\n",
      "Found 3000 real test samples\n",
      "\n",
      "Total test samples loaded: 6000\n",
      "Class distribution: Real=3000, Fake=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Test Accuracy: 0.5968\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.56      0.95      0.70      3000\n",
      "        Fake       0.83      0.24      0.38      3000\n",
      "\n",
      "    accuracy                           0.60      6000\n",
      "   macro avg       0.69      0.60      0.54      6000\n",
      "weighted avg       0.69      0.60      0.54      6000\n",
      "\n",
      "\n",
      "Test results saved in 'processed_audio_test_results' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76\n",
    "RESULTS_DIR = \"processed_audio_test_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Feature extraction that matches your training setup\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # 1. MFCCs (40 features: 20 means + 20 std)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        features.extend([\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ])\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # 6. Spectral Contrast (12 features: 6 means + 6 std)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        contrast_mean = np.mean(contrast[:6], axis=1)\n",
    "        contrast_std = np.std(contrast[:6], axis=1)\n",
    "        features.extend(contrast_mean)\n",
    "        features.extend(contrast_std)\n",
    "\n",
    "        # 7. Tonnetz (12 features: 6 means + 6 std)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Load data from processed_audio folder\"\"\"\n",
    "    X_test, y_test = [], []\n",
    "\n",
    "    # Process Fake audio files\n",
    "    fake_path = os.path.join(PROCESSED_DIR, \"Fake\")\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_files = [f for f in os.listdir(fake_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(fake_files)} fake test samples\")\n",
    "\n",
    "        for file in fake_files:\n",
    "            file_path = os.path.join(fake_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X_test.append(features[0])\n",
    "                y_test.append(1)  # Label 1 for fake\n",
    "\n",
    "    # Process Real audio files\n",
    "    real_path = os.path.join(PROCESSED_DIR, \"Real\")\n",
    "    if os.path.exists(real_path):\n",
    "        real_files = [f for f in os.listdir(real_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(real_files)} real test samples\")\n",
    "\n",
    "        for file in real_files:\n",
    "            file_path = os.path.join(real_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X_test.append(features[0])\n",
    "                y_test.append(0)  # Label 0 for real\n",
    "\n",
    "    print(f\"\\nTotal test samples loaded: {len(X_test)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y_test)==0)}, Fake={sum(np.array(y_test)==1)}\")\n",
    "    return np.array(X_test), np.array(y_test)\n",
    "\n",
    "def evaluate_model(model, scaler, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Scale test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled).argmax(axis=1)\n",
    "    y_probs = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, y_pred, target_names=['Real', 'Fake'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    with open(f'{RESULTS_DIR}/classification_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "def main():\n",
    "    # Load test data\n",
    "    X_test, y_test = load_test_data()\n",
    "    if len(X_test) == 0:\n",
    "        print(\"No valid test samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Load model and scaler\n",
    "    try:\n",
    "        model = load_model('audio_model_nn_32.h5')\n",
    "        scaler = joblib.load('scaler_nn_32.joblib')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or scaler: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(model, scaler, X_test, y_test)\n",
    "    print(f\"\\nTest results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9675828bcae54155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T16:47:05.992620Z",
     "start_time": "2025-04-25T16:10:22.296984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Repeat 1/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0018\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 0.9972, Loss: 0.0061\n",
      "\n",
      "Repeat 1 - Mean Accuracy: 0.9997 (±0.0008)\n",
      "\n",
      "=== Repeat 2/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0003\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 2 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 3/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0008\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 3 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 4/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 4 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 5/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 5 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 6/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 6 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 7/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 0.9972, Loss: 0.0029\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 7 - Mean Accuracy: 0.9997 (±0.0008)\n",
      "\n",
      "=== Repeat 8/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9972, Loss: 0.0111\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 0.9972, Loss: 0.0101\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 8 - Mean Accuracy: 0.9994 (±0.0011)\n",
      "\n",
      "=== Repeat 9/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0001\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 9 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Repeat 10/10 ===\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 3/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 4/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 5/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 6/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 7/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 8/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Accuracy: 1.0000, Loss: 0.0016\n",
      "\n",
      "Fold 10/10\n",
      "Accuracy: 1.0000, Loss: 0.0000\n",
      "\n",
      "Repeat 10 - Mean Accuracy: 1.0000 (±0.0000)\n",
      "\n",
      "=== Final Cross-Validation Results ===\n",
      "Total runs: 100\n",
      "Mean Accuracy: 0.9999 (±0.0005)\n",
      "Mean Loss: 0.0004 (±0.0016)\n",
      "\n",
      "Training final model on all data...\n",
      "Epoch 1/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9096 - loss: 0.2217\n",
      "Epoch 2/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9993 - loss: 0.0057\n",
      "Epoch 3/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9995 - loss: 0.0023\n",
      "Epoch 4/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 8.8365e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0011\n",
      "Epoch 6/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.4371e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.7354e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.5576e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.2889e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.7228e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 7.4797e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 7.2912e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4625e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.3131e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2479e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.5263e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6295e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 5.9618e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 4.8241e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.4412e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.0167e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 4.7633e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1555e-05\n",
      "Epoch 24/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3874e-05\n",
      "Epoch 25/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.3956e-05\n",
      "Epoch 26/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.0604e-05\n",
      "Epoch 27/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.0018e-06\n",
      "Epoch 28/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.7571e-06\n",
      "Epoch 29/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.1950e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.4637e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 10  # Number of folds per CV\n",
    "N_REPEATS = 10  # Number of times to repeat 10-fold CV\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Feature extraction that consistently returns 76 features\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "        feature_counts = {}\n",
    "\n",
    "        # 1. MFCCs (40 features: 20 means + 20 std)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "        feature_counts['mfcc'] = 4\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "        feature_counts['chroma'] = 2\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "\n",
    "        spectral_features = [\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ]\n",
    "        features.extend(spectral_features)\n",
    "        feature_counts['spectral'] = 6\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "        feature_counts['zcr'] = 2\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "        feature_counts['rms'] = 2\n",
    "\n",
    "        # 6. Spectral Contrast (12 features: 6 means + 6 std)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        contrast_mean = np.mean(contrast[:6], axis=1)\n",
    "        contrast_std = np.std(contrast[:6], axis=1)\n",
    "        features.extend(contrast_mean)\n",
    "        features.extend(contrast_std)\n",
    "        feature_counts['contrast'] = 12\n",
    "\n",
    "        # 7. Tonnetz (12 features: 6 means + 6 std)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "        feature_counts['tonnetz'] = 12\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset():\n",
    "    X, y = [], []\n",
    "    for label, folder in enumerate([\"real\", \"fake\"]):\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X, y = load_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    all_accuracies = []\n",
    "    all_losses = []\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        print(f\"\\n=== Repeat {repeat + 1}/{N_REPEATS} ===\")\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        fold_accuracies = []\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            print(f\"\\nFold {fold + 1}/{N_SPLITS}\")\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            model = create_model()\n",
    "            model.fit(X_train, y_train,\n",
    "                     epochs=30,\n",
    "                     batch_size=32,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0)\n",
    "\n",
    "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            fold_accuracies.append(accuracy)\n",
    "            fold_losses.append(loss)\n",
    "            print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "\n",
    "        mean_acc = np.mean(fold_accuracies)\n",
    "        std_acc = np.std(fold_accuracies)\n",
    "        all_accuracies.extend(fold_accuracies)\n",
    "        all_losses.extend(fold_losses)\n",
    "        print(f\"\\nRepeat {repeat + 1} - Mean Accuracy: {mean_acc:.4f} (±{std_acc:.4f})\")\n",
    "\n",
    "    # Final statistics\n",
    "    print(\"\\n=== Final Cross-Validation Results ===\")\n",
    "    print(f\"Total runs: {N_REPEATS * N_SPLITS}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Mean Loss: {np.mean(all_losses):.4f} (±{np.std(all_losses):.4f})\")\n",
    "\n",
    "    # Save final model trained on all data\n",
    "    print(\"\\nTraining final model on all data...\")\n",
    "    final_model = create_model()\n",
    "    final_model.fit(X_scaled, y, epochs=30, batch_size=32, verbose=1)\n",
    "    final_model.save(\"final_model_nn_70.h5\")\n",
    "    joblib.dump(scaler, \"scaler_nn_70.joblib\")\n",
    "    print(\"Training complete. Model and scaler saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc42a30eccd6125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:09:32.013765Z",
     "start_time": "2025-04-25T09:09:22.953917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 14:39:30.146 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:30.462 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-25 14:39:30.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:30.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:30.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:30.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-25 14:39:31.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.133 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-25 14:39:31.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_29948\\1003724151.py:77: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='importance', y='feature', data=df.head(15), palette=color_palette)\n",
      "2025-04-25 14:39:31.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-25 14:39:31.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMWCAYAAADs4eXxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAo0ZJREFUeJzs3Qm8TfX+//HPPsc8hmOeh1AyqxSpUP0alNAgwy90C4VuVHQJSSmlpEFpoEJF5VaaFJp0G6SbITJkyHQ7HLMMZ5//4/39/de++0zOYC/7DK/n47Efx9l7nbW/e52lvNfn8/2uQFJSUpIBAAAAAICIi4n8LgEAAAAAgBC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAJCvJSUlRXsIuQrHK//hd35qcbyBvIfQDQDIEYYPH24NGjRI9/Hxxx9H9P2OHj1qDz30kL3//vsWTVOmTHGfL6fLKccru8aNG2dPPPGE+/M777zjjvmDDz6Ya34n3333nRuTvqanV69eduaZZ9ry5cvTfL19+/bu71lWrF271rp3727R8Mcff7jPrN9XZsPq+eefb4MGDUr12ksvveT2dcstt6R67eWXX3avbd++PVPv450/Gl9mZfZnnn32WTfWzNqwYYP7ve7bty/TPwPg1CN0AwByjPLly9ubb76Z5qN169YRfa///Oc/NmPGDDt+/HhE95tX5ebj9e2339qCBQusf//+yZ6fOXOm/fjjj5aXJCYm2ogRI9xFkkjQxa5ly5ZZbhAIBNx/J9Ia71dffWWnnXaa/fDDD3bkyJFkr+m5OnXqWOXKlTP1PhdddJH7b1KFChUs0iZPnmyHDx/O9PYad4cOHdK9gAQgZyB0AwByjEKFClmzZs3SfOgfzEB2PPzww3bzzTdb0aJFkz1fokQJu+++++yvv/6yvKJkyZKuOv3MM89YfnTeeefZn3/+aVu2bAk9pxC7dOlS69evn/tdf//998mq4z/99JO1adMm0+9RtmxZ998k/fcqJ7j11ltt/vz5tnLlymgPBUA6CN0AgFzns88+sy5duljjxo3dP5ZV5Tl06FCqbW666SZr3ry5nXXWWfY///M/rrIpavFUdUhUFVR7pteeq8eJ2nrVJqoW3jlz5rj3Puecc2zdunWZHldGtH/9vCqwXbt2dX++7LLLbOHCha6V9H//93+tadOmdskll7h/aIf/nMb573//26699lpr0qSJderUKVVb/v79+10I7dixo9v3VVddZXPnzk22jY6HWsn1XtqPAmtax0t0HPSZFUK07TXXXGMfffRRsnHpeGlcN9xwg3vPiy++OFUL7YEDB1wL+AUXXOD2pc++ePHiZNvova688kr3+1S1UW3gquyeiPbx22+/uZ9L6d5777XNmzfbpEmTMvitmNvHbbfdZi1atHCP22+/PVmwS699OGVLt7Z5+umn3THT8dKfvWqrQuHZZ5/tPp9+Tp8vGAxaVpxxxhnWuXNne/HFF23FihUZbn+iY6o/e+PTuFWFVSU5vKqqirrOR/1dC6fz4P7773d/VmVZFwH0d1C//0svvdReeOGFZJ9Nf++GDRtmgwcPdr//Pn36pBqrArLOPx23r7/+Ot3QLQrZHv3dPXbsmHXr1s2qV6+e7Gd1gWLPnj2uLd2jv3s9e/Z0n0t/v3We7N69+4S/63fffdeuuOIK9/muvvpq112h8z5la7z+Htx4441uOx1v/Z483pQGHXPvz7pIMGbMGGvXrl3ov2Mp/+6oQ0i/l+effz7NYwIg+gjdAIAcRe3LKR/hCwtpTrECj9oq9Q/5O+64w9577z0bOHBgaDsFLW3TqFEjN0dS4UH/2H7ggQfcP3rVFuqFiQEDBoT+nFkKJZoHOn78eBcC6tatm6lxZeUYDB061P3j/LnnnnMVWgUStUfrH+pTp051n0FhYMeOHcl+VsFQAVmfqXbt2nbnnXfaF198EfoHvMKRxqq5rTo2LVu2tH/84x9un+F0gULBQNvoM6R1vLSNgpUCvP7B/9hjj7nqn8YaPi6FK41DoURhS6H10UcfdS2/3vHs27evG5fGr/fUcdTx9Nq/tf9Ro0a5UKWx9ujRw6ZNm+aeOxH9DhTiKlasmOo1BRVdCHjttdeShbSUfv/9d/e72LVrlz3yyCPu967ArbnOei6rNH5dEHnqqafcBZXVq1e7Cxvq5tC8c/3OW7Vq5Y5z+AWMzFL1vkyZMhm2mWd0TK+77joXVEXt1DpWuiiiQOlRK7fOK80j99q2NRVBn0nnqs59nbcKl9qf3kfB8cknn7TRo0cnG48+a/Hixd3nT2vutcL+Bx984I5L27Zt0/xM1apVc3/XVb32KGQrAKtCrZ/zzjvvYkfBggXt3HPPDX2v30WRIkXcGHUsVRnv3bt3uh0R8+bNcxdWdF7r3NXvVH9n0rogpACtixz6e6ALghMnTrRFixaFjrHomHt/1sWvL7/80v1dV9jW32393Xn77beT7VfHVBfmDh48mOYYAURXgSi/PwAAIVu3bnVBOSUFULVQ6h/wCnb6h7++emrVquX+oaxwqX/oq/Ksaq/CpEf/wNU/rFX1UgVLFUGpUaOG+wd5VnkBWDI7rsxSSNX+FVJEiyT9/e9/d5VnrwKoNmJVg1XNrFSpUrKKocKqaDw6DroIcOGFF7qqmyq2b7zxhjse3jYK+QoLCpZeG3+VKlVcePZ4Vb3w46XgqeqsAoanatWqroqrEOtVl3V8tI33eRT0NcdaF0f0/goVuhiicSrAe4FY+//Xv/7lqn4an0LfyJEj3esKTxqrvtcxOf3009M8lvr5tKrcnnvuuceFMIWrf/7zny5spaSQpwsf06dPdy3poqCqsSpMKhBlhQJ1eCVXoU2VVgWwmJj/q4eoU0IhSufricafltKlS7sLTLpAomOqcycldTxk5ph655YuXIjOY13IULDWhR8FcP2dVWvzzz//7P6O6XjqOOoz6Xe7ZMkS103gfQ59Nr2uyrnCrPe7U/gdO3ZsqG07vJL8+OOPuyCq34Wqviei3034vG6NR6HU+4yzZ8+2bdu2uXNcF3X03wOFfe99dLFKFyRiY2Pdc3pdY1fQ1YWJlPQ51L3hdQDonNZn0b5Suuuuu0IL0+mY6u+BzlH9vHeMdcy9Pyvw63h5x07Ht1ixYlauXLlk+9UFMlXz9Xn0dx1AzkKlGwCQY6hNUq3OKR8KjqL2alVQ1XobXglXS67C0DfffOO2U5VswoQJruqjUPrhhx+GWi8jtcCUF9qzMq6s8EKxeP/A1j/+PV44TrlqsXesvIWl1Ib+yy+/hOayKhSH71vUDqsqpYJvWp8vParuKZhrDApcCq1eC3/K4xz+ngpVqjp6rfcK6Aop4W3rCp+6OKCOAa+amvL4etund3y1f1WiVf1Mj8KWKtcbN24MrW6ekkKR2owVFL331u9V4VmBMqtSHlu1g6vCrNCkCvEnn3ziquCqlOq57NCx0e9VFwXSmuub3WOq0Kow6n1uHRsFWl1gUpVYFLR10UTHS+dcgQIFQqHXo7FJ+PxqdTekNU9a55Qqwwqembl4pdCttnGdl7pwo9+tVxnXuDQeb/wKqV5rueZ+6++AQqsuFHnHRJVzdbOkdUw2bdrkAnzKz5fehRKdMx5dyImLizvhyuMK2W+99Zb97W9/s9dff919Hl1US3kc9PdasrKiOoBTh0o3ACDH0D+4VbFJj+ZeiqpheqSk6pto/qVaVzXHWsGzZs2aoX/sRuoeuKo2ZXVcWeFVVMOlXAgsLSlXVFZg12fWP+z37t3rLmykpH/4S/g//sM/X3o0H1rt5ap2KjQrNDVs2DDN45yygqxQ7W2j46eLCF6VNyXv+KrbIS3pHV9VczPzWRTSVPF99dVXXWtwWu+vCzd6pKSLB1mVcjwKv5rProsWCnm6SKCLFAqHJ3O+qmKt343azFO2I2f3mKqKrrFpv6r0q61cF18UBhWgdaFAr6miKzrn1OruVY093nno/Y7EqzanpAsRCs1qLVe3R0adKd6dDnRhQd0z2q9XOdbfK80JVweBAq0+pxfIdf6ry0QXQPRIqXDhwqme8+Z6p6w8e3+nMvo7HP73IC3q1lHlW90FOkf00PFXm7r3dy18v1obAUDOQ+gGAOQapUqVCrUEq/KYViAQVV9VfVY7sP6BqjCvKpYqRhlJOQ8zMwuhZXZcp4LCVPg/+OPj413gUajVOFSZS0mrPYvCUWYpnCiwKWyrG0HVW4VEtfYrPGaFWuU1boUPXSTxrFq1yj3nHV+17quimlJ6Acf7PJm5h7HXZq6A6rW4h49P1dC0FvfSZxZv3CkXPsvMHFtV2lXd1hxivY8Xyr1FwbJLv2+FM1VG1UoeLrvHVFQJVtVVVWL93dICX6qwKhgqeCtoq13aG0NCQoL7exUevL1Qn5lzbsiQIa4NXdVjXUjQ4m8pQ3zKCyHeooI6HxXCdZ561K6tqRYaq46Dxi8K5/o9akpIWpXqtC56ee33Kef2Z2euf1p0fDVNQA9V1DX/W79LTbkJX0jRO8ez8ncYwKlDezkAINdQJVUVJf0DXxVx76FFsjR/UiHNa1fWCsmqZHntqmp5DQ9Faf2jXVWwlAuTnWiBrayO61RQdd+jwPrpp5+6OdQ6Dmp3V+Uv5X2MFZYUSlQBTE/K46UgpQXGtOiTPqsXPlMe58xQF4LaqL2f9cauAKxpAWqr1/h27tyZ7PjqPTVXOL2WWn1mVVS3b9+e4Rj0u9ecXLUie4tYebwV6nVhwXtvBTVd1NGcXO/nJfz8Wb9+faiifCI6x3SuKux7gVvTIlRFzerq5Slpn1qhXu3Z4StwZ/aYptV9oNZm/ZzCrxYP089o/KrYa9FCVaK9het07FS9T7mKvs450bmZEV0AUKeEuirUKv/KK69k+DO6eKFpFbowkHLRNX2vvwdaa0GB3Du39TvU2HXBLvyYaM65Ppd3B4OUoVvrHHjngUd/77Ij/HjreKrzQos2iuaga065LggogIfzzjttAyDnodINAMg19I9jLQqlf3zrz6qmqcKjyo9CgLcIm8KjVsLW9/pHsVYyVuhQFUsVb696KWqF1XxNhRDtT4tX6ZZamtuqf7BrkatIjetU0MrGmp+txaAUihT8ZsyY4V7TAmezZs1ylU/dmkltzPq8aj3W3Gmv+pmWtI6X5pFqvq2OsX5WlWK1aIt3nDNDIU4dCWpT1irnmkOrarnGrnZaVe80T18LVql9VgFPx1Xf63ca3mabkqqa4StZn4i21WJvOm7htAicFpnTyupaBEttxgrmusChudeiMSkYai0BVWZV4dZrmbm/vM5XrdytBb50bNVOrRW8w8/Xk6HVyDX3Wl0PnsweU++cUGu3fuf63dSvX9+FO31+VVxFFzc0dl1ACF9YT4uead+qUGv/2q8qzGrf1voD9erVy/TnUIVdc6cVgBVGNZb0KExrVXpdzNHCZuEUpFWB17nvLSLnUVu8Ojj0uTTv3LtTgarm4Z/Lo2Olv0vqrtGUFq2hoN+fd5/09KZMpEfHW+er5sfrYpT+26HF43SBRNV7XejS7clSToPQcVclPnzOOICcg9ANAMhVFIrUBqoFohR8VBlUtU1tst4/whV8vPmPovZZzbVWdc27BZWqWmoX1j5U8dIiSVoNXPOU9Y9aLeKlyrCCk7fa8MmO61RQO7Gqw5pjq6qdAoP3D3H9o1xBRNV3L2ypSq/2Zu/WUOlJ63jpooJ+VmFZVWUFKIVF3eZIxznlPc/TowsVCmE6VhqXgqYChsbuVd8VxhXsdNFAx1ihSe3XCkneBYG0KJzoAowCX1q3DUtJn0W3mAqvjiso6uKCFlpTG7qq8AqeClbe/csVlhQGdWx1UUMXJHQhIzMXbfSeCodqL9cCdLoYonZiVdcVDDO6F3lGFPx1Xmg84TJzTNUxogsgGqPOEe3HC8C6SBA+nULhWhdKvNZyL5TqfNTfI3UGqNquz6f3SKtdPyNaZV6/H11I0P7So7+7ovUcUv790/mmUK6Wfl1oSVkF1625FHQVphV2FXxVXffmhaek279pGop+ThewVBnXXGw9MrM2QjjdtUB/r7RwmtYQ0Cr0Oi/0d0HTQNRRo9+DLuyEU5eILl6ltfo+gOgLJEVqRRkAABA1mqOqduzPP//8hKt15zf6Z44qlgrfKUMnEAnqAtAFLl3A8uh2eOqM0AWLE3ViRIJa5VVh19oK2bn9IQD/MacbAADkWaq03n333a5zgZWd4Qd10KgyrY4KdXio2q1Wc3UB+B24RVVwtd0TuIGci/ZyAACQp2lesdrA1ebszUEGIuWRRx5x0womTpzo2ue18JtCsNrT/aZ2fk1B0JQYADkX7eUAAAAAAPiE9nIAAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8wi3DgHTs3r3fgsFojwJ5QSBgVq5cSdu1a79xvwhEAucUIo1zCpHGOYX8cF4F/v94MkLoBtKhv8g54S8z8g7OKUQa5xQijXMKkcY5BT/ktvOK9nIAAAAAAHxCpRtIR0xMjMVwWQoRFBvLCYXI4pxCpHFOIdI4pxBJSbmpvB0mkJRbRw4AAAAAyDcSjydabIFYi4/POXO64+KY0w1k26ThL9v6VZujPQwAAAAg36tet7INf+I2y40I3UA6tv6+09at3BTtYQAAAADIxZhkAQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3RHUoEED99i2bVuq12bPnu1emzJlivv+nXfeCW2f8rFkyZLQz23fvt1Gjhxp7dq1s2bNmlnnzp1t3rx5qfa/d+9emzBhgrVv396aNm1ql19+uU2fPt2CwWCWP8eePXvs/PPPtz/++CPZ8wsWLHD7bd68uXXv3t1WrlyZ5X0DAAAAQH5SINoDyGsKFixoCxcutJ49eyZ7/rPPPrNAIJDsuUqVKtncuXNT7aN06dLu68aNG+2mm26yFi1a2OTJk61cuXL27bff2ujRo2337t3Wt29ft11CQoLdcMMNVqFCBRs/frxVq1bNli9fbuPGjbMtW7bYqFGjMj1+hff+/fvbrl27kj2/du1aGzp0qD3wwANuPAr0t912mwviRYsWzdIxAgAAAID8gtAdYa1atUoVug8cOGDLli2zM888M9m2sbGxVr58+XT3NXbsWGvYsKGrjnuBvUaNGnb06FGbNGmSdevWzUqVKmWPP/64FSpUyF566SUrXLiw26569epWpEgRGzhwoBtL7dq1Mxz7jz/+aPfee68VL1481WvffPON1atXz1Xa5a677rKZM2faunXrrHHjxlk4QgAAAACQf9BeHmEdOnSw77//3gVtz+LFi10YTyvMpmfHjh2uqn3zzTenqpArbE+bNs2KFSvmAvj8+fOtR48eocDtufjii11FumrVqpl6z6+//tq6du0aaoEPd9ppp7mAvXTpUteyrvb4EiVKuIsAGVGbutrmdRzU/q729AcffNB+++0369Kli2ubV9U8/Ji98cYboW179epla9asCb22c+dOGzx4sJ199tl21lln2bXXXuvGFf5en376qXXs2NFdENC+1TIPAAAAAKcaoTvC6tevbxUrVrQvv/wy9JxasBUAs0IhMykpKc0qstq5FeILFChgmzdvtkOHDqW5ncJ669atXRU8M+68805XGVcFPqUrrrjCLrroItfurqD76KOP2lNPPRVqhc+MF154wZ599lnX9v7aa6/ZHXfc4VrWVaH/+eefQ6326hR4+umnXVv8u+++ay1btrTevXu71ncZNmyYJSYmumCu+e063mPGjEn2XlOnTnXdAK+//rprtX/llVcyPU4AAAAAiBRCt0/VbgVHUSVardl6LiUtuKZKbvjjiSeecK/t27fPfS1ZsuQJ3yuz250szRv/888/7f7777e33nrLrrnmGhsxYkSqud8nokCvdvmrrrrKzU+/8sorrU2bNi5Un3feebZhwwa33Ysvvuiq06rU16pVy10MULX+vffecxcidAFDgbxu3bqu5V1VflXhw6kS3qRJE7eoXKdOnVzwBgAAAIBTjTndPlDAVug7fvy4axFX9VshMyUtfKaKbzjN0fbaub1QXbZs2XTfy9vOqwL75bHHHnOfQwFXVK3WSuZvv/223XrrrZnah+aZezTfPLztXd/rAoWsX7/eJk6c6CrVniNHjriF5VS918rpH374of3000/2+++/24oVK1Kt0l6zZs3Qn9UGf+zYsZP49AAAAACQPYRuH6hyK5pnrFXLL7nkkjS3U3t4eDgM16hRIxcwFSh1u7Bwaie//fbb3aJnqvSqyq3bd6mym9KAAQPcnGjdAuxkaP/ajycmJsZVrdO6PVp6Uratax9pUev4fffd56rf4RSeFa61arsuRqjlXfO+FajVqp5yFXkAAAAAiDbay32gMH3hhRe6FvNFixZleT63qLqt1usZM2a4lupwqi5rpfHKlSu791L41EriXqXYo/fXQxX1k6V9qAIdTlVm3Z4s0rTSuhaS0wUJ76E52pr3rTbyH374wS0Qp1ubaZ75f/7zH/dzKY8TAAAAAEQbodvHFvM5c+a4tvLwtuqs0JzpX375xYYMGeK+KuS+/PLLrvVaC5B5i5gNGjTIrfzdr18/t3K6FlfTew8fPtwtQKZq+Mm6/vrr3VxuLVy2adMm126uKrdWDo+0Pn36uIsNei99Fn3ejz76yM3hVvu9KuRasX3r1q328ccfh1ZbT3nRAQAAAACijfZyn7Rt29bN6c5OldujsDxr1iwXKtUmfvDgQatTp46NHz/eLQ7m0b2+Z8+e7bbTyt66PZZu5aV55Zr/HAmqpuv9n3/+eVeFPuOMM1wwTmuueiTeKz4+3q2Orq86Ds8995xbVE20Uvkzzzzj5nyrKj5y5EjXar9q1aoT3vccAAAAAE61QBI9uUCaht7wsK348bdoDwMAAADI9+o1qmnPvPd/twmOj99vOSHFBgJmcXEZ30WK9nIAAAAAAHxCe3k+oHtpZ9TmvmzZsmzv/9xzzz3hfGrNv65SpUq29w8AAAAAuRWhOx/Qvby1KJlf5s6dm+o+2eEisXo6AAAAAORGhO58QPfHTu9+4JGQ3dXZAQAAACCvY043AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+4T7dQDqq1q5ofx0+Eu1hAAAAAPle9bqVLbcKJCUlJUV7EAAAAAAAnEji8USLLRBr8fH7LSek2EDALC6uZIbbUekG0pGQcDDaQ0AeUqZMcc4pRBTnFCKNcwqRxjmFSEtKSrKyZUtYbkPoBtIRDAYtGIz2KJAX6CqoJCYGc8RVWeR+nFOINM4pRBrnFPw8r3IbFlIDAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ6xeDqQjJibGYrgshQiKjeWEQmRxTiHSOKcQaZxTSCkYTHKP/CSQpJudAQAAAADgs8TEoO3ZcyhbwVu3DIuLK2nx8ftzxK3ovPFkhEo3kI7JD71p69f8Ee1hAAAAAHlC9VoV7e6xPS0mJpCvqt2EbiAdWzf9x9av2RrtYQAAAADIxZhkAQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3adQgwYN3GPbtm2pXps9e7Z7bcqUKe77d955J7R9yseSJUtCP7d9+3YbOXKktWvXzpo1a2adO3e2efPmpdr/3r17bcKECda+fXtr2rSpXX755TZ9+nQLBoOZHv/69eutb9++1qJFC7efqVOnZunnAQAAACC/KRDtAeQ3BQsWtIULF1rPnj2TPf/ZZ59ZIBBI9lylSpVs7ty5qfZRunRp93Xjxo120003uRA8efJkK1eunH377bc2evRo2717twvIkpCQYDfccINVqFDBxo8fb9WqVbPly5fbuHHjbMuWLTZq1KgMx3348GG79dZb7ZxzznFj0s8NHz7cSpYsaT169DjJowIAAAAAeROh+xRr1apVqtB94MABW7ZsmZ155pnJto2NjbXy5cunu6+xY8daw4YNXXXcC+w1atSwo0eP2qRJk6xbt25WqlQpe/zxx61QoUL20ksvWeHChd121atXtyJFitjAgQPdWGrXrn3Ccf/www+uWq731L7q1KljN998s73//vuEbgAAAABIB+3lp1iHDh3s+++/d0Hbs3jxYhfGixcvnun97Nixw1W1FXxTVsgVtqdNm2bFihVzAXz+/PkuGHuB23PxxRe7FvOqVatm+H5nnHGGPfPMMy5whwv/HCfSq1cvF/r79OljTZo0cWPctGmTq7I3b97cLr30UndcPL/99pv7GW172WWX2cyZM0OvJSUludZ2tbifddZZ1rZtW3v66aeTvddzzz1n/fr1C/38V199lalxAgAAAEAkEbpPsfr161vFihXtyy+/DD23YMEC69ixY5b2s2bNGhc+GzdunOq1okWLuhBfoEAB27x5sx06dCjN7RTWW7dunSpIp0UV93PPPTf0/V9//WVvvfWW+/nMUmi//vrr3Xz1/fv3u+AdFxfn2tVPP/10e/DBB0P7/tvf/mYtW7a09957z+6991579tlnQ3PV9XXGjBmuVf7jjz+222+/3VX7V65cGXovhfIrr7zSPvjgA9cNoHDP/HMAAAAApxqhO0rVbrWYiyrR33zzjXsuJS24pipw+OOJJ55wr+3bt8991ZzqE8nsdlmh8Kr53AcPHrTbbrst0z+nyroWcKtXr567yFCiRAkbPHiw1a1b14XxDRs2uO3Usq756XfeeafVqlXLVbT79+9vr776qnu9cuXK9vDDD9t5553n5qd3797dXRRYu3Zt6L0uvPBC69Kli2u3HzBggFtw7s8//4zYMQAAAACAzGBOdxQoYCtsHj9+3LWIq/qtkJmSFj577bXXkj2nOdpy2mmnhUJ12bJl030vbzvNx44EjVmVZ7XEv/zyyyecc56SArJH88mrVKkSao3X98eOHXN/VvhevXq1u8jgSUxMdHPcRdX1f//7326uulZU//XXX12gDq9kK6x7FO69sQMAAADAqUSlOwrUNi1Lly51q5ZfcsklaW6n9vCaNWsme5QpU8a91qhRIxdYV6xYkern1E6uudMKrqr0qsod3nodTlXg8FuQnYhCsarPqtK/8MILbtX0rNDnCRcTk/bpp3CsKrbayL2Hqt9ee/mcOXPcXPYjR464ueCal66V3lOuEp+S2vEBAAAA4FQidEeBwqfanxVeFy1alOX53KLqdps2bdzc5pRh8u2337Yff/zRtWHrva644gq3EJla2cPp/fVQRT0z7r//ftcKr0XadOswv2gl9d9//91Vxr2LDT///HOo6q97mmse93333efuS64LEbt27SJUAwAAAMhxCN1RbDFXxVZt5bp9V3aMGDHCfvnlFxsyZIj7qqCqlu+JEyfa0KFDQ/fzHjRokFtlXKt5a4VwLa6m99a87N69e7s51hlR2NYCaPoZhWC1c+uh+4FH2tVXX+0WU1PIV/v4F1984RZN81rwFbLVlq/Pq0r/3//+d1eFT3lRAQAAAACijTndUaLbXKmNOjtVbo/C8qxZs9zK3WoT18Jmun+2AmqnTp1C22netarD2m7YsGG2Z88e13aueeVahCwzPvnkE/dVQVgPj2435i0KFymag61q+kMPPeQq2ZqXrlueeYu2qcKtxzXXXOOCuBZn04rtmtsNAAAAADlJIImeXCBN99w2xVb++/doDwMAAADIE+o2qGpPzRhqCQkH7fjxrN/OV2swx8WVtPj4/ZYTUqw3nozQXg4AAAAAgE9oL4dbhCyjNvdly5al+5ra2efOnZvu62oL1322AQAAACC/IXTDzZn2bseVHZpP3rNnz3Rf9xZ0AwAAAID8htANi42NdSuSZ5duX6YHAAAAACA55nQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOAT7tMNpKNqzQr2119Hoz0MAAAAIE+oXqui5UeBpKSkpGgPAgAAAACQ9yUmBm3PnkMWDGY9hgYCZnFxJS0+fr/lhBTrjScjVLqBdCQkHIz2EJCHlClTnHMKEcU5hUjjnEKkcU4hLcFgUrYCd25G6AbSEQwGLRiM9iiQF+gqqHdlNydclUXuxzmFSOOcQqRxTgH/xUJqAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITVy4F0xMTEWAyXpRBBsbGcUIgszilEGucUIo1zKv/Kj7cGS08gKYlF/AEAAAAAkaPbxe3ZcyiiwVu3oouLK2nx8ftzxK3ovPFkhEo3kI7JT86z9eu3R3sYAAAAQK5SvXp5u/uebhYTE6DaTegG0rd1azyhGwAAAMBJYZIFAAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0R1CDBg3cY9u2balemz17tnttypQp7vt33nkntH3Kx5IlS0I/t337dhs5cqS1a9fOmjVrZp07d7Z58+al2v/evXttwoQJ1r59e2vatKldfvnlNn36dAsGg1n+HHv27LHzzz/f/vjjjzRf1/PNmze37777Lsv7BgAAAID8pEC0B5DXFCxY0BYuXGg9e/ZM9vxnn31mgUAg2XOVKlWyuXPnptpH6dKl3deNGzfaTTfdZC1atLDJkydbuXLl7Ntvv7XRo0fb7t27rW/fvm67hIQEu+GGG6xChQo2fvx4q1atmi1fvtzGjRtnW7ZssVGjRmV6/Arv/fv3t127dqW7zZgxY+zQoUOZ3icAAAAA5FeE7ghr1apVqtB94MABW7ZsmZ155pnJto2NjbXy5cunu6+xY8daw4YNXXXcC+w1atSwo0eP2qRJk6xbt25WqlQpe/zxx61QoUL20ksvWeHChd121atXtyJFitjAgQPdWGrXrp3h2H/88Ue79957rXjx4ulu895779nBgwczdSwAAAAAIL+jvTzCOnToYN9//70L2p7Fixe7MH6iMJvSjh07XFX75ptvTlUhV9ieNm2aFStWzAXw+fPnW48ePUKB23PxxRe7FvOqVatm6j2//vpr69q1a6gFPiVV1CdOnGgPPPCAZYXa0dU2r+Og9ne1pj/44IP222+/WZcuXVzb/G233ZbsmL3xxhuhbXv16mVr1qwJvbZz504bPHiwnX322XbWWWfZtddea0uXLk32Xp9++ql17NjRGjdu7PatlnkAAAAAONUI3RFWv359q1ixon355Zeh5xYsWOACYFYoZCYlJbnQmFLRokVdiC9QoIBt3rzZtXqntZ3CeuvWrV0VPDPuvPNOVxlXBT4tmjOugHv66adbdrzwwgv27LPPurb31157ze644w4bOnSoq9D//PPPoVZ7dQo8/fTTri3+3XfftZYtW1rv3r1d67sMGzbMEhMTXTDX/HYdb7W8h5s6darrBnj99dddq/0rr7ySrTEDAAAAwMkgdPtU7VZwFFWiv/nmG/dcSlpwTZXc8McTTzzhXtu3b5/7WrJkyRO+V2a3O1la3E3VZIXy7NLPql3+qquucvPTr7zySmvTpo0L1eedd55t2LDBbffiiy+66rQq9bVq1XIXA1StV2u7LkToAoYCed26da1evXquyr9u3bpk76VKeJMmTdyicp06dXLBGwAAAABONeZ0+0ABW6Hv+PHjrkVc1W+FzJS08JkqvuE0R1tOO+20UKguW7Zsuu/lbedVgf3w119/2f333+8WcNM88ezSPHOP9hPe9q7vdYFC1q9f79rYVan2HDlyxC0sp+p99+7d7cMPP7SffvrJfv/9d1uxYkWqVdpr1qwZ+nOJEiXs2LFj2R43AAAAAGQXodsHqtyKKsNatfySSy5Jczu1h4eHw3CNGjVyAVOBUrcLC6d28ttvv90teqZKr6rcK1eudJXdlAYMGODmROsWYNn1yy+/uFXQdSEh3N/+9jd3C7PMzvFO2bYeE5N2o4Vax++77z5X/Q6n8KxwrVXbdTHiiiuucPO+FajVqp5yFXkAAAAAiDZCtw8Upi+88ELXYr5o0SK79dZbs7wPVbfVej1jxgy74IILki2m9vbbb7uVxitXruzeS+Fz5syZbhG08Pnben89NG/6ZCjMa2GycJdeeqlbDE1jjDSttK6F5MIvSIwYMcK1lata/sMPP7gOAq8DQJ9d1HoOAAAAADkJc7p9bDGfM2eOaysPb6vOCgVNVZmHDBnivqqV+uWXX3at1wrS3v28Bw0a5Fb+7tevn1s5XYur6b2HDx/uFiBTNfxkqPVbATj8IVrALK22+ZPVp08fd7FBi6Tps+jzfvTRR24Ot9rvVSHXiu1bt261jz/+OLTauteeDgAAAAA5BZVun7Rt29bN6c7qquXhFJZnzZrlQqXaxHV/7Dp16tj48ePd4mAe3et79uzZbjut7K3bY+l+3moH1/zn3EaV+/j4eHvqqafcVx2H5557zi2qJlqp/JlnnnFzvlUVHzlypGu1X7Vq1Qnvew4AAAAAp1ogiZ5cIE333P2irVy5OdrDAAAAAHKVunUr21NTBlhCwkE7fjz5gscnQzNu4+JKWnz8fssJKdYbT0ZoLwcAAAAAwCe0l+cDu3btyrDNfdmyZdne/7nnnnvC+dSaf12lSpVs7x8AAAAAcitCdz6ge3lrUTK/zJ07N9V9slPejxwAAAAA8iNCdz6g+2Ondz/wSMju6uwAAAAAkNcxpxsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ9wn24gHVWrxtlffx2L9jAAAACAXKV69fLRHkKOEkhKSkqK9iAAAAAAAHlHYmLQ9uw5ZMFg5OJmIGAWF1fS4uP3W05Isd54MkKlG0hHQsLBaA8BeUiZMsU5pxBRnFOINM4pRBrnVP6msB3JwJ2bEbqBdASDQQsGoz0K5AW6Cupd8c0JV2WR+3FOIdI4pxBpnFPAf7GQGgAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATQjcAAAAAAD5h9XIgHTExMRbDZSlEUGwsJxQii3MKkcY5hUjjnMqfuF1YcoGkJBbxBwAAAABEhm4Vt2fPoYgH70DALC6upMXH788Rt6LzxpMRKt1AOiZNfc/WbdwR7WEAAAAAuUaNqnE2fFAXi4kJUO3+/wjdQDr+2L7L1v1O6AYAAACQfUyyAAAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeE7lOoQYMG7rFt27ZUr82ePdu9NmXKFPf9O++8E9o+5WPJkiWhn9u+fbuNHDnS2rVrZ82aNbPOnTvbvHnzUu1/7969NmHCBGvfvr01bdrULr/8cps+fboFg8Esf47jx4/bNddcExorAAAAACBtBdJ5Hj4pWLCgLVy40Hr27Jns+c8++8wCgUCy5ypVqmRz585NtY/SpUu7rxs3brSbbrrJWrRoYZMnT7Zy5crZt99+a6NHj7bdu3db37593XYJCQl2ww03WIUKFWz8+PFWrVo1W758uY0bN862bNlio0aNytJnePnll2316tXWsWPHbBwBAAAAAMg/CN2nWKtWrVKF7gMHDtiyZcvszDPPTLZtbGyslS9fPt19jR071ho2bOgqzl5gr1Gjhh09etQmTZpk3bp1s1KlStnjjz9uhQoVspdeeskKFy7stqtevboVKVLEBg4c6MZSu3btTI1/06ZN9uqrr1q9evWyeQQAAAAAIP+gvfwU69Chg33//fcuaHsWL17swnjx4sUzvZ8dO3a4qvbNN9+cqkKusD1t2jQrVqyYC+Dz58+3Hj16hAK35+KLL3Yt5lWrVs30+95///02aNAgK1u2rGWF2tpVte/atas1adLEVeG3bt3q9qV2d7Wrr127NrT9jz/+aF26dHHbdurUyT755JPQa/pMDz/8sF1wwQXWqFEjt+8333wz2XvNnDnTrr/+emvcuLHb94oVK7I0XgAAAACIBEL3KVa/fn2rWLGiffnll6HnFixYkOVW7TVr1lhSUpILlSkVLVrUhfgCBQrY5s2b7dChQ2lup7DeunVrVwXPjLffftuOHDniwmx2PPnkkzZ06FCbNWuWrVq1yq699lo7//zzXRjXmFWdlz///NNuu+02F7rff/99u+WWW2z48OEuiMsLL7zgLlSowv/xxx+7eexqlY+Pjw+9l1679dZb7b333rOSJUvagw8+mK0xAwAAAMDJIHRHqdqtFnOvavvNN9+451LSgmvNmzdP9njiiSfca/v27XNfFShPJLPbZWTXrl0uFD/wwAOpKuuZpRCtkH3WWWe5sH/66adb9+7d3derr77aNmzY4LZTlVrbqe29Zs2arlKtOekzZsxwr6ulXnPTtXCc2uT79+9vx44dc3PcPQr0upChtvk+ffpQ6QYAAAAQFczpjgIF7MGDB7tVwNUiruq3FkFLSQufvfbaa8me0xxtOe2000Kh+kSt3t52Wr38ZCjkKjRrrNmlgOzRfPLwtnZ9r+AsCt+LFi1yFxk8es2bd64wrQsVWo1d26pqLomJiaHta9WqFfpziRIlQvsGAAAAgFOJ0B0FLVu2dF+XLl3qVi2/5JJL0txO7eGq9KZFc5lVcVYFV7cLC6d28ttvv93uvfdet+CZqtwrV65086NTGjBggPXq1ctVlk9E88IVjF9//XX3/V9//eUWf1N7t17LDC0MFy4mJu1GC12M0DxuVbBTHg9RtX/OnDnuIoBay7Vau+Zxp1wlHgAAAACijdAdBQqPF154oWsxV0VXc4+zStXtNm3auJZrLSgW3vKtudea/1y5cmX3XldccYVr2dYiZuHzt/X+emiedUY+/fTTZN8PGzbMLYCm1u1IU0VbgT78goNuU6ZWfAXxN954w8aMGePuNS7r1q1zXzXHHQAAAAByEuZ0R7HFXNVatZWHt11nxYgRI+yXX36xIUOGuK+///67C6cTJ050Qdq7n7dWCNdq6f369XMrp2txNb23Fifr3bt3pm7/pQAc/lDVW/vPysrnmaV7j6uCr4q25mlrMTXNJ69SpUqoZV4XK3SPcV1cuOeee9zzCuUAAAAAkJNQ6Y6Stm3bujbqrK5aHk5hWSuBa6VutYkfPHjQ6tSp4+Zfqz3bo3t9z549222nCvWePXvc/bw1r1wLmeU0CvJTp061xx57zN1bXKu96wKBFluThx56yFW6r7zySvfadddd51rXf/3111St9gAAAAAQTYEkenKBNN01+hVbsXpLtIcBAAAA5Br1aleyZyfcagkJB+348WBE960ZtXFxJS0+fr/lhBTrjScjtJcDAAAAAOAT2svh7sGdUZu7FjZLj1ZKX7JkSbqvjx07NtQaDgAAAAD5CaEbbmGyefPmZfvndcuuw4cPp/t6WvcgBwAAAID8gNANtwhZevcDz4wKFSpEdDwAAAAAkFcwpxsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ9wn24gHdUql7O/jhyL9jAAAACAXKNG1bhoDyHHCSQlJSVFexAAAAAAgLwhMTFoe/YcsmAwslEzEDCLiytp8fH7LSekWG88GaHSDaQjIeFgtIeAPKRMmeKcU4gozilEGucUIo1zKv9S2I504M7NCN1AOoLBoAWD0R4F8gJdBfWu+uaEq7LI/TinEGmcU4g0zingv1hIDQAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ+wejmQjpiYGIvhshQiKDaWEwqRxTmFSOOcQqRxTuUt3AosewJJSSziDwAAAAA4Md0Cbs+eQ1EL3oGAWVxcSYuP358jbkXnjScjVLqBdDzyyvu2dvOOaA8DAAAAiLqaleNs1K3XWkxMgGp3FhG6gXRs2bGL0A0AAADgpDDJAgAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAgLwYuo8dO2ZTpkyxDh062FlnnWUXXXSRPfzww3bgwAFf3/fo0aP21ltvZfvn33nnHWvfvr378x9//GENGjQIPc444wxr27atTZw40Y4fP25+0ftrHFnRq1cvd7zTo/F/99137s+7du2yjz76KM3XAAAAAACZU8Ci6LHHHrMlS5bYgw8+aNWrV7ctW7bY+PHjbdOmTTZ16lTf3nf+/Plu/9dff33E9jlnzhyrXLmyJSYm2u+//27Dhw+30qVL26233mo5hQJ3wYIFM/27SUpKsssvv9z3cQEAAABAXhXVSve7775rQ4YMsfPOO8+qVavmvo4ZM8YWLVpk//nPf3x7X4XJSCtbtqyVL1/eKlWq5D5Hjx49klWKc4LTTjvNihcvHrVjBAAAAAD5TVRDdyAQsH/9618WDAZDzzVv3txVosuUKeNaqKdPn26dOnWyZs2auarxn3/+Gdr2t99+cy3TTZo0scsuu8xmzpyZbP///Oc/7X/+53+sadOmduONN9qqVatci/SIESNs69atrmVa7eHax7hx41ybu1rc1d6+dOlS6969u/tZvfff/va3LF0IKFq0aLLv161bZ/369XOfr3HjxnbTTTfZ+vXr3Wsakz7rrFmz7IILLnDvd/fdd7s2eM8bb7zhxtaiRQt79tlnQ8/r+HTp0iX0/Xvvvec+l7oG5ODBg651X90DKdvLn376aXeB4Nxzz3WVeo+20QURPbw2evnxxx/d70Lj79mzpzuGmaH93XPPPe4Y6/Nrn19//bW9/vrrdv7551vr1q3t1VdfDW2/b98+9/n1WdWqr5/766+/Qq9//vnn1rlzZzeOVq1a2V133eU+p/deQ4cOtdGjR7uf1+ebNm1apsYJAAAAAHkqdPfu3dtee+01F8IUkj755BMXrurVqxdqg1aIuuWWW+zNN9+0w4cP26BBg9zz2k5BuGXLli5o3nvvvS6Mzps3z73+1Vdf2T/+8Q/73//9X/e6gudtt93mQt99993nKtIKfmoJF82P1jxsBVFVebVtmzZt7IMPPrCXXnrJNm/ebC+88EKmPtf27dtdiL366qvd97qo0L9/f6tataq7EKAArTZ0vZ9HgV6f/8UXX3Sf+dNPP032WdR2f+edd7rjsHz58lDgVShdvXq17d+/333/ww8/uIsZP/30U+h7fcaaNWsmG6P2o6D70EMPueD+9ttvh17r27evayvXY+7cuaHn9ZlGjhzpntu7d69rQc+sDz/80EqWLOk+vy6S6LPo+Ov3r4sBjzzyiO3evdttq9+bPs/s2bPd71Sf94EHHnCv6feg7ghdtFAnwZNPPummKITP0ddxLFy4sLtooAsdGqda/gEAAAAgX83pvv32291cblV4FZoURtX+rNDVtWtXt42+XnPNNe7PCogdO3Z0Fe5///vfVq5cORfepFatWi6IKkiqCqpQedVVV7lqtajSqiCvsKjwFxsb69rBPV4VWVRNHzhwoPXp08cFWI3x0ksvtV9++SXdz6L30rYK2LogoJDrjVvfq9KuoFisWDH33LXXXusCdviicgq0p59+uqtUq+KtsKl55wq7qjDrc3nH4cILL3R/1gUKfQ5VoS+++GIXstu1a+dCt95fgVT7SknHWxck9DOiefVXXnml+7N+B0WKFAm1zXsGDBjgquLSrVs39/vKLHUuKCzrGOmzKzDr96xjq2D81FNPuWq8ugw+++wz+/77793vSVTp1mdXh4KOr46TNx9f0xJULV+7dm2yNnpdhNHvWBdsVOlesWKF1a5dO9PjBQAAAIBcH7pF1WA9EhISQi3HCmMKnuIFYVFAU6BSW/aGDRtchVeVa4+qxwpaosqmgq6nUKFCLoilR1Voj0KsQp4qwL/++qtrDV+zZk2ysaSkKnjFihVdKIyPj7fnnnvOhWxV2RW0Ff5VuVb409jV6h4XF5dsH+HV6BIlSoRWP9fnDf8sCrA6Fh5V5BVS1W6t9x42bJhNnjzZvfbtt9+69uuUtE9d9PAovHsXBNJTo0aN0J8ViI8cOWKZpXCswC1eoPeOufe92uk1Lh1DXTgIp+cUytWxoN+ljq+Cth76/XgXOLz38s4D7yKCnyvJAwAAAECOC90KzAqhWuXbC5Kq5mputqrKmuvtBlgg+RAVrGNiYlyI0nzd+++/P839p/y5jKgd2bNz505XYW/UqJGroqqqunjxYlddT0+VKlVc2BNVVBWgVWH+5ptv7JxzznGVYW+euqriCt4vv/xysn0oTKa3mFnKhc3CVyFXi7mq5t78c81zVnjVY+PGjaHqdEop95nRMdNxz6609p3W/vT7VaAPb3f36KKGzhtdwNBx1Oe8+eabbcaMGcm2S2uFdhaGAwAAAJCv5nQrXL3yyiuu4psyeKry6bU1K2R5VOnUXF9VwRVsVc1W0FXA1ePnn392c4RF34f/rN5PQU0LpHkV1/QsWLDA3e7r+eefdy3YCndamCwrwc3bVu+rKrTmbKv1Xe3OCvLbtm3L9P7Ucq5Wc49asHUsPLr4oJb7L774wo1V3QB16tSxZ555xs15T6uCnXKfWlBOC5h5MjpGftHvVb9jvb/3e1V7/qOPPuoq4ZoTfvbZZ9vjjz/uOgk0P1zHglANAAAAICeKWuhWFVnzqDV3+v3333ehT6FZC6opXKnaLQqqWq1aAVoLoKmVWvO31ZKuMKZKtyq6CpxabEzzvEWLc6m1W4tpKZQ9/PDDLpjpfbWyuOZ2qwqcVtuxQqtCsVqzFbbVOq6FzcJXE09Ji4BpLrgeannWwl+qbGtlbu3v0KFDbq6yPqfmaGul9RPtL5xWCtccaM3D1mfVZw5fzVvv07BhQ3ccFbJFX7V4WVrzub196thq0TEFdrX0h1eedYw0R15V/1Opbt26bsxqkdcc+pUrV7q53Dp+pUqVcsdSrf56TRddJkyY4C4eZPZYAgAAAEC+mdOtlaenTp3qVgxXyFVFVq3SmtetOc2iRbcmTZrkXtfiYWPHjnXP63UtkKVFxTT/WmFM98bWquOiaqgCvKq9CsKaC6z3UhVdQVgVVLWzaxG3lLRqtxYkGzx4sKu4aq605oNrVfH0wt11110X+rPGptCr9nH9WfPONX9aY9c8aFXqFZwVdDMTalW91kUDHS+Fe7W+n3HGGcm28VYxV+XX+xktdJZe6NYcaM2j927HpduxhXcG6HWNWRc3vFb/U0VVbS3sptZxtaXrM2jxNO9iiroj9JqmBOj3rHHqNnMAAAAAkNMEknJwX67awe+4445k96EGTpU7Hp5uy9f93/3OAQAAgPzs9BqV7MXRf7OEhIN2/HgwKmMIBMzi4kpafPx+ywkp1htPjr5PNwAAAAAAeVnUbxmG3E1zwr0V6NOiNvvw+5EDAAAAQH6So0P3woULoz0EZEBzyXXrt/R49+AGAAAAgPwoR4du5HzFixd3DwAAAABAaszpBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ9ynG0hH9Url7K+jx6I9DAAAACDqalaOi/YQcq1AUlJSUrQHAQAAAADI2RITg7ZnzyELBqMTIQMBs7i4khYfv99yQor1xpMRKt1AOhISDkZ7CMhDypQpzjmFiOKcQqRxTiHSOKfyHoXtaAXu3IzQDaQjGAxaMBjtUSAv0FVQ7+pwTrgqi9yPcwqRxjmFSOOcAv6LhdQAAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfcMswIB0xMTEWw2UpRFBsLCcUIotzCpHGOYVI45zK3bgvd2QEkpK4cx4AAAAAIDndZ33PnkM5JngHAmZxcSUtPn5/jrj/uzeejFDpBtLx0Oz3bc2WHdEeBgAAAHDK1aoUZ2N7X2sxMYEcE7pzK0I3kI5NO3fZb38QugEAAABkH5MsAAAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QuiOoQYMG7rFt27ZUr82ePdu9NmXKFPf9O++8E9o+5WPJkiWhn9u+fbuNHDnS2rVrZ82aNbPOnTvbvHnzUu1/7969NmHCBGvfvr01bdrULr/8cps+fboFg8Esf449e/bY+eefb3/88Uey59esWWPdu3e3Jk2aWKdOnexf//pXlvcNAAAAAPlJgWgPIK8pWLCgLVy40Hr27Jns+c8++8wCgUCy5ypVqmRz585NtY/SpUu7rxs3brSbbrrJWrRoYZMnT7Zy5crZt99+a6NHj7bdu3db37593XYJCQl2ww03WIUKFWz8+PFWrVo1W758uY0bN862bNlio0aNyvT4Fd779+9vu3btSvb8/v373fsp1Cvc//Of/7Q77rjDPvnkEzcuAAAAAEBqhO4Ia9WqVarQfeDAAVu2bJmdeeaZybaNjY218uXLp7uvsWPHWsOGDV113AvsNWrUsKNHj9qkSZOsW7duVqpUKXv88cetUKFC9tJLL1nhwoXddtWrV7ciRYrYwIED3Vhq166d4dh//PFHu/fee6148eKpXnv33XetWLFiNmbMGDfuwYMH2xdffGErVqywCy+8MEvHCAAAAADyC9rLI6xDhw72/fffu6DtWbx4sQvjaYXZ9OzYscNVtW+++eZUFXKF7WnTprkQrAA+f/5869GjRyhwey6++GLXYl61atVMvefXX39tXbt2DbXAh9Nn0mdT4Pa8/fbbmQrcalNX27yOgyrlzZs3twcffNB+++0369Kli2ubv+2225IdszfeeCO0ba9evVxru2fnzp0u9J999tl21lln2bXXXmtLly5N9l6ffvqpdezY0Ro3buz2rZZ5AAAAADjVCN0RVr9+fatYsaJ9+eWXoecWLFjgAmBWKGQmJSW50JhS0aJFXYgvUKCAbd682Q4dOpTmdgrrrVu3dlXwzLjzzjtdZTw8WHvUpl62bFnXqt6mTRu7/vrrQ0E3s1544QV79tlnXdv7a6+95trThw4d6ir0P//8c6jVXp0CTz/9tHsvVdhbtmxpvXv3dq3vMmzYMEtMTHTBXPPbdbxVgQ83depU1w3w+uuvu1b7V155JUtjBQAAAIBIIHT7QBVhBUdRJfqbb75xz6WkBddUyQ1/PPHEE+61ffv2ua8lS5Y84XtldruTpWCv0Kx2eFXZVWXu16+fW+gtsxTo1S5/1VVXuXngV155pQvwCtXnnXeebdiwwW334osvuuq0KvW1atVyFwNUrX/vvffchQhdwFAgr1u3rtWrV89V+detW5fsvVQJ14JvWlROi74peAMAAADAqcacbh8oYCv0HT9+3LWIq/qd1mJjWvhMFd9wmqMtp512WihUq8KcHm87rwrsF1W/zzjjDPe5RPPTdTFBC6pp4bXM0Dxzj+abh7e963tdoJD169fbxIkTXaXac+TIEbewnKr3WkH9ww8/tJ9++sl+//13N6885SrtNWvWDP25RIkSduzYsZP49AAAAACQPYRuH6hyK2q/1qrll1xySZrbqT08PByGa9SokQuYCpS6XVjKqvPtt9/uFj1TpVdV7pUrV7rKbkoDBgxwc6J1C7CToQp3nTp1kj2nKnRWKt0p29ZjYtJutFDr+H333eeq3+EUnhWutYq6LkZcccUVbt63ArVa1VOuIg8AAAAA0UZ7uQ8UprXAmFrMFy1alOX53KLqtlqvZ8yY4Vqqw2kBM600XrlyZfdeCp8zZ84MVYo9en89VFE/WVrsLHwxM1E7eGYXacsKrbSuheR0QcJ7aI625n2rjfyHH35wC8Spwn7RRRfZf/7zH/dzKY8TAAAAAEQbodvHFvM5c+a4tvLwtuqsGDFihP3yyy82ZMgQ91Wt1C+//LJrvdYCZN79vAcNGuRW/tYca60yrsXV9N7Dhw93C5CpGn6ybrzxRhe6tbL5pk2b3H3DtbjaNddcY5HWp08fd7FBi6Tps+jzfvTRR24Ot9rvVSHXiu1bt261jz/+OLTaesqLDgAAAAAQbbSX+6Rt27ZuTnd2qtweheVZs2a5UKk28YMHD7oW7/Hjx7vFwcJbv2fPnu2208reuj2W7uet+dea/xwJqmhrgTO9txZUUwDWV60cHmmq3MfHx9tTTz3lvuo4PPfcc66dXbRS+TPPPOPmfKsqPnLkSNdqv2rVqhPe9xwAAAAATrVAEj25QJpue3K6/bJhS7SHAQAAAJxy9atVshn3/M0SEg7a8ePJFy2OlkDALC6upMXH77eckGK98WSE9nIAAAAAAHxCe3k+sGvXrgzb3JctW5bt/Z977rknnE+t+ddVqlTJ9v4BAAAAILcidOcDupe3FiXzy9y5c1PdJztcJFZPBwAAAIDciNCdD+j+2OndDzwSsrs6OwAAAADkdczpBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ9ynG0hHzYrl7K+jx6I9DAAAAOCUq1UpLtpDyDMCSUlJSdEeBAAAAAAgZ0lMDNqePYcsGMwZkTEQMIuLK2nx8fstJ6RYbzwZodINpCMh4WC0h4A8pEyZ4pxTiCjOKUQa5xQijXMq91PYzimBOzcjdAPpCAaDFgxGexTIC3QV1LtanBOuyiL345xCpHFOIdI4p4D/YiE1AAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ9wyDEhHTEyMxXBZChEUG8sJhcjinEKkcU4h0jincg/uye2fQFISd84DAAAAgPxM91Tfs+dQjg7egYBZXFxJi4/fnyPu/+6NJyNUuoF0jHv7PVu9bUe0hwEAAAD4qnaFOHvwhi4WExPI0aE7tyJ0A+nY9OcuW0PoBgAAAHASmGQBAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITQnYM0aNDAPbZt25bqtdmzZ7vXpkyZ4r5/5513QtunfCxZsiT0c9u3b7eRI0dau3btrFmzZta5c2ebN29eqv3v3bvXJkyYYO3bt7emTZva5ZdfbtOnT7dgMJjlz7Fnzx47//zz7Y8//kj2/IIFC9x+mzdvbt27d7eVK1daJHz77be2fv36dF/v1atX6LgBAAAAwKlU4JS+GzJUsGBBW7hwofXs2TPZ85999pkFAoFkz1WqVMnmzp2bah+lS5d2Xzdu3Gg33XSTtWjRwiZPnmzlypVzAXX06NG2e/du69u3r9suISHBbrjhBqtQoYKNHz/eqlWrZsuXL7dx48bZli1bbNSoUZkev8J7//79bdeuXcmeX7t2rQ0dOtQeeOABNx4F+ttuu80F8aJFi9rJuPnmm+3VV1+1unXrntR+AAAAACDSCN05TKtWrVKF7gMHDtiyZcvszDPPTLZtbGyslS9fPt19jR071ho2bOiqvF5gr1Gjhh09etQmTZpk3bp1s1KlStnjjz9uhQoVspdeeskKFy7stqtevboVKVLEBg4c6MZSu3btDMf+448/2r333mvFixdP9do333xj9erVc5V2ueuuu2zmzJm2bt06a9y4cRaOEAAAAADkHrSX5zAdOnSw77//3gVtz+LFi10YTyvMpmfHjh2uqq0qcMoKucL2tGnTrFixYi6Az58/33r06BEK3J6LL77YVaSrVq2aqff8+uuvrWvXrmm2cp922mkuYC9dutS1rKs9vkSJEu4iQGaokq3xKKB36dLFBXxRO7z07t079L6qnl922WWunV6V9cTExEy9BwAAAABEGqE7h6lfv75VrFjRvvzyy9BzCpEdO3bM0n7WrFljSUlJaVaR1c6tEF+gQAHbvHmzHTp0KM3tFNZbt27tquCZceedd7rKuCrwKV1xxRV20UUXuXb3s846yx599FF76qmnQq3wJ7Jq1Sq3vdriP/roIzd2vZfCu9der8CtdnkFe72mOeNvv/22HT9+3AV9AAAAAIgGQncOrXarxVxUiVZrtp5LSQuuaVGy8McTTzzhXtu3b5/7WrJkyRO+V2a3O1maN/7nn3/a/fffb2+99ZZdc801NmLEiFRzv9OydetWdwGgSpUqbr65QvXEiRNd6C5btqzbRuFdnQAK2grlqvBrjrfmo2uuOgAAAABEA3O6cyAF7MGDB7sqrVrEVf3WImgpKUy+9tpryZ7THG2vndsL1V4wTYu3nRZA89Njjz3mPofa2EWLtGklc4XkW2+99YQ/27ZtW/eznTp1cvPadXyuu+46V6lPSauYn3HGGckWpgv/HgAAAABOJSrdOVDLli3dV7VFa9XySy65JM3tFDpr1qyZ7FGmTBn3WqNGjVx1eMWKFal+Tu3kffr0sdWrV7s51apyp3f7rgEDBiS7BVl2af9a1M0TExPjvk/r9mhptcPPmTPHZsyYYeecc46bD6553Tt37kxze7XVh1PwBgAAAIBoIHTnQArTF154oWsxX7RoUZbnc4uq223atHFBNWUIVXVZC5FVrlzZvZfmW2slcbWyh9P76xGJ9mztI+W9tH///XfXLp4Rrdz+/PPPu/nlakn/+OOP7ciRI2nO1T799NPd7c48akHXxQUAAAAAiAZCdw6lFmpVd9VWrtt3ZYcC6i+//GJDhgxxXxVyX375ZTcfWvfM9hYxGzRokFstvV+/fm7ldC2upvcePny4WxVct/o6Wddff72byz1v3jzbtGmTazdXlfvaa6/N8Gd167JnnnnGjemPP/5wq62rWt+gQQP3ulZh133A9+/f795H1f3nnnvONmzYYI888kimqukAAAAA4AfmdOdQmsesOd3ZqXJ7FJZnzZrlVvZWm/jBgwetTp06Nn78eDc/2qN7fc+ePdttN2zYMNuzZ49rO9e8cq0CHgmqpuv9VbHW7cw0z1pV+LTmqqekbTXmZ5991t0CTAuq6cKBFkqTXr16udXNdbHgvvvuc4H74Ycfdl91/NQ1AAAAAADREEhK2XsMwLll6iv286Yt0R4GAAAA4KsGVSrZzEG3WkLCQTt+PGg5VSBgFhdX0uLj91tOSLHeeDJCezkAAAAAAD6hvRwZ0r20M2pz12Jn2XXuueemWsQtnOZwq6UcAAAAAHIbQjcypHt5awE0v8ydO9etMp6eSKyeDgAAAADRQOhGhmJjY909wP2S3dXZAQAAACCnY043AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+4T7dQDpqli9nh48di/YwAAAAAF/VrhAX7SHkaYGkpKSkaA8CAAAAABA9iYlB27PnkAWDOTceBgJmcXElLT5+v+WEFOuNJyNUuoF0JCQcjPYQkIeUKVOccwoRxTmFSOOcQqRxTuUuCts5OXDnZoRuIB3BYNCCwWiPAnmBroJ6V5BzwlVZ5H6cU4g0zilEGucU8F8spAYAAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNANAAAAAIBPCN0AAAAAAOSkW4YtXbrUZsyYYZs2bbKpU6fa+++/b1WrVrUrr7wy8iMEoiQmJsZiuCyFCIqN5YRCZHFOIdI4pxBpnFO5A/fozmGh+9NPP7URI0bY9ddfb4sXL7bjx49bgQIFbPjw4bZ371676aab/BkpcIqVKVM82kNAHsM5hUjjnEKkcU4h0jincgfdT33PnkME75wSup9++mkbM2aMderUyd544w33XN++fa18+fL21FNPEbqRZ4z94J/2644d0R4GAAAA4Js6cXH2UOeuFhMTIHTnlNCtlvJmzZqler5Jkya2c+fOSI0LiLqNu3bZ6h3boz0MAAAAALlYlidZ1KtXz7766qtUz7/77rvuNQAAAAAAkM1Kt+Zz9+/f3/71r3/ZsWPH3EJqqn6vWLHCnnvuuazuDgAAAACAPCvLle5WrVrZxx9/bHXr1rX27dvbnj17XLv5hx9+aOedd54/owQAAAAAID9UugcOHGhDhw61IUOG+DMiAAAAAADya6X7p59+crcIAwAAAAAAJ5bl9Kxbgv3973+3G2+80apUqWKFCxdO9vrZZ5+d1V0CAAAAAJAnZTl0P/vss+7r/fffn+q1QCBgv/76a2RGBgAAAABAfgvdq1ev9mckAAAAAADk99C9bdu2E76ulnMAAAAAAJCN0K3bhKmNPCkpyX2vP4ejvRwAAAAAgGyuXv7555/bZ5995r7q8cknn9i0adOsSZMmNnXq1KzuDmEaNGjgHml1E8yePdu9NmXKFPf9O++8E9o+5WPJkiWhn9u+fbuNHDnS2rVr5+6n3rlzZ5s3b16q/e/du9cmTJjgLqo0bdrULr/8cps+fboFg8Esfw7du/3888+3P/74I9nzX3/9tV199dXWvHlzu/nmm23Dhg0WCbrQo1X10zN8+HD3AAAAAIAcX+muWrVqqudq1KhhpUqVsrvvvtsuvPDCSI0tXypYsKAtXLjQevbsmex5XehI2VVQqVIlmzt3bqp9lC5d2n3duHGjW22+RYsWNnnyZCtXrpx9++23Nnr0aNu9e7f17dvXbZeQkGA33HCDVahQwcaPH2/VqlWz5cuX27hx42zLli02atSoTI9f4b1///62a9euZM+vXbvWbrvtNrv11lutU6dObtz/+7//ax9//LEVL17cTsbtt99ud9xxh/ucAAAAAJCTROyG2wqEO3fujNTu8q1WrVqlCt0HDhywZcuW2Zlnnpls29jYWCtfvny6+xo7dqw1bNjQVce9wK4LJEePHrVJkyZZt27d3MWSxx9/3AoVKmQvvfRS6BZw1atXtyJFitjAgQPdWGrXrp3h2H/88Ue799570wzRqtSrwj1kyBD3vS7QLF682N5//313+zkAAAAAyIuyHLqffvrpVM8dPHjQVSzbtGkTqXHlWx06dLBHHnnEBe0SJUq45xROFcYPHz6c6f3s2LHDVbVfeOGFVBVyhe0zzjjDihUr5gL4/Pnz7Z577kl1z/WLL77YtZin1d2QFrWPd+3a1a688kq79NJLk72mirmmIHg0pvr169vPP/+cqdD94Ycfumq9Wu91QeCuu+6yjh07Wq9evWzr1q02YsQI+/77712LvML/gw8+6NrX9RmkaNGimfoMAAAAABDVOd3fffddsoeCzm+//WbXXHONPfTQQxEdXH6kIFqxYkX78ssvQ88tWLDABcysWLNmjVvsrnHjxqleUwBViC9QoIBt3rzZDh06lOZ2CsatW7d2VfDMuPPOO11lXBX4lOLi4lJ1QujCgFrbM6JWdV0UUHu6Lu4o2Ct0a+64qvhqs7/vvvvsH//4h2ub13aaU6656/Xq1XM/AwAAAAC5otKtKqxCTkxM8ryemJjo7uHtzSfGyVW71WJ+xRVXuEr0N998Y/fff79rxQ6nqq9atsP17t3b/v73v9u+ffvc9yVLljzhe2V2u5OlhdkUyK+66iq74IIL3GfRvPFzzz03w59VWD927Jg771R111x0LRinyrwuICjka/x6zJw508qWLeva13XRYNCgQfbFF1/4+tkAAAAAIGKhW4FQIVDBJpxWqtaiXf/+97+zukukcYwHDx5sx48fdy3iqn5rEbSUtPDZa6+9luw5zdGW0047LRSqU/6uwnnbaQE0P2n1dC14phCsCzQK2+qOUBt9RtQKf9FFF1mfPn3c3HIdn+uuuy7NlvF169a5eezhLfWq4melNR8AAAAATmnonjNnTuh2YGpZVntvykq3wl3dunUjNrD8rGXLlu7r0qVL3arll1xySZrbqT28Zs2aab7WqFEjFzxXrFjhAm84tZMrAGvRM7Vfq0K8cuXKZHOuPQMGDHDzptWufbK0r379+tn+/fvdRQQtqpaZ+eL6HM8//7z98ssv7jZ1arefNWuWeyiQp+TdQz58RXhCNwAAAIAcG7p1b2cFF92zWXNnVXEMb0dWKFLVUfN/cfIUpnXrNbWYL1q0yN1mK6tU3dbCdjNmzHDt3OGV37ffftstNla5cmX3XmpjV1u2LqaEz9/W++sxdOjQk/5MH3zwgeuC0LxrBe6//vrLrQmghc8ysn79eneLMV0k0IUBzR3XYm1fffVVqtB9+umnu3ZyVdO9ueW6j3dmF4MDAAAAgFMeuhW4FbxF93DW/ZAV1uAftVBrRW6t1K1Hdujnu3fv7irKt9xyi7tQohD/5JNPuiDtzb9Xy7fatVWF1p81d1qBeOLEiW6OuKrhJ6tWrVpuPGeffbZrl9e+FfpTVuHTopZ53XJM49c9vtVCrhXLvVuoaRV2rVSuhdUUxrW4mu43rlud6aKBOgYI3QAAAACiIcvJWaFJLb5r16511USPFvxatWqVvfjii5EeY77Utm1bN6c7q6uWh1NYVgu2Qqhau3Vrtzp16rhAqvDq0b2+FWq13bBhw1x41f28Na9coT0SzjrrLBszZoyrbGv/5513nmsZTzlNIS0an8b22GOPuWkOqpRr9XIdI9EY9drGjRvdLe10Duq9NGdc56u+pmw5BwAAAIBTIZCUxTTywAMPuFZfVRk1x1arZ+u2U/Hx8S78aJVtIC/oM+NlW7Zlc7SHAQAAAPimYaXK9sYtt1lCwkE7fjxoOVkgoFsRl7T4+P2WE2pq3ngifp/uDz/80FUV33jjDVcNVUVRLctq69VtnQAAAAAAQDbby3WLJ7UKi+bmqtqtxatuu+02NycYec+uXbsybHNftmxZtvffpUsX+/3339N9fdq0adaqVats7x8AAAAAck3o1qJemrtdpUoVF7YVurXqtbrUdSso5D26l/e8efN827/mYZ+oS6JixYq+vTcAAAAA5KjQ3bdvX7v77rvdYly61ZSqlFrJXJVO7/7SyFt066307gceCbqAAwAAAAB5UZZDt24tpds/6TZNdevWdVXKOXPmuJZz3W4KAAAAAAD8n2zdbFu3YZK9e/damzZt3K2bAlq6DQAAAAAAZH/1cs3dfu655+zcc89191reunWrazfXrcJ0r24AAAAAAJDN0P3MM8/Ye++9ZxMmTLBChQq556699lr75ptv7NFHH83q7gAAAAAAyLOyHLrfffdde+CBB+ziiy8OtZSrxfyRRx6xjz76yI8xAgAAAACQP0K37tlcoUKFVM+XKlXKDh06FKlxAQAAAACQ/0J369at7aWXXkr23IEDB2zSpElunjcAAAAAAMjC6uWar60VyzWHe8yYMXbHHXe4lvIjR47YwIEDbdu2be5ey1pgDcgrapUrZ4ePHYv2MAAAAADf1ImLi/YQ8rxAkpYjz0Dz5s3dfO1KlSpZhw4dbO7cubZ69WrbsGGDHT9+3GrXru1uGxYTk+XCOQAAAAAgihITg7ZnzyELBjOMhlEVCJjFxZW0+Pj9lnGKPXXjiUilW/O1tWp5ixYt3C3C5s+fbyVKlLDixYu713fv3u1WNJfOnTuf7NiBHCEh4WC0h4A8pEyZ4pxTiCjOKUQa5xQijXMq91DYzumBOzfLVOjWPbinTJliS5YscSuWv/jii2lWtfUaoRt5RTAYtGAw2qNAXvD/b/TgriLnhKuyyP04pxBpnFOINM4pIIuhWy3lekj79u3t7bfftjJlymTmRwEAAAAAyLcyFbrDLVy40J+RAAAAAACQx7DyGQAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAJBTFlID8gvdFi+NO+MB2RYbywmFyOKcQqRxTiHSOKdyHu7JfeoFkpK4cx4AAAAA5AeJwaDtSTiUK4N3IGAWF1fS4uP354j7v3vjyQiVbiAdYxfMs1//syPawwAAAAAiok7ZOHvo8m4WExPIlaE7tyJ0A+nYuHuXrf5ze7SHAQAAACAXY5IFAAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCdw7SoEED99i2bVuq12bPnu1emzJlivv+nXfeCW2f8rFkyZLQz23fvt1Gjhxp7dq1s2bNmlnnzp1t3rx5qfa/d+9emzBhgrVv396aNm1ql19+uU2fPt2CwWCWP8eePXvs/PPPtz/++CPZ82vWrLHu3btbkyZNrFOnTvavf/3LIuHbb7+19evXp/t6r169QscNAAAAAE6lAqf03ZChggUL2sKFC61nz57Jnv/ss88sEAgke65SpUo2d+7cVPsoXbq0+7px40a76aabrEWLFjZ58mQrV66cC6ijR4+23bt3W9++fd12CQkJdsMNN1iFChVs/PjxVq1aNVu+fLmNGzfOtmzZYqNGjcr0+BXe+/fvb7t27Ur2/P79+937KdQr3P/zn/+0O+64wz755BM3rpNx880326uvvmp169Y9qf0AAAAAQKQRunOYVq1apQrdBw4csGXLltmZZ56ZbNvY2FgrX758uvsaO3asNWzY0FV5vcBeo0YNO3r0qE2aNMm6detmpUqVsscff9wKFSpkL730khUuXNhtV716dStSpIgNHDjQjaV27doZjv3HH3+0e++914oXL57qtXfffdeKFStmY8aMceMePHiwffHFF7ZixQq78MILs3SMAAAAACC3oL08h+nQoYN9//33Lmh7Fi9e7MJ4WmE2PTt27HBVbVWBU1bIFbanTZvmQrAC+Pz5861Hjx6hwO25+OKLXYt51apVM/WeX3/9tXXt2jXNVm59Jn02BW7P22+/nenArUq2xtO4cWPr0qWLC/iiyrn07t079L4LFiywyy67zLXTP/DAA5aYmJip9wAAAACASCN05zD169e3ihUr2pdffhl6TiGyY8eOWdqP5k8nJSW5kJpS0aJFXYgvUKCAbd682Q4dOpTmdgrrrVu3dlXwzLjzzjtdZTw8WHvUpl62bFnXqt6mTRu7/vrrbenSpZna76pVq+zRRx91bfEfffSRG7veS/PNvfZ6BW61r69bt869prnjCvXHjx/P9PsAAAAAQKQRunMgVYTVYi6qRH/zzTfuuZS04Frz5s2TPZ544gn32r59+9zXkiVLnvC9MrvdyVKwf+GFF1w7vKrsZ599tvXr188t9JaRrVu3ugsAVapUcfPNFaonTpzoQreCvDePXZ0ACtoK5arwa463Qr7mqgMAAABANDCnOwdSwNacZ1Vp1SKu6ndai40pTL722mvJntMcbTnttNNCodoLpmnxttMCaH5S9fuMM85wn0s0P10XE7SgmhZeO5G2bdu6Y6AVz/VzOj7XXXedq9SnpFXM9T7hC9OFfw8AAAAApxKV7hyoZcuW7qvaorVq+SWXXJLmdgqdNWvWTPYoU6aMe61Ro0auOqyFytKqOvfp08dWr17tFlZTlXvlypVpvseAAQOS3YIsu1ThrlOnTrLnatWqlalKt9rh58yZYzNmzLBzzjnH3S5N87p37tyZ5vZqqw+n4A0AAAAA0UDozoEUprXAmFrMFy1alOX53KLqtuZOK6imDKFqwdZCZJUrV3bvdcUVV9jMmTNdK3s4vb8ekWjP1qJmmmcebsOGDZlapE0rtz///PNufvmIESPs448/tiNHjqQ5V/v00093tzvzqAVdFxcAAAAAIBoI3TmUWqhV3VVbuW7flR0KqL/88osNGTLEff3999/t5ZdfdvOhhw4dGrqf96BBg9xq6ZpjrVXGtbia3nv48OFuVfB69eqd9Oe58cYbXejWgmebNm1y9w3X4mrXXHNNhj+rW5c988wzbkx//PGHW21d1foGDRq417UK+9q1a929wLVAm6r7zz33nAv1jzzyiJv7DgAAAADRQOjOoTSPWXO6s1Pl9igsz5o1K9Qmfu2119oHH3xg48ePdwuNhbd+z54924X7YcOG2VVXXeUq5Jp/reAdCapov/jii65yr/3rqxZW00rtGdGcbI1ZP3/55Zfb1KlT3YUDLZQmvXr1cqubK9CrxV6BW8G8c+fO9ueff3IfcAAAAABRE0hK2XsMwOnz5ku2bPvmaA8DAAAAiIiG5SvbGz36W0LCQTt+PGi5TSBgFhdX0uLj91tOSLHeeDJCpRsAAAAAAJ9wyzBkaNeuXRm2uWuxs+w699xzUy3iFk6t4rpHNwAAAADkNoRuZEj38p43b55v+587d65bZTw9kVg9HQAAAACigdCNDMXGxroFyvyS3dXZAQAAACCnY043AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+4T7dQDpqlS1nh48fi/YwAAAAgIioUzYu2kPIlwJJSUlJ0R4EAAAAAMB/icGg7Uk4ZMFg7ouBgYBZXFxJi4/fbzkhxXrjyQiVbiAdCQkHoz0E5CFlyhTnnEJEcU4h0jinEGmcUzmTwnZuDNy5GaEbSEcwGLRgMNqjQF6gq6CSmBjMEVdlkftxTiHSOKcQaZxTwH+xkBoAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATbhkGpCMmJsZiuCyFCIqN5YRCZHFOIdI4pxBpnFM5C/fojo5AUhJ3zgMAAACAvC4xGLQ9CYdybfAOBMzi4kpafPz+HHH/d288GaHSDaTj4a/ftTW7t0V7GAAAAMBJq1W6go258DqLiQnk2tCdWxG6gXRs2ven/bZre7SHAQAAACAXY5IFAAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4JM8Ebr37t1rEyZMsPbt21vTpk3t8ssvt+nTp1swGDzpfSclJdnMmTPNLx999JHt2rUrU9v26tXLpkyZYqeKjuc777zj/pyYmGiPPfaYtWnTxpo3b25Dhgyx+Pj4UzYWAAAAAMiNcn3oTkhIsOuuu85WrFhh48ePtw8++MAGDRpkzz//vPv+ZP3www/2wAMPmB+2bt1qd955px0+fNhyuhdeeME+/PBDe/LJJ23OnDnuQsc999wT7WEBAAAAQI5WwHK5xx9/3AoVKmQvvfSSFS5c2D1XvXp1K1KkiA0cONB69uxptWvXPqlKt1/83HekqdI9YsQIO/vss0NV97vuuivawwIAAACAHC1XV7qPHj1q8+fPtx49eoQCt+fiiy92LeZVq1Z1VdlRo0bZ+eefby1btrS7777bPSffffeda6OeNWuWXXDBBdasWTP3uvb9xx9/WO/evd12DRo0cNsOHz7cPa6++mo777zzbOPGjbZu3Trr16+fa7tu3Lix3XTTTbZ+/frQWCZNmmRt27a1Jk2auLC6du1a93yHDh1CX7027ozs2LHDXUjQ+1x//fW2evXq0GsnGseJPqfnjTfesIsuushatGhhzz77bLL3veOOO+ySSy5xf1Y7vKrd55xzTqbG7L333LlzXXu6gvu0adNcF8H//M//uPGqau5NB9DFiGeeecYds1atWln//v1t27ZtEfucAAAAAHCq5OrQvXnzZjt06JALXikFAgFr3bq1q4IrMP766682depUe+WVV1xAU3D2/Oc//7FPPvnEXnzxRTdn+tNPP7V58+ZZ5cqVQ3Oov/76axfy5J///KdrC1cLe40aNVwoVLjX8wquqgpPnDjRbbtgwQJ78803XVu2Wt/j4uJcxVgUXL2vV1xxRaY+87vvvuuCqsanir4+m95PgfVE4zjR55SvvvrKtePrc2m8y5cvd+3vKT311FPu4sVPP/2U7BhmRO/92Wef2WuvvebGqQsRDz30kJuLrz+rdf3zzz93277++uv2/vvvuy4GjaVcuXLWt29fO3bs2El/TgAAAAA4lXJ16N63b5/7WrJkyXS3USX4+++/d6FMlWY99OeFCxfahg0b3DYKcyNHjnTVbFVH9VDojI2NtdKlS7ttypcv7wK8KOSrmqp9/fXXX3bjjTe6AKoA3qhRI7v22mtdNVYUXAsWLGhVqlRxr6vi7oXVsmXLhr6qHT4zOnbs6CrddevWtbFjx7qq8zfffJPhOE70Ob3g36lTJ+vcubOdfvrpLhCn7B6Qa665xlWsVeVXED5w4ECmxq33vvfee61OnTquM0HhWV9ViVZXwhlnnBH6fSgsq/J97rnnus+pOfXqTNCFgZP9nAAAAABwKuXqOd2nnXaa++q1iqdFQa5UqVLJ5nUryClM6zUvsNesWTP0eokSJez48ePp7lNVVk+xYsWse/furpKqxdy0z1WrVrmKtlx55ZWucqsWcgVMheZu3bpl+zMr6IePU59L79muXbsTjsOT3udU9V9h1lOmTBlXSU/J+/lHH33UvaeqyF26dMnU2L39eRcYwo+jnlML+MGDB10L/d///neLifnvNSGFbbXy62LHyXxOAAAAADiVcnXoVqVToXnlypXJwqhnwIAB1rVr1zR/Vi3Jeni8KnZmFjkLrwArJCpEK6QqEF511VUuCL788suhCrluC6Zq9KJFi9yCb2+99Va2251VfQ+nirEq6RmNIzOfM+Vn1n49GvuZZ55pFStWDB0DhWitHp9ZBQokP93CQ7XH+51Mnjw51QJ4ulASic8JAAAAAKdKrg7dCnGaC637aCtchwcttY/rMXToUNeGrmCm1mZRK7LaohXqMgqNmht+Impd1xxizUH2QqXmf3shb/HixW4RMC32pUXKNAdbC4T99ttvqaqzmaGf8+hzqfqrz5XRODKilvLwFmwdn02bNoW+f+SRR1wb92233RZ6Xe+troFIUleC5nD/+eef7niJKuBaKV2Lp+3Zs+ekPicAAAAAnEq5ek636J7cCoAKZAqeWlxN85M151crj9erV8+1QWs+8S+//OIe+rNW0K5fv36G+y9atKj7qlbmI0eOpNnirsXctEiYVjvXe+sigLdatirRasXWgmp6XauUa5+1atUK7VvzzlXBzQyFTVXKdeHgvvvuc23UWjAuo3FkRPPEVZHXvtVqfv/997uWbo/mX6tK/8UXX7jV17UiuDoNdGwj7eabb3YLz+miiYK95mdr4TZdXDjZzwkAAAAAp1KurnR77duzZ892q1QPGzbMVUIVBgcPHuzm/npV2gcffNCFObVna361t4J4RrQYl25zpfnOWmU7Ja1ofvvtt7tFzRTKtb0C6z/+8Q/buXOna4HWWB5++GFXvVVw1O24vAXadOsxrRiusWt8GdEtx7SQ2bhx49x7P/30064an9E4MqJbc2mMCru7d+92nQNa3Cw8dB8+fNjGjBnjXtcxee6559JsET9ZuoCiixAavy6onHXWWS7w65id7OcEAAAAgFMpkERfLpCm/h++YL/s3BztYQAAAAAnrX65yjb96tstIeGgHT8etNwoEDCLiytp8fH7LSekWG88eb69HAAAAACAnCrXt5fnFePHj3dt4+nRAmb9+/e3nEb3Cddt0E5k2bJlp2w8AAAAAJCTELpzCN3eTIuZpcebA57TaGGz7N7+DAAAAADyOkJ3DlG2bFn3yG20MJ1WUAcAAAAApMacbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJwX82jGQ29UsVd7+On4s2sMAAAAATlqt0hWiPYR8K5CUlJQU7UEAAAAAAPyVGAzanoRDFgzmzggYCJjFxZW0+Pj9lhNSrDeejFDpBtKRkHAw2kNAHlKmTHHOKUQU5xQijXMKkcY5lfMobOfWwJ2bEbqBdASDQQsGoz0K5AW6CiqJicEccVUWuR/nFCKNcwqRxjkF/BcLqQEAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATVi8H0hETE2MxXJZCBMXGckIhsjinEGmcU4g0zqmcgVuFRVcgKYlF/AEAAAAgr0oMBm1PwqFcH7wDAbO4uJIWH78/R9yKzhtPRqh0A+l4eukcW793W7SHAQAAAGRb9ZIV7K6zu1tMTCDXh+7citANpOOPA/G2YQ+hGwAAAED2MckCAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhOwfZu3evTZgwwdq3b29Nmza1yy+/3KZPn27BYNC93qBBA/vuu++iPUwAAAAAQCYVyOyG8FdCQoLdcMMNVqFCBRs/frxVq1bNli9fbuPGjbMtW7bYqFGjoj1EAAAAAEAWEbpziMcff9wKFSpkL730khUuXNg9V716dStSpIgNHDjQevbsGe0hAgAAAACyiPbyHODo0aM2f/5869GjRyhwey6++GLXYl61alX3/Y8//midOnWyxo0buyC+detW97zaztWWPnr0aGvZsqW98MIL7vl33nnHtak3adLEunTpYj/88ENo39p+7ty51rVrV/d637593f4GDRrk2tuvueYaW7t2bWj7OXPm2P/8z//YWWedZeeee66NHTvWEhMTM/UZs/pe+pwar7bV5/3kk0+SHa+HH37YLrjgAmvUqJHb95tvvpnsvWbOnGnXX3+9O07a94oVK7LxmwEAAACAk0PozgE2b95shw4dcgExpUAgYK1bt3ZVcC/4jhw50gVYzQF/7LHHQtsqxCqQKmhfddVV7qva02+77TabN2+enX/++Xbrrbfazp07Qz/z5JNP2tChQ23WrFm2atUqu/baa9122n/RokVt0qRJbrvvv//eHnzwQbvrrrvs448/doFb23z++eeZ/pyZfa8///zTjVmh+/3337dbbrnFhg8f7oK46ILC4sWLbcqUKW4snTt3dp8zPj4+9F56TZ/1vffes5IlS7qxAwAAAMCpRujOAfbt2+e+KhxmZMCAAa7KrEXVunXrZqtXr072ugJqzZo1rUqVKvbaa69Zr169XCitU6eODRs2zOrXr2+vv/56aHsFWwVfVa8V7k8//XTr3r27+3r11Vfbhg0b3HbFihVzc80vvfRSN99cFe8zzzwzWXU6I5l9L1WptZ0q+fosqlRrvvuMGTPc6w0bNnRjadasmWvB79+/vx07dsw2btwYei8F+o4dO1rt2rWtT58+VLoBAAAARAVzunOA0047zX1V5TojNWrUCP1ZIf3IkSPJXlcg9qxfv95uv/32ZK8rqOp5j0KrR/PHvTZ273uFWVFQ1vdPPfWUrVu3ztasWWObNm2ytm3bZvpzZva9FL4XLVpkzZs3D72u1xSgRWH6m2++cSu9a1tVzSW81b1WrVqhP5coUSK0bwAAAAA4lah05wAK0grQK1euTLe6vWTJEvfnmJgT/8rC54SnnB/uBVPvFmQSGxub7PX09v/VV1+5SrVauDWXWuG7RYsWlhWZfa/jx4+7edxqifcemvM+depU9/oTTzxhd999txUoUMBV8cPnc3sKFiyYpbEBAAAAgB8I3TmAwuMVV1zh2qo1JzvcwoUL3UO3EssqVYb//e9/J3tO33sV46zQXHItgvbAAw/YddddZ3Xr1nVz0ZOSkizSND5V0dVa7j00d1zzu+WNN95wt1BTu7yO2+HDh93zfowFAAAAAE4GoTuH0CreBw4csH79+rlFyxRoFXS1gFjv3r2tXr16Wd7nzTff7OZvq1L8+++/u0XXNAdcc8Gz0wK/bNky11auedwalxY8S3mRIBJuuukmNwdbFW3N01bY1iJrmqfujUXt57p/uRZXu+eee9zzfowFAAAAAE4Gc7pziPLly9vs2bPdqtuq4O7Zs8e1nQ8ePNgtNpYdqgKrHVyt4ArIZ5xxhr388suuSp1Vd9xxh40YMcItaKY50hdeeKEb16+//mqRprneaiXXRQLdt7xixYou5GuxNXnooYdszJgxduWVV7rXVHlX67rG0q5du4iPBwAAAACyK5BETy6QpuFfPGe/7vrviugAAABAblPntCr2RPshlpBw0I4f/+/aTrlRIGAWF1fS4uP3W05Isd54MkJ7OQAAAAAAPqG9HCdNtyXzVldPy9ixY0Ot4QAAAACQnxC6cdJGjx4dWkE8LeXKlTul4wEAAACAnILQjZOWnduZAQAAAEB+wJxuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnBfzaMZDbVSsRZ0cSj0Z7GAAAAEC2VS9ZIdpDyPcCSUlJSdEeBAAAAADAH4nBoO1JOGTBYO6OfoGAWVxcSYuP3285IcV648kIlW4gHQkJB6M9BOQhZcoU55xCRHFOIdI4pxBpnFM5h8J2bg/cuRmhG0hHMBi0YDDao0BeoKugkpgYzBFXZZH7cU4h0jinEGmcU8B/sZAaAAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPmH1ciAdMTExFsNlKURQbCwnFCKLcwqRxjmFSOOc8h+3A8v5AklJLOIPAAAAALlRYjDR9iQczhfBOxAwi4srafHx+3PErei88WSESjeQjlmrZtuWA39EexgAAABAmioVq2j/e1Zvi4kJ5IvQnVsRuoF07Dz0H/tjP6EbAAAAQPYxyQIAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELqRTPv27a1BgwahR8OGDe2cc86xAQMG2Pbt25Nt88MPP6T6+S+//NK9Nnz48NBzmzZtskGDBtnZZ59tTZs2ta5du9oHH3wQkfEePXrU3nrrrXRf/+OPP9x49BUAAAAATjVCN1K577777Ouvv3aPL774wp544glbu3at3XvvvaFtChYsaAsXLkz1s5999pkFAoHQ94cPH7bevXtbuXLlbObMmfbee+9Zly5d3L4++eSTkx7r/PnzberUqSe9HwAAAADwA6EbqZQsWdLKly/vHhUrVrQ2bdrY4MGD7bvvvrP9+/e7bVq1apUqdCclJbnnmjVrFnpuyZIldujQIRszZozVr1/fatasaT169LDOnTufsEKdWXpPAAAAAMipCN3IlEKFCrmvMTH/d8pcdNFFrmV7/fr1oW1+/vlnK126tNWqVSv0nLY/ePCgey3c0KFD7cEHH8zUe+/bt8+1pyvoq0V92LBhduDAAXcRYMSIEbZ169ZQC/mxY8ds3Lhxbtt27dq5Sj0AAAAARAuhGxnavHmzvfDCC3bBBRdY8eLF3XOlSpWyli1bJqt2L1iwwDp27JjsZ88//3yrXbu23Xjjjda9e3d7+umn7d///reVLVvWKleunKn3f+qpp+zPP/+02bNn26uvvmqrV6+2Z5991po3b+5a4StVquRa4bW/KVOm2KJFi+y5556zyZMnu+0BAAAAIFoI3Uhl9OjRLtDq0bhxY9cKXrduXZs4cWKy7Tp06JAsdH/++eepQnfhwoVt1qxZ1qdPH9uxY4cLxddff71de+21tnHjxkyNR5Vshf1q1arZGWec4cK0FmNT9V2t8LGxsa4VXlX1OXPmuFZ4VcS9UA4AAAAA0ULoRioKrfPmzXMLn7Vt29aFXbWDlylTJlXoVtV69+7dtm7dOjty5IgL6Smp5VwLp6kC/f7779udd97pgrTeJzO0ENtPP/1k5513nltFffny5cla2D0JCQluLArmnrTGAwAAAACnCqEbqWilcS14duaZZ7qqsgwcONDNlw6nMF6vXj1bvHixW7U8ZZVbtFjahx9+GPpei6kpOD/22GO2Zs0aF5IzorCtudmqwKu6ff/99ydbSf1Ei6tplXUAAAAAiBZCN05IIVcLnv366682ffr0VK+r2q3QnVZrufz22282bdo0CwaDyZ7XnHDtu0SJEhmOQe+7cuVK15KuiwAPP/ywffrpp+618NuTqRIfFxfnKuGeVatWZfkzAwAAAECkELqRoSZNmli3bt3c4mU7d+5MFbq/+uor27Jli5tHnVZruF674447bOnSpe7PajMfNWqUu3WYtyr6iWgu+AMPPOBWQNc8cN3fW1V4KVq0qO3du9c9n5iY6Paphdd0qzKFbwV0AAAAAIiWAlF7Z+Qqf//7313YTbmY2llnneWq1moB14JmKdWoUcOtOq4KtYK37vNdpUoVF+L79euXqfceMmSI+zm1peue3wr33jhat27tWuE7derkFmzr37+/HT582I1X47n99ttdYAcAAACAaAgkhU+ABRDyxI+TbcPeDdEeBgAAAJCmaiWr2b3n3G0JCQft+PHk0znzokDALC6upMXH77eckGK98WSE9nIAAAAAAHxCezmiSi3rw4cPT/f1li1b2osvvnhKxwQAAAAAkULoRlTpPuC6J3h6ihQpckrHAwAAAACRROhGVBUvXtw9AAAAACAvYk43AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE0I3AAAAAAA+4T7dQDoqFqtgR4NHoz0MAAAAIE2VilWM9hCQCYGkpKSkzGwIAAAAAMhZEoOJtifhsAWDeT/WBQJmcXElLT5+v+WEFOuNJyNUuoF0JCQcjPYQkIeUKVOccwoRxTmFSOOcQqRxTp0aCtv5IXDnZoRuIB3BYNCCwWiPAnmBroJKYmIwR1yVRe7HOYVI45xCpHFOAf/FQmoAAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNXLgXTExMRYDJelEEGxsZxQiCzOKUQa5xQijXPKX9wuLHcIJCWxiD8AAAAA5DbBYKIlJBzON8E7EDCLiytp8fH7c8St6LzxZIRKN5COD9e+aDsPbIz2MAAAAIBUyhWralc3GGAxMYF8E7pzK0I3kI7dh7bbzoOboj0MAAAAALkYkywAAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaE7j2nfvr01aNAg9GjYsKGdc845NmDAANu+fXuybX744YdUP//ll1+614YPHx56btOmTTZo0CA7++yzrWnTpta1a1f74IMPTunnAgAAAIDciNCdB91333329ddfu8cXX3xhTzzxhK1du9buvffe0DYFCxa0hQsXpvrZzz77zAKBQOj7w4cPW+/eva1cuXI2c+ZMe++996xLly5uX5988skp+0wAAAAAkBsRuvOgkiVLWvny5d2jYsWK1qZNGxs8eLB99913tn//frdNq1atUoXupKQk91yzZs1Czy1ZssQOHTpkY8aMsfr161vNmjWtR48e1rlzZ3vrrbdO+WcDAAAAgNykQLQHgFOjUKFC7mtMzP9dZ7nooots4sSJtn79eqtbt6577ueff7bSpUtbrVq1Qj+n7Q8ePOheCw/jQ4cOtSNHjmTqvadMmWJbtmxxFwPeeecdK1OmjD3wwAO2ceNGe/bZZy0YDNrAgQNdRV327dtn48aNs88//9yKFStml112md19991WpEgR97qe1z419sKFC1u7du3c9sWLF3fPa78lSpSw999/373et29f+9vf/hbBowkAAAAAmUOlOx/YvHmzvfDCC3bBBRe4YCqlSpWyli1bJqt2L1iwwDp27JjsZ88//3yrXbu23Xjjjda9e3d7+umn7d///reVLVvWKleunOkxfPjhhy50//Of/7QmTZrYnXfe6drfX3vtNevVq5c98sgjtnv3brftP/7xD1eRnz17tgvly5cvdyHd+yxDhgyxm266yT766CN78sknXTU+vOqutneF7Xfffdf69etnjz32mP3+++8nfRwBAAAAIKsI3XnQ6NGjrXnz5u7RuHFj1wquarYq2+E6dOiQLHSrgpwydCu8zpo1y/r06WM7duxwleTrr7/err32WldRzixVtxWWa9So4X5WoVrhWuNSMD5+/LhbsE2hWvPKNVYt6KaAriq2ArR+RlXxkSNHujFUq1bN2rZt6y4MaM6657TTTnNzztUKf8stt7jvV6xYcVLHFAAAAACyg/byPEjzty+99FLXFq6QvHXrVtcOruCbMnR7FWY91C6ukJ6SWs4VYvX47bffXDh/5ZVX3PtoYbXMUED2Fmjz2sSrVq2a7PujR4+6lnEFa7WMh9NzCuVnnXWWa5V/7rnnXNDWY926dXbNNdcke6/Y2NjQ96ruK9QDAAAAwKlG6M6DtNK4qrwyefJk69atm5sz/eabb7pVy8PDab169Wzx4sX2n//8J1WVW9S2rfnRV1xxhftei6np0ahRIzdPWmFdreYZKVAg9anmzS8Pl5iY6NrQ33777VSvaVG41atXuzZ33fZMi8HdfPPNNmPGjGTbhX/G8EXiAAAAAOBUo708j1NV+MEHH7Rff/3Vpk+fnup1VbsVutNqLRdVtqdNm+YqzeE0J1z7ViCPJM0fVxu5quK6cKDHX3/9ZY8++qirhGtOuO4X/vjjj7t53Wo/VwWcUA0AAAAgJyJ05wMKpqp2a1GynTt3pgrdX331lVtdXGE2Ja0ortfuuOMOW7p0qfvzokWLbNSoUe7WYd6q6JGiOd5a8G3YsGH2yy+/2MqVK23EiBHutmUK+pqfvWbNGveaFkebMGGCW2hNgRwAAAAAchpCdz7x97//3bVdp1xMTXOkFWZ1C7HwedAeLXymVcTVCq7gffnll9vDDz9snTp1crfx8oOq2mp9V+u4FnBT9XvSpEnuNa10rluX6TVVurdt22a33367rVq1ypexAAAAAMDJCCTRlwuk6fV/j7M/9v8W7WEAAAAAqVQsXtP6NH/QEhIO2vHjyaeC5lWBgFlcXEmLj99vOSHFeuPJCJVuAAAAAAB8wurlOCmffPKJDR8+PN3XW7ZsaS+++OIpHRMAAAAA5BSEbpyUtm3b2rx589J93bsHNwAAAADkR4RunJTixYu7BwAAAAAgNeZ0AwAAAADgE0I3AAAAAAA+IXQDAAAAAOATQjcAAAAAAD4hdAMAAAAA4BNCNwAAAAAAPiF0AwAAAADgE+7TDaSjbLHKdix4JNrDAAAAAFIpV6xqtIeATCJ0A+m44vRboj0EAAAAIF3BYKIFg0nRHgYyQOgG0pGQcDDaQ0AeUqZMcc4pRBTnFCKNcwqRxjnlPwVuQnfOR+gG0hEMBi0YjPYokBcEAv/3NTExaEn8fxERwDmFSOOcQqRxTgH/xUJqAAAAAAD4hNANAAAAAIBPCN0AAAAAAPiE0A0AAAAAgE8I3QAAAAAA+ITVy4F0xMTEWAyXpRBBsbGcUIgszilEGucUIo1zKnK4PVjuFUhKYhF/AAAAAMjJgsFES0g4nK+DdyBgFhdX0uLj9+eIW9F548kIlW4gHd+un2QJB9dHexgAAADI50oVrW5tTx9uMTGBfB26cytCN5CO/Ye32u5D66I9DAAAAAC5GJMsAAAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhOwdp0KCBe2zbti3Va7Nnz3avTZkyxX3/zjvvhLZP+ViyZEno57Zv324jR460du3aWbNmzaxz5842b968VPvfu3evTZgwwdq3b29Nmza1yy+/3KZPn27BYDDLn2PPnj12/vnn2x9//JHs+QEDBqQa66JFi+xkffvtt7Z+/fp0X+/Vq1fouAEAAADAqVTglL4bMlSwYEFbuHCh9ezZM9nzn332mQUCgWTPVapUyebOnZtqH6VLl3ZfN27caDfddJO1aNHCJk+ebOXKlXMBdfTo0bZ7927r27ev2y4hIcFuuOEGq1Chgo0fP96qVatmy5cvt3HjxtmWLVts1KhRmR6/wnv//v1t165dqV5TMJ44caKdd955qcZ6Mm6++WZ79dVXrW7duie9LwAAAACIJEJ3DtOqVatUofvAgQO2bNkyO/PMM5NtGxsba+XLl093X2PHjrWGDRu6Kq8X2GvUqGFHjx61SZMmWbdu3axUqVL2+OOPW6FCheyll16ywoULu+2qV69uRYoUsYEDB7qx1K5dO8Ox//jjj3bvvfda8eLFU72m91Tlu3HjxiccMwAAAADkJbSX5zAdOnSw77//3gVtz+LFi10YTyvMpmfHjh2uqq0qcMoKucL2tGnTrFixYi4Mz58/33r06BEK3J6LL77YtZhXrVo1U+/59ddfW9euXdNs5d6wYYMbh8J8dqiSrfEotHfp0sUFfFE7vPTu3Tv0vgsWLLDLLrvMtdM/8MADlpiYmK33BAAAAICTRejOYerXr28VK1a0L7/8MvScQmTHjh2ztJ81a9ZYUlKSC6kpFS1a1IX4AgUK2ObNm+3QoUNpbqeQ3Lp1a1cFz4w777zTVcZVgU8rdJcoUcLuuecea9u2rQv+X3zxRab2u2rVKnv00UddW/xHH33kxq730nxzr71egVvt8uvWrXOvde/e3d5++207fvy4LV26NFPvAwAAAACRRujOodVutZiLKtHffPONey4lLbjWvHnzZI8nnnjCvbZv3z73tWTJkid8r8xud7IUuv/66y8XuF988UW78MIL3cJqmjueka1bt7oLAFWqVHHzzRWqNTdcobts2bKhueHqBFDQVihXhV9zvDUfXXPVAQAAACAamNOdAylgDx482FVp1SKu6rcWQUtJYfK1115L9pzmaMtpp50WCtVeME2Lt50WQPOTKuBaRdxbOE1zzVeuXGlvvfVWmlX2cArqOgadOnVy89p1fK677jpXqU9rsbYzzjgj2cJ04d8DAAAAwKlEpTsHatmypfuqtmitWn7JJZekuZ1CZ82aNZM9ypQp415r1KiRqw6vWLEi1c+pnbxPnz62evVqt7CaqtwKwGlRNTr8FmTZFRMTk2ql8jp16tjOnTsz/Fm1w8+ZM8dmzJhh55xzjrtdmuZ1p/ezaqsPp+ANAAAAANFA6M6BFKbVfq0Wc93HOqvzuUXV7TZt2rigmjKEqgVbC5FVrlzZvdcVV1xhM2fOdK3s4fT+ekSiPXv48OE2YsSIZM8p9Ct4Z0Qrtz///PNufrn28fHHH9uRI0fSnKt9+umnJ2tZVwu63gcAAAAAooHQnUOphVrVXbWVZ3fFbwXUX375xYYMGeK+/v777/byyy+7+dBDhw4NVZ4HDRrkVkvv16+fWzldi6vpvRWUtSp4vXr1TvrzaJXx999/3+bNm2ebNm2yp59+2oXmlPcjT4tuXfbMM8+4Mem2Y1ptXdX6Bg0auNe1CvvatWtt//79dv3117vq/nPPPefmkT/yyCNu7jsAAAAARANzunMozWPWnO7sVLk9CsuzZs1yK3urTfzgwYOusjx+/Hg3P9qj+2bPnj3bbTds2DDbs2ePazvXvHKtAh4Jl156qVt9XGFYIVgVaS2opoXRMqI52Rrzs88+624BpgXVdOFAC6WJ5oprdXNdLLjvvvvcezz88MPuq46fugYAAAAAIBoCSSl7jwE4n64Yav85kHpOPAAAAHAqlS1Wz65o8owlJBy048eDll8FAmZxcSUtPn6/5YQU640nI7SXAwAAAADgE9rLkaFdu3Zl2Oauxc6y69xzz021iFs4zeFWSzkAAAAA5DaEbmRI9/LWAmh+mTt3rltlPD2RWD0dAAAAAKKB0I0MxcbGunuA+yW7q7MDAAAAQE7HnG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHzCfbqBdJQsWtWOB/+K9jAAAACQz5UqWj3aQ8BJIHQD6Tiv7l3RHgIAAADgBIOJFgwmRXsYyAZCN5COhISD0R4C8pAyZYpzTiGiOKcQaZxTiDTOqchS4CZ0506EbiAdwWDQgsFojwJ5QSDwf18TE4OWxP8rEQGcU4g0zilEGucU8F8spAYAAAAAgE8I3QAAAAAA+ITQDQAAAACATwjdAAAAAAD4hNANAAAAAIBPWL0cSEdMTIzFcFkKERQbywmFyOKcQqRxTiHSOKcig9uF5W6BpCQW8QcAAACAnCoYTLSEhMP5PngHAmZxcSUtPn5/jrgVnTeejFDpBtLx24ZxduDQ6mgPAwAAAPlYsaK1rWHdBy0mJpDvQ3duRegG0nHor0124NCaaA8DAAAAQC7GJAsAAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8QugGAAAAAMAnhG4AAAAAAHxC6AYAAAAAwCeEbgAAAAAAfELoBgAAAADAJ4RuAAAAAAB8Qug+hRo0aOAe27ZtS/Xa7Nmz3WtTpkxx37/zzjuh7VM+lixZEvq57du328iRI61du3bWrFkz69y5s82bNy/V/vfu3WsTJkyw9u3bW9OmTe3yyy+36dOnWzAYzPT4tX3KsTzyyCPZPh4AAAAAkNcViPYA8puCBQvawoULrWfPnsme/+yzzywQCCR7rlKlSjZ37txU+yhdurT7unHjRrvpppusRYsWNnnyZCtXrpx9++23Nnr0aNu9e7f17dvXbZeQkGA33HCDVahQwcaPH2/VqlWz5cuX27hx42zLli02atSoTI193bp17v0GDhwYeq5o0aLZOg4AAAAAkB8Quk+xVq1apQrdBw4csGXLltmZZ56ZbNvY2FgrX758uvsaO3asNWzY0FXHvcBeo0YNO3r0qE2aNMm6detmpUqVsscff9wKFSpkL730khUuXNhtV716dStSpIgL0BpL7dq1Mxz7+vXrXSX9RGMCAAAAAPwX7eWnWIcOHez77793QduzePFiF8aLFy+e6f3s2LHDVbVvvvnmVBVyhe1p06ZZsWLFXACfP3++9ejRIxS4PRdffLFrGa9atWqm3nPDhg1Wq1Ytyw61tatq37VrV2vSpImrwm/dutUGDRrk2t2vueYaW7t2bWj7H3/80bp06eK27dSpk33yySeh1/SZHn74YbvgggusUaNGbt9vvvlmsveaOXOmXX/99da4cWO37xUrVmRr3AAAAABwMgjdp1j9+vWtYsWK9uWXX4aeW7BggXXs2DFL+1mzZo0lJSW5UJmSWr4V4gsUKGCbN2+2Q4cOpbmdwnrr1q1dFTwj8fHxtmfPHnv33XddqNWccFXONYbMevLJJ23o0KE2a9YsW7VqlV177bV2/vnnuzCuMas6L3/++afddtttLnS///77dsstt9jw4cNdEJcXXnjBXahQhf/jjz921Xe1ymuMHr1266232nvvvWclS5a0Bx98MNPjBAAAAIBIIXRHqdqtFnOvavvNN9+451LSgmvNmzdP9njiiSfca/v27XNfFShPJLPbZabKLZo3/txzz7lQrK8zZszI9D4UohWyzzrrLBf2Tz/9dOvevbv7evXVV4feQ1Vqbae295o1a7pKteake++llnrNTdfCcWqT79+/vx07dszNcfco0OtChtrm+/TpQ6UbAAAAQFQwpzsKFLAHDx5sx48fdy3iqn4rzKakhc9ee+21ZM9pjracdtppoVBdtmzZdN/L206rl5+Mc845x/71r39ZmTJl3PdauVyLtWnVdbW4Z4YCskfzycPb2vW9grMofC9atMhdZPDoNW/eucK0LlRoNXZtq6q5JCYmhrYPb4MvUaJEaN8AAAAAcCpR6Y6Cli1buq9Lly51q5ZfcsklaW6n9nBVesMfXujVXGa1h6dVwVU7uaq7q1evdgurqcq9cuXKNN9jwIAByW5BdiLee3vq1q1rO3futMzSwnDhYmLSPv10MULzuHXrM++heelTp051r6vaf/fdd7vjo9by8Pnc4avEAwAAAEC0EbqjQGHxwgsvdC3mquhmdT63qLrdpk0b13Kdcl7122+/7eY/V65c2b3XFVdc4Vq21coeTu+vhyrqGZkzZ45ddtllyd7r119/tTp16likqaK9adOmZBcbPv/8cze/W9544w13m7Nhw4a5z3b48GH3fFbmlwMAAADAqUDojmKLuYKs2srD266zYsSIEfbLL7/YkCFD3Nfff//dXn75ZZs4caJbsMy7n7dWCNdq6f369XMrp2txNb23Fifr3bu31atXL8P30hxrLXD2yCOPuECsyrNWSNciZ5Gme4Grgq+KtuZpK2xrkbUqVaqEWuZ1sUL3GNfFhXvuucc9n/KiAgAAAABEG3O6o6Rt27aujTo7VW6PwrJWAtdK3WoTP3jwoKs8a5ExtWd7dF9tzb3WdqoOaxVytZ1rXrkWMssMzb/WquEK9NqXLhZ4leZI03uplfyxxx5zK6RrtXddINBia/LQQw/ZmDFj7Morr3SvXXfdda51XZX3du3aRXw8AAAAAJBdgSR6coE0/bzqFtt34OdoDwMAAAD5WIliDazFWTMtIeGgHT8etPwsEDCLiytp8fH7LSekWG88GaG9HAAAAAAAn9BeDtu1a1eGbe7Lli1L97Xbb7/9hCugjx07NtQaDgAAAAD5CaEbbmEy3ZYru0aPHh1aQTwtad2DHAAAAADyA0I33CJkui1XdmXmlmMAAAAAkB8xpxsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoBAAAAAPAJoRsAAAAAAJ9wn24gHcWK1LRg8HC0hwEAAIB8rFjR2tEeAk4SoRtIR/06o6I9BAAAAMCCwUQLBpOiPQxkE6EbSEdCwsFoDwF5SJkyxTmnEFGcU4g0zilEGudU5ChwE7pzL0I3kI5gMGjBYLRHgbwgEPi/r4mJQUvi/5eIAM4pRBrnFCKNcwr4LxZSAwAAAADAJ1S6gRNcofWu0gInwzuPOJ8QKZxTiDTOKUQa5xT8kNPOq8yOI5CURMMHAAAAAAB+oL0cAAAAAACfELoBAAAAAPAJoRsAAAAAAJ8QugEAAAAA8AmhGwAAAAAAnxC6AQAAAADwCaEbAAAAAACfELoB4P+1dyewUdRfAMdfuYkoyCEo4IHRSrCliBCUVqByxoAVqAFFToEqRiqmrYgCUVBAPCIWBBMvQqTUEkANBEWRoJGKCAoIpHhjpWDxCKVU6PzzXjKbbQGh/+xud2e+n2TZnfnNHkNed39vfhcAAAAQJiTd8IWTJ0/K448/LjfffLMkJyfL66+/fs5j9+7dK+np6dK5c2cZNmyY7N69u0r5+++/L3379rXyKVOmSGlpaQTOAF6NKcdxZNmyZZKamio33XSTjBkzRoqKiiJ0FvDq95Rr/fr1Eh8fH8ZPDb/E1IYNG2TAgAGSlJQk48ePl0OHDkXgDOD1379FixbJbbfdJt26dZPMzEzqVD51sgYx5dq+fbvcfvvtZ+yP1no6STd8YcGCBfZF/9Zbb8msWbPklVdesQpEdWVlZTJp0iT7o1+9erV06dJFJk+ebPvVN998IzNmzJCHHnpI8vLy5O+//5bp06fXwhnBKzG1cuVK+3F58sknpaCgQNq1aycTJ06UEydO1MJZwQsx5dLvp7lz50bwDODVmNqxY4c8+uijMm7cOCtv0KCBTJs2rRbOCF6KK61Hvfvuu7Jw4UJZsWKFlJSUWB0L/rPgAmPKtX//fpk6dapduAkW1fV0B/C448ePOwkJCc4XX3wR2Jebm+uMGjXqjGPz8/Od1NRUp7Ky0rb1vl+/fk5BQYFtZ2VlOTk5OYHjf/vtNyc+Pt75+eefI3Iu8F5MpaenO0uXLg0cX1FR4SQlJTlbt26NyLnAezHlmjFjhjNixAjn+uuvj8AZwMsxNWXKFOexxx4LHK+/eX369HH++OOPiJwLvBlXGRkZzrx58wLHb9q0yX7/4C/HaxBT6p133rE4GTx4sH0PBYvmejot3fC8ffv2yalTp+wKq6tr166ya9cuqaysrHKs7tOyuLg429Z77fK7c+fOQLlesXVdfvnlcsUVV9h++EcoYyo7O1uGDBkSOF7L9crtP//8E7HzgbdiShUWFtotIyMjgmcBr8aUxlK/fv0Cx7dv314+/vhjad68ecTOB96Lq2bNmsnmzZvl8OHDUl5eLh988IF07NgxwmeEWIoptWXLFpk/f76MHTtWqovmejpJNzzvyJEjcumll1p3OFfLli1t/Miff/55xrGXXXZZlX0tWrSQ33//3R5r16f/Koc/hDKm9MehTZs2gbL8/Hz78dEfHPhHKGOqoqLChivMnDlTGjVqFKEzgFdjSrtn/vXXX3L69GmZMGGC9OzZUx544AFLlOA/ofyu0vG29erVszHdmozrGN0XXnghQmeCWIwptXjxYunfv7+cTTTX00m64Xk6Njb4D1m521o5vZBj3eP0Sux/lcMfQhlTwfRKrF691Yptq1atwvLZ4f2Yys3NlU6dOtlkNPCvUMWUO/52zpw5MnjwYFmyZInt17G5Z2uFgreF8rtKJ+PTC4OvvvqqLF++3C5A62Ra8JcTNYip84nmejpJNzyvYcOGZ/yxudvVW4HOdax73LnKGzduHKZPD6/HlOvrr7+2ZFuv+OvkIPCXUMXUgQMHZNWqVVRcEbKYqlu3rm3rDNRpaWmSmJhoE19prAUPaYA/hCqudBhVTk6OTc7Xp08f69310ksvyeeffx4VXYERnTF1PtFcTyfphue1bt1ajh07Zl12g7uy6B/yJZdccsaxR48erbJPt92uKucqp1XSX0IZU2rbtm22BE+PHj3k+eeflzp1+Gr2m1DF1MaNG60rsI6/1fFxOhO+0sfr1q2L0NnASzGl3T7r168vHTp0CJTpPh2PGw1dNhGbcaXLOBUXF1dZ0lDH32pssRydv7SuQUxdyGtFaz2dmh08Tyfl0DFDwVfkv/rqK0lISDgjudE1/bTF0V2CQO91qRTd75brc136g6E3txz+EMqY0tYiHR+ZkpJiV/m1cgv/CVVMjRo1ytbmXrNmjd20S7DSx7oWPPwjVDGlr6HDFXSyI5cmTFpJbtu2bQTPCF6Kq6ZNm1q334MHD1aJKx3Dq0tnwj861iCmziea6+kk3fA87VKiXeJmz55t6/d99NFHti7y6NGjA1fTdAyIGjhwYGBt26KiIrvXsSaDBg2y8pEjR8ratWttsiutgOjM071797aZXOEfoYwpnexKr+7rOpJaidXnBj8f/hCqmNLWx6uuuipw06v+Sh83adKkVs8Rsfs9pV2AdcytXtDRJEmHL2hFWbuaw19CFVeaZA0dOtTmMfnyyy/tAnRWVpYlR5pswT8a1yCmzieq6+m1vWYZEAllZWVOdna2reuXnJzsvPHGG4EyXcM2eH3bXbt2OWlpabZm4PDhw509e/ZUeS09tlevXvZaunZpaWlpRM8F3ompkpISO/Zst+prLsP7Qvk95dJ1T1mn279CGVN5eXm2Jm5iYqJz//33O8XFxRE9F3gvrsrLy22d7pSUFKd79+5OZmYma7/7VFkNYsql+6qv0x3N9fQ4/ae2E38AAAAAALyI7uUAAAAAAIQJSTcAAAAAAGFC0g0AAAAAQJiQdAMAAAAAECYk3QAAAAAAhAlJNwAAAAAAYULSDQAAAABAmJB0AwAAAAAQJiTdAAAgqv36668SHx9v97XJcRxZsWJFrX4GAEDsiXP0FwQAACBKnT59WkpLS6V58+ZSt27dWvschYWFct9998n+/ftr7TMAAGJPvdr+AAAAAP9FE+1WrVrV9sewlm4AAGqK7uUAACBmupfr/fr162XQoEHSuXNnmTZtmvzyyy8yevRo277nnnvk8OHD9rxFixbJI488ItOnT7eyAQMGyKZNmwKve/LkSXnuueekV69ekpSUJBkZGVJcXFzlPXNzc6Vbt24yefJkew+l+7dt2yYVFRXy7LPPSkpKinTq1ElSU1MlLy8v8Pq6rd3R7777bklISJA777xTdu/eHSj/6aefZMKECdKlSxfp3bu3vP3224GyAwcOWKt6YmKifW66tQNA7CLpBgAAMeXll1+WefPmydKlS2Xjxo0ycuRIu61cuVKOHDkir732WuDYDz/80FqoV69eLcOGDZOHH35YioqKrGzWrFlWPn/+fHvuqVOn5MEHH5TKysrA83fs2CEFBQWSk5NjSbzaunWrJcrLli2TzZs32/4NGzZIWlqaPP3003L06NHA87Vs0qRJsm7dOrn44otlzpw5gYR//PjxctFFF8mqVatk5syZ8uKLL8onn3wi5eXlMnHiROnatas9T9978eLFsmbNmgj+LwMAQoXu5QAAIKaMHTvWWq5Vx44d5ZprrrGWb9W/f3/Zt29f4NimTZvKU089JQ0aNJBrr71WtmzZYkm0tmqvXbvWEvQePXrYsQsXLrQW588++8xeU40ZM0auvPJKe6wJvXK7ut9www32XG0lV/qa2jL+448/SsuWLW3fXXfdJX379rXH48aNk6lTpwYSdx2n/swzz0iTJk3kuuuukyeeeELq1Kkj7733nrRo0UIyMzPt2KuvvloOHTpkLeGa2AMAYgtJNwAAiCnt27cPPG7UqJG0bdu2yrZ2+3bdeOONlnAHbx88eNASY23RdpN31axZM0u2tdxNuoNfuzpNpjVB11b377//Xvbu3RuY+M2lCbNLk+t///3XHv/www/2HrrPpS3xSlve9cKBtqa79DVrcxI5AMD/j6QbAADElOrJp7YOn0u9elWrOpq86vENGzY86/FaHty9/FzHKe0Onp+fL0OHDrUWaO2uruO4g9WvX/+CPlcw7eZ+yy23WJdzAEDsI+kGAACepct7aRLtJuY6kVn37t2ttVwT3507d9pEaOrYsWM2uZnbyl1dXFxclW0dBz579uxA13Z3rPiFzHKuLeD6XidOnJDGjRsHWri1JbxDhw424Vu7du0CFxi0K/y3335rXdABALGFidQAAIBn6czmOkO5dv9esmSJ7NmzR4YPH24TmKWnp9vEZzoTuXbnzsrKkjZt2kjPnj3P+lpucqyJu06Ept3RdeIzfY/t27dLdna2lQd3bz+X5ORkG/etrdnanV2TbE3idf+QIUNsMjW37NNPP5W5c+faOG8AQOwh6QYAAJ6lY7Z1wjLt/q1LjemM4+6YcJ0V/NZbb7UZzXX2c+1K/uabb1YZAx5MlwrThHzEiBGWCOskaN99953ccccdtizZwIEDbYkv3Xc+2squM5KXlJTYZGuaVGvSrhO56ThvneBNx53r59bW7XvvvdeWLQMAxJ4450L6QAEAAMQYXa6rsLBQli9fXtsfBQDgY7R0AwAAAAAQJiTdAAAAAACECd3LAQAAAAAIE1q6AQAAAAAIE5JuAAAAAADChKQbAAAAAIAwIekGAAAAACBMSLoBAAAAAAgTkm4AAAAAAMKEpBsAAAAAgDAh6QYAAAAAIExIugEAAAAAkPD4H0NtvD3QXB2HAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tensorflow.keras.models import load_model\n",
    "import streamlit as st\n",
    "\n",
    "# Configuration\n",
    "FEATURE_NAMES = [\n",
    "    *[f\"MFCC_{i+1}_mean\" for i in range(20)],\n",
    "    *[f\"MFCC_{i+1}_std\" for i in range(20)],\n",
    "    \"Chroma_mean\", \"Chroma_std\",\n",
    "    \"SpectralCentroid_mean\", \"SpectralCentroid_std\",\n",
    "    \"SpectralBandwidth_mean\", \"SpectralBandwidth_std\",\n",
    "    \"SpectralRolloff_mean\", \"SpectralRolloff_std\",\n",
    "    \"ZCR_mean\", \"ZCR_std\",\n",
    "    \"RMS_mean\", \"RMS_std\",\n",
    "    *[f\"Contrast_band{i+1}_mean\" for i in range(6)],\n",
    "    *[f\"Contrast_band{i+1}_std\" for i in range(6)],\n",
    "    *[f\"Tonnetz_{i+1}_mean\" for i in range(6)],\n",
    "    *[f\"Tonnetz_{i+1}_std\" for i in range(6)]\n",
    "]\n",
    "\n",
    "def load_model_and_data():\n",
    "    \"\"\"Load trained model and data\"\"\"\n",
    "    model = load_model(\"audio_model_nn_32.h5\")\n",
    "    scaler = joblib.load(\"scaler_nn_32.joblib\")\n",
    "    background_data = joblib.load(\"background_nn_32.joblib\")\n",
    "    return model, scaler, background_data\n",
    "\n",
    "def analyze_nn_weights(model):\n",
    "    \"\"\"Analyze neural network layer weights\"\"\"\n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    importance = np.mean(np.abs(weights), axis=1)\n",
    "    return pd.DataFrame({\n",
    "        'feature': FEATURE_NAMES,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "def permutation_importance_analysis(model, X, y, n_repeats=10):\n",
    "    \"\"\"Calculate permutation importance\"\"\"\n",
    "    result = permutation_importance(\n",
    "        model, X, y, n_repeats=n_repeats, random_state=42\n",
    "    )\n",
    "    return pd.DataFrame({\n",
    "        'feature': FEATURE_NAMES,\n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "def analyze_by_class(model, X_real, X_fake):\n",
    "    \"\"\"Analyze feature importance separately for real and fake samples\"\"\"\n",
    "    # For real samples (class 0)\n",
    "    perm_real = permutation_importance(\n",
    "        model, X_real, np.zeros(len(X_real)), n_repeats=5, random_state=42\n",
    "    )\n",
    "    real_df = pd.DataFrame({\n",
    "        'feature': FEATURE_NAMES,\n",
    "        'importance': perm_real.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # For fake samples (class 1)\n",
    "    perm_fake = permutation_importance(\n",
    "        model, X_fake, np.ones(len(X_fake)), n_repeats=5, random_state=42\n",
    "    )\n",
    "    fake_df = pd.DataFrame({\n",
    "        'feature': FEATURE_NAMES,\n",
    "        'importance': perm_fake.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return real_df, fake_df\n",
    "\n",
    "def plot_feature_importance(df, title, color_palette):\n",
    "    \"\"\"Plot feature importance\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=df.head(15), palette=color_palette)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def compare_feature_sets(real_df, fake_df, top_n=10):\n",
    "    \"\"\"Compare top features between real and fake\"\"\"\n",
    "    top_real = set(real_df.head(top_n)['feature'])\n",
    "    top_fake = set(fake_df.head(top_n)['feature'])\n",
    "\n",
    "    comparison = {\n",
    "        'unique_to_real': list(top_real - top_fake),\n",
    "        'unique_to_fake': list(top_fake - top_real),\n",
    "        'common_features': list(top_real & top_fake)\n",
    "    }\n",
    "    return comparison\n",
    "\n",
    "def feature_description(feature_name):\n",
    "    \"\"\"Provide descriptions for audio features\"\"\"\n",
    "    desc = {\n",
    "        'MFCC': 'Mel-Frequency Cepstral Coefficients - represent the short-term power spectrum of sound',\n",
    "        'Chroma': 'Pitch class information',\n",
    "        'SpectralCentroid': 'Center of gravity of the spectrum',\n",
    "        'SpectralBandwidth': 'Spread of the spectrum around centroid',\n",
    "        'SpectralRolloff': 'Frequency below which a specified percentage of energy lies',\n",
    "        'ZCR': 'Zero Crossing Rate - number of times signal crosses zero',\n",
    "        'RMS': 'Root Mean Square - energy of the audio signal',\n",
    "        'Contrast': 'Spectral contrast - compares peaks to valleys in spectrum',\n",
    "        'Tonnetz': 'Tonal centroid features - represents tonal relationships'\n",
    "    }\n",
    "\n",
    "    for key in desc:\n",
    "        if feature_name.startswith(key):\n",
    "            return f\"{feature_name}: {desc[key]}\"\n",
    "    return f\"{feature_name}: No description available\"\n",
    "\n",
    "def show_streamlit_analysis():\n",
    "    \"\"\"Streamlit interface for feature analysis\"\"\"\n",
    "    st.title(\"🎙️ Audio Feature Importance Analysis\")\n",
    "\n",
    "    try:\n",
    "        with st.spinner(\"Loading model and data...\"):\n",
    "            model, scaler, background_data = load_model_and_data()\n",
    "            X_scaled = background_data  # Using background data as sample\n",
    "\n",
    "            # Create synthetic labels (50/50 split)\n",
    "            y = np.array([0]*(len(X_scaled)//2) + [1]*(len(X_scaled)//2))\n",
    "\n",
    "        st.success(\"Model and data loaded successfully!\")\n",
    "\n",
    "        # Analysis options\n",
    "        analysis_type = st.selectbox(\n",
    "            \"Select analysis type\",\n",
    "            [\"Neural Network Weights\", \"Permutation Importance\", \"Real vs Fake Comparison\"]\n",
    "        )\n",
    "\n",
    "        if analysis_type == \"Neural Network Weights\":\n",
    "            st.subheader(\"Neural Network Layer Weights Analysis\")\n",
    "            nn_weights_df = analyze_nn_weights(model)\n",
    "            fig = plot_feature_importance(\n",
    "                nn_weights_df,\n",
    "                \"Feature Importance (Neural Network Weights)\",\n",
    "                \"viridis\"\n",
    "            )\n",
    "            st.pyplot(fig)\n",
    "\n",
    "        elif analysis_type == \"Permutation Importance\":\n",
    "            st.subheader(\"Permutation Importance Analysis\")\n",
    "            perm_df = permutation_importance_analysis(model, X_scaled, y)\n",
    "            fig = plot_feature_importance(\n",
    "                perm_df,\n",
    "                \"Feature Importance (Permutation Importance)\",\n",
    "                \"magma\"\n",
    "            )\n",
    "            st.pyplot(fig)\n",
    "\n",
    "        elif analysis_type == \"Real vs Fake Comparison\":\n",
    "            st.subheader(\"Real vs Fake Audio Feature Importance\")\n",
    "\n",
    "            # Split data\n",
    "            X_real = X_scaled[y == 0]\n",
    "            X_fake = X_scaled[y == 1]\n",
    "\n",
    "            real_df, fake_df = analyze_by_class(model, X_real, X_fake)\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.markdown(\"**Top Features for Real Audio**\")\n",
    "                fig = plot_feature_importance(\n",
    "                    real_df,\n",
    "                    \"Top Features for Real Audio\",\n",
    "                    \"Blues_d\"\n",
    "                )\n",
    "                st.pyplot(fig)\n",
    "\n",
    "            with col2:\n",
    "                st.markdown(\"**Top Features for Fake Audio**\")\n",
    "                fig = plot_feature_importance(\n",
    "                    fake_df,\n",
    "                    \"Top Features for Fake Audio\",\n",
    "                    \"Reds_d\"\n",
    "                )\n",
    "                st.pyplot(fig)\n",
    "\n",
    "            # Feature comparison\n",
    "            st.subheader(\"Feature Comparison\")\n",
    "            comparison = compare_feature_sets(real_df, fake_df)\n",
    "\n",
    "            with st.expander(\"Unique Features for Real Audio\"):\n",
    "                for feature in comparison['unique_to_real']:\n",
    "                    st.markdown(f\"- {feature_description(feature)}\")\n",
    "\n",
    "            with st.expander(\"Unique Features for Fake Audio\"):\n",
    "                for feature in comparison['unique_to_fake']:\n",
    "                    st.markdown(f\"- {feature_description(feature)}\")\n",
    "\n",
    "            with st.expander(\"Common Important Features\"):\n",
    "                for feature in comparison['common_features']:\n",
    "                    st.markdown(f\"- {feature_description(feature)}\")\n",
    "\n",
    "            # Difference visualization\n",
    "            st.subheader(\"Feature Importance Differences\")\n",
    "            merged_df = pd.merge(\n",
    "                real_df, fake_df, on='feature',\n",
    "                suffixes=('_real', '_fake')\n",
    "            )\n",
    "            merged_df['difference'] = merged_df['importance_real'] - merged_df['importance_fake']\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(\n",
    "                x='difference', y='feature',\n",
    "                data=merged_df.sort_values('difference', ascending=False).head(20),\n",
    "                palette=\"coolwarm\"\n",
    "            )\n",
    "            plt.title(\"Feature Importance Difference (Real - Fake)\")\n",
    "            plt.axvline(0, color='black', linestyle='--')\n",
    "            st.pyplot(plt)\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    show_streamlit_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bc1207cf19582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbeeb6f2720d2472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T08:50:59.640840Z",
     "start_time": "2025-04-26T08:49:27.503968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3000 fake samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fake files: 100%|██████████| 3000/3000 [00:23<00:00, 130.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3000 real samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Real files: 100%|██████████| 3000/3000 [01:08<00:00, 43.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audio processing complete. Processed files saved to: c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/fake-real-audio/\"\n",
    "OUTPUT_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "TARGET_DURATION = 4.0  # 4 seconds\n",
    "SR = 22050  # Sample rate\n",
    "\n",
    "def process_audio_file(input_path, output_path):\n",
    "    \"\"\"Process a single audio file to make it exactly 4 seconds\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(input_path, sr=SR)\n",
    "        duration = librosa.get_duration(y=audio, sr=sr)\n",
    "\n",
    "        # Calculate target number of samples\n",
    "        target_samples = int(TARGET_DURATION * SR)\n",
    "\n",
    "        if duration > TARGET_DURATION:\n",
    "            # Trim the audio to 4 seconds (from the beginning)\n",
    "            processed_audio = audio[:target_samples]\n",
    "        elif duration < TARGET_DURATION:\n",
    "            # Pad the audio with silence at beginning and end\n",
    "            padding_samples = target_samples - len(audio)\n",
    "            # Split padding between beginning and end (60% at beginning, 40% at end)\n",
    "            pad_start = int(padding_samples * 0.6)\n",
    "            pad_end = padding_samples - pad_start\n",
    "            processed_audio = np.pad(audio, (pad_start, pad_end), mode='constant')\n",
    "        else:\n",
    "            # Exactly 4 seconds, no processing needed\n",
    "            processed_audio = audio\n",
    "\n",
    "        # Save processed audio\n",
    "        sf.write(output_path, processed_audio, SR)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_dataset():\n",
    "    \"\"\"Process all audio files in the dataset\"\"\"\n",
    "    # Create output directory structure\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"Fake\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"Real\"), exist_ok=True)\n",
    "\n",
    "    # Process Fake audio files\n",
    "    fake_path = os.path.join(DATA_DIR, \"Fake\")\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_files = [f for f in os.listdir(fake_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Processing {len(fake_files)} fake samples\")\n",
    "\n",
    "        for file in tqdm(fake_files, desc=\"Processing Fake files\"):\n",
    "            input_path = os.path.join(fake_path, file)\n",
    "            output_path = os.path.join(OUTPUT_DIR, \"Fake\", file)\n",
    "            process_audio_file(input_path, output_path)\n",
    "\n",
    "    # Process Real audio files\n",
    "    real_path = os.path.join(DATA_DIR, \"Real\")\n",
    "    if os.path.exists(real_path):\n",
    "        real_files = [f for f in os.listdir(real_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Processing {len(real_files)} real samples\")\n",
    "\n",
    "        for file in tqdm(real_files, desc=\"Processing Real files\"):\n",
    "            input_path = os.path.join(real_path, file)\n",
    "            output_path = os.path.join(OUTPUT_DIR, \"Real\", file)\n",
    "            process_audio_file(input_path, output_path)\n",
    "\n",
    "    print(\"\\nAudio processing complete. Processed files saved to:\", OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228a141a5e16307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T05:38:11.924793Z",
     "start_time": "2025-04-24T05:30:21.352241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 1800 fake samples\n",
      "Processed 100/1800 fake files\n",
      "Processed 200/1800 fake files\n",
      "Processed 300/1800 fake files\n",
      "Processed 400/1800 fake files\n",
      "Processed 500/1800 fake files\n",
      "Processed 600/1800 fake files\n",
      "Processed 700/1800 fake files\n",
      "Processed 800/1800 fake files\n",
      "Processed 900/1800 fake files\n",
      "Processed 1000/1800 fake files\n",
      "Processed 1100/1800 fake files\n",
      "Processed 1200/1800 fake files\n",
      "Processed 1300/1800 fake files\n",
      "Processed 1400/1800 fake files\n",
      "Processed 1500/1800 fake files\n",
      "Processed 1600/1800 fake files\n",
      "Processed 1700/1800 fake files\n",
      "Processed 1800/1800 fake files\n",
      "Found 1800 real samples\n",
      "Processed 100/1800 real files\n",
      "Processed 200/1800 real files\n",
      "Processed 300/1800 real files\n",
      "Processed 400/1800 real files\n",
      "Processed 500/1800 real files\n",
      "Processed 600/1800 real files\n",
      "Processed 700/1800 real files\n",
      "Processed 800/1800 real files\n",
      "Processed 900/1800 real files\n",
      "Processed 1000/1800 real files\n",
      "Processed 1100/1800 real files\n",
      "Processed 1200/1800 real files\n",
      "Processed 1300/1800 real files\n",
      "Processed 1400/1800 real files\n",
      "Processed 1500/1800 real files\n",
      "Processed 1600/1800 real files\n",
      "Processed 1700/1800 real files\n",
      "Processed 1800/1800 real files\n",
      "\n",
      "Total samples loaded: 3600\n",
      "Class distribution: Fake=1800, Real=1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8540 - loss: 0.3158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8551 - loss: 0.3138 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 2/30\n",
      "\u001b[1m88/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 9.1174e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m88/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9996 - loss: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 2.7739e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m44/90\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0043 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.2911e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m52/90\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.3834e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 8.8299e-04 - val_accuracy: 1.0000 - val_loss: 6.9265e-05\n",
      "Epoch 6/30\n",
      "\u001b[1m38/90\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.0300e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.9577e-04 - val_accuracy: 1.0000 - val_loss: 4.3550e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m61/90\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.3766e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.5759e-04 - val_accuracy: 1.0000 - val_loss: 3.8858e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m43/90\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.9638e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.0041e-04 - val_accuracy: 1.0000 - val_loss: 1.7322e-05\n",
      "Epoch 9/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 5.2139e-04 - val_accuracy: 1.0000 - val_loss: 3.2389e-05\n",
      "Epoch 10/30\n",
      "\u001b[1m52/90\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.7355e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6714e-04 - val_accuracy: 1.0000 - val_loss: 1.7026e-05\n",
      "Epoch 11/30\n",
      "\u001b[1m66/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.5935e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.6568e-04 - val_accuracy: 1.0000 - val_loss: 8.1797e-06\n",
      "Epoch 12/30\n",
      "\u001b[1m42/90\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.3205e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.2001e-04 - val_accuracy: 1.0000 - val_loss: 6.2327e-06\n",
      "Epoch 13/30\n",
      "\u001b[1m45/90\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.2932e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.0037e-05 - val_accuracy: 1.0000 - val_loss: 5.1793e-06\n",
      "Epoch 14/30\n",
      "\u001b[1m40/90\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.8493e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.2670e-05 - val_accuracy: 1.0000 - val_loss: 4.4620e-06\n",
      "Epoch 15/30\n",
      "\u001b[1m47/90\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.9556e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.4450e-05 - val_accuracy: 1.0000 - val_loss: 4.2043e-06\n",
      "Epoch 16/30\n",
      "\u001b[1m38/90\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.3170e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.6940e-05 - val_accuracy: 1.0000 - val_loss: 2.8412e-06\n",
      "Epoch 17/30\n",
      "\u001b[1m49/90\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.0221e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.3752e-05 - val_accuracy: 1.0000 - val_loss: 2.2355e-06\n",
      "Epoch 18/30\n",
      "\u001b[1m40/90\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.0554e-05 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.8171e-05 - val_accuracy: 1.0000 - val_loss: 1.9402e-06\n",
      "Epoch 19/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 2.1335e-04 - val_accuracy: 1.0000 - val_loss: 2.0532e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6104e-04 - val_accuracy: 1.0000 - val_loss: 4.6956e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8458e-05 - val_accuracy: 1.0000 - val_loss: 1.1484e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.3310e-05 - val_accuracy: 1.0000 - val_loss: 2.2711e-06\n",
      "Epoch 23/30\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.7824e-04 - val_accuracy: 1.0000 - val_loss: 3.5707e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model and artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76  # Must be exactly 76\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Feature extraction that consistently returns 76 features\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "        feature_counts = {}\n",
    "\n",
    "        # 1. MFCCs (40 features: 20 means + 20 std)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "        feature_counts['mfcc'] = 40\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "        feature_counts['chroma'] = 2\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "\n",
    "        spectral_features = [\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ]\n",
    "        features.extend(spectral_features)\n",
    "        feature_counts['spectral'] = 6\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "        feature_counts['zcr'] = 2\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "        feature_counts['rms'] = 2\n",
    "\n",
    "        # 6. Spectral Contrast (12 features: 6 means + 6 std)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        # Ensure we only get 6 bands\n",
    "        contrast_mean = np.mean(contrast[:6], axis=1)\n",
    "        contrast_std = np.std(contrast[:6], axis=1)\n",
    "        features.extend(contrast_mean)\n",
    "        features.extend(contrast_std)\n",
    "        feature_counts['contrast'] = 12\n",
    "\n",
    "        # 7. Tonnetz (12 features: 6 means + 6 std)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "        feature_counts['tonnetz'] = 12\n",
    "\n",
    "        features = np.array(features)\n",
    "        total_features = sum(feature_counts.values())\n",
    "\n",
    "        if len(features) != FEATURE_COUNT or total_features != FEATURE_COUNT:\n",
    "            print(\"\\nFeature count breakdown:\")\n",
    "            for name, count in feature_counts.items():\n",
    "                print(f\"{name}: {count}\")\n",
    "            print(f\"Total features extracted: {len(features)}\")\n",
    "            print(f\"Expected: {FEATURE_COUNT}\")\n",
    "            raise ValueError(\"Feature count mismatch!\")\n",
    "\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset():\n",
    "    X, y = [], []\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    for label, folder in enumerate([\"fake\", \"real\"]):  # Changed here\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Directory not found: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(files)} {folder} samples\")\n",
    "\n",
    "        for i, file in enumerate(files):\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Processed {i+1}/{len(files)} {folder} files\")\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(label)\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Class distribution: Fake={sum(np.array(y)==0)}, Real={sum(np.array(y)==1)}\")  # Changed here\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    X, y = load_dataset()\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Prepare SHAP background data\n",
    "    background_samples = min(100, len(X_scaled))\n",
    "    background = X_scaled[np.random.choice(len(X_scaled), background_samples, replace=False)]\n",
    "\n",
    "    # Save artifacts\n",
    "    joblib.dump(scaler, \"scaler_nn_62.joblib\")\n",
    "    joblib.dump(background, \"background_nn_62.joblib\")\n",
    "\n",
    "    # Model architecture\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             batch_size=32,\n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=callbacks,\n",
    "             verbose=1)\n",
    "\n",
    "   model.save(\"audio_model_nn_62.h5\")\n",
    "\n",
    "    print(\"Training complete! Model and artifacts saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99e52273ca562900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:45:07.805143Z",
     "start_time": "2025-04-23T22:44:58.665166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Options:\n",
      "1. Test single file\n",
      "2. Test directory\n",
      "3. Test sample files (real and fake)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "File: fake_1.wav\n",
      "Prediction: Fake\n",
      "Confidence: 100.0%\n",
      "Real probability: 0.000\n",
      "Fake probability: 1.000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Configuration (must match training)\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "FEATURE_COUNT = 76\n",
    "CLASS_NAMES = [\"Real\", \"Fake\"]\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Identical to training feature extractor\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # 1. MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20,\n",
    "                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # 2. Chroma (2)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR,\n",
    "                                          n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # 3. Spectral Features (6)\n",
    "        features.append(np.mean(librosa.feature.spectral_centroid(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_centroid(y=audio, sr=SR)))\n",
    "        features.append(np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_bandwidth(y=audio, sr=SR)))\n",
    "        features.append(np.mean(librosa.feature.spectral_rolloff(y=audio, sr=SR)))\n",
    "        features.append(np.std(librosa.feature.spectral_rolloff(y=audio, sr=SR)))\n",
    "\n",
    "        # 4. Zero Crossing Rate (2)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio,\n",
    "                                              frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # 5. RMS Energy (2)\n",
    "        rms = librosa.feature.rms(y=audio,\n",
    "                               frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # 6. Spectral Contrast (12)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR,\n",
    "                                                  n_bands=6,\n",
    "                                                  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        features.extend(np.mean(contrast[:6], axis=1))  # Ensure only 6 bands\n",
    "        features.extend(np.std(contrast[:6], axis=1))\n",
    "\n",
    "        # 7. Tonnetz (12)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def test_single_file(file_path, model, scaler):\n",
    "    \"\"\"Test a single audio file\"\"\"\n",
    "    # Extract features\n",
    "    features = extract_features(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "\n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "\n",
    "    # Make prediction\n",
    "    proba = model.predict(features_scaled, verbose=0)[0]\n",
    "    prediction = np.argmax(proba)\n",
    "    confidence = proba[prediction]\n",
    "\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(CLASS_NAMES, proba, color=['green', 'red'])\n",
    "    plt.title(f\"Prediction: {CLASS_NAMES[prediction]} ({confidence*100:.1f}% confidence)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'prediction': CLASS_NAMES[prediction],\n",
    "        'confidence': float(confidence),\n",
    "        'probabilities': {\n",
    "            'Real': float(proba[0]),\n",
    "            'Fake': float(proba[1])\n",
    "        }\n",
    "    }\n",
    "\n",
    "def test_directory(directory_path, model, scaler):\n",
    "    \"\"\"Test all WAV files in a directory\"\"\"\n",
    "    results = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            result = test_single_file(file_path, model, scaler)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                print(f\"{file}: {result['prediction']} ({result['confidence']*100:.1f}%)\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Load model and scaler\n",
    "    try:\n",
    "        model = load_model(\"audio_model_nn_32.h5\")\n",
    "        scaler = joblib.load(\"scaler_nn_32.joblib\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Test options\n",
    "    print(\"\\nTesting Options:\")\n",
    "    print(\"1. Test single file\")\n",
    "    print(\"2. Test directory\")\n",
    "    print(\"3. Test sample files (real and fake)\")\n",
    "    choice = input(\"Enter your choice (1-3): \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        file_path = input(\"Enter path to audio file: \")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"File not found!\")\n",
    "            return\n",
    "        result = test_single_file(file_path, model, scaler)\n",
    "        if result:\n",
    "            print(\"\\nTest Results:\")\n",
    "            print(f\"File: {result['file']}\")\n",
    "            print(f\"Prediction: {result['prediction']}\")\n",
    "            print(f\"Confidence: {result['confidence']*100:.1f}%\")\n",
    "            print(f\"Real probability: {result['probabilities']['Real']:.3f}\")\n",
    "            print(f\"Fake probability: {result['probabilities']['Fake']:.3f}\")\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        dir_path = input(\"Enter path to directory: \")\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(\"Directory not found!\")\n",
    "            return\n",
    "        results = test_directory(dir_path, model, scaler)\n",
    "        print(\"\\nSummary Results:\")\n",
    "        real_count = sum(1 for r in results if r['prediction'] == \"Real\")\n",
    "        fake_count = len(results) - real_count\n",
    "        print(f\"Real: {real_count} files\")\n",
    "        print(f\"Fake: {fake_count} files\")\n",
    "\n",
    "    elif choice == \"3\":\n",
    "        # Test with included sample files\n",
    "        test_files = {\n",
    "            \"Real\": \"path_to_real_sample.wav\",  # Replace with actual paths\n",
    "            \"Fake\": \"path_to_fake_sample.wav\"   # Replace with actual paths\n",
    "        }\n",
    "\n",
    "        for label, file_path in test_files.items():\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"\\nTesting {label} sample:\")\n",
    "                result = test_single_file(file_path, model, scaler)\n",
    "                if result:\n",
    "                    print(f\"Expected: {label}, Predicted: {result['prediction']}\")\n",
    "                    print(f\"Confidence: {result['confidence']*100:.1f}%\")\n",
    "            else:\n",
    "                print(f\"Sample file not found: {file_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933207deaa38dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Previous imports and configuration remain the same]\n",
    "\n",
    "# [Keep all previous code until after model training]\n",
    "\n",
    "# After model training, add feature names for explanations\n",
    "feature_names = []\n",
    "\n",
    "# 1. MFCC Features (1-based indexing)\n",
    "for i in range(1, N_MFCC+1):\n",
    "    feature_names.append(f\"MFCC_{i}_mean\")\n",
    "for i in range(1, N_MFCC+1):\n",
    "    feature_names.append(f\"MFCC_{i}_std\")\n",
    "\n",
    "# 2. Chroma Features\n",
    "feature_names.extend([\"Chroma_stft_mean\", \"Chroma_stft_std\"])\n",
    "\n",
    "# 3. Spectral Features\n",
    "feature_names.extend([\n",
    "    \"Spectral_centroid_mean\",\n",
    "    \"Spectral_centroid_std\",\n",
    "    \"Spectral_bandwidth_mean\",\n",
    "    \"Spectral_bandwidth_std\",\n",
    "    \"Spectral_rolloff_mean\",\n",
    "    \"Spectral_rolloff_std\"\n",
    "])\n",
    "\n",
    "# 4. Zero Crossing Rate\n",
    "feature_names.extend([\"ZCR_mean\", \"ZCR_std\"])\n",
    "\n",
    "# 5. RMS Energy\n",
    "feature_names.extend([\"RMS_mean\", \"RMS_std\"])\n",
    "\n",
    "# 6. Spectral Contrast (band-based naming)\n",
    "for band in range(6):\n",
    "    feature_names.append(f\"Spectral_contrast_band{band}_mean\")\n",
    "for band in range(6):\n",
    "    feature_names.append(f\"Spectral_contrast_band{band}_std\")\n",
    "\n",
    "# 7. Tonnetz Features (tonal centroid features)\n",
    "for i in range(6):\n",
    "    feature_names.append(f\"Tonnetz_{i}_mean\")\n",
    "for i in range(6):\n",
    "    feature_names.append(f\"Tonnetz_{i}_std\")\n",
    "\n",
    "# Verify feature count matches\n",
    "assert len(feature_names) == FEATURE_COUNT, \\\n",
    "    f\"Feature count mismatch: {len(feature_names)} vs {FEATURE_COUNT}\"\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(feature_names, \"feature_names_0.joblib\")\n",
    "print(\"Training complete! Model, artifacts, and feature names saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de7c6561ee8f542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:18:16.584020Z",
     "start_time": "2025-04-23T21:18:16.442796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution - Real: 1442, Fake: 1438\n"
     ]
    }
   ],
   "source": [
    "print(f\"Class distribution - Real: {sum(y_train==1)}, Fake: {sum(y_train==0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e17e40ce99fac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T10:20:12.557246Z",
     "start_time": "2025-04-26T09:56:34.694871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset...\n",
      "Found 3000 fake samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fake files: 100%|██████████| 3000/3000 [05:59<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 real samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Real files: 100%|██████████| 3000/3000 [06:04<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples loaded: 6000\n",
      "Class distribution: Real=3000, Fake=3000\n",
      "Saved background data (100 samples)\n",
      "\n",
      "Starting 10×10-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 1: Accuracy = 0.9967\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 3: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 4: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step\n",
      "Repeat 1, Fold 5: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 6: Accuracy = 1.0000\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 1, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 1, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 9: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 10: Accuracy = 0.9917\n",
      "Repeat 1 complete. Mean accuracy: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 1: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step\n",
      "Repeat 2, Fold 3: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 2, Fold 4: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 5: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 2, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 2, Fold 7: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 8: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 2, Fold 9: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step\n",
      "Repeat 2, Fold 10: Accuracy = 0.9967\n",
      "Repeat 2 complete. Mean accuracy: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 3, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 3, Fold 2: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 3: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 4: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step\n",
      "Repeat 3, Fold 5: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 6: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 8: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 9: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step\n",
      "Repeat 3, Fold 10: Accuracy = 1.0000\n",
      "Repeat 3 complete. Mean accuracy: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step\n",
      "Repeat 4, Fold 1: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 4, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 4: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 5: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 6: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 7: Accuracy = 0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 4, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 10: Accuracy = 0.9967\n",
      "Repeat 4 complete. Mean accuracy: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 1: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 2: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 3: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 5: Accuracy = 0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 6: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 5, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 8: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 9: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 10: Accuracy = 0.9950\n",
      "Repeat 5 complete. Mean accuracy: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 1: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 2: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 3: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 4: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 5: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 7: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 6, Fold 8: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step\n",
      "Repeat 6, Fold 9: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 6, Fold 10: Accuracy = 0.9967\n",
      "Repeat 6 complete. Mean accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 7, Fold 3: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 4: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 5: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 7, Fold 7: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 9: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 7, Fold 10: Accuracy = 1.0000\n",
      "Repeat 7 complete. Mean accuracy: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 1: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 3: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 4: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 8, Fold 5: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 7: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 8: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 8, Fold 9: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 8, Fold 10: Accuracy = 0.9983\n",
      "Repeat 8 complete. Mean accuracy: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 9, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 9, Fold 2: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 3: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 4: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 5: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 6: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 7: Accuracy = 0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 8: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 9: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 9, Fold 10: Accuracy = 0.9950\n",
      "Repeat 9 complete. Mean accuracy: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 1: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step\n",
      "Repeat 10, Fold 2: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step\n",
      "Repeat 10, Fold 3: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 4: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 5: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 8: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 10, Fold 9: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 10, Fold 10: Accuracy = 0.9950\n",
      "Repeat 10 complete. Mean accuracy: 0.9957\n",
      "\n",
      "Final Results:\n",
      "Mean Accuracy: 0.9960 (±0.0028)\n",
      "Best Accuracy: 1.0000\n",
      "Worst Accuracy: 0.9883\n",
      "\n",
      "All results saved in 'model_results' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76  # Number of audio features\n",
    "N_SPLITS = 10  # Number of folds for cross-validation\n",
    "N_REPEATS = 10  # Number of times to repeat CV\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract 76 audio features from processed WAV file\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # 1. MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        features.extend([\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ])\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # 6. Spectral Contrast (12 features)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR, n_bands=6)\n",
    "        features.extend(np.mean(contrast[:6], axis=1))\n",
    "        features.extend(np.std(contrast[:6], axis=1))\n",
    "\n",
    "        # 7. Tonnetz (12 features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load dataset from processed audio files\"\"\"\n",
    "    X, y = [], []\n",
    "    print(\"Loading processed dataset...\")\n",
    "\n",
    "    # Process Fake audio files\n",
    "    fake_path = os.path.join(PROCESSED_DIR, \"Fake\")\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_files = [f for f in os.listdir(fake_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(fake_files)} fake samples\")\n",
    "\n",
    "        for file in tqdm(fake_files, desc=\"Loading Fake files\"):\n",
    "            file_path = os.path.join(fake_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(1)  # Label 1 for fake\n",
    "\n",
    "    # Process Real audio files\n",
    "    real_path = os.path.join(PROCESSED_DIR, \"Real\")\n",
    "    if os.path.exists(real_path):\n",
    "        real_files = [f for f in os.listdir(real_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(real_files)} real samples\")\n",
    "\n",
    "        for file in tqdm(real_files, desc=\"Loading Real files\"):\n",
    "            file_path = os.path.join(real_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features[0])\n",
    "                y.append(0)  # Label 0 for real\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y)==0)}, Fake={sum(np.array(y)==1)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_evaluation_metrics(y_true, y_pred, y_probs, repeat, fold):\n",
    "    \"\"\"Save evaluation metrics for each fold\"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title(f'Confusion Matrix (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/confusion_matrix_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Real', 'Fake'])\n",
    "    with open(f'{RESULTS_DIR}/classification_report_r{repeat+1}_f{fold+1}.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'ROC Curve (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/roc_curve_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_background_data(X_scaled, y, n_samples=100):\n",
    "    \"\"\"Save representative background data for SHAP analysis\"\"\"\n",
    "    try:\n",
    "        if len(X_scaled) > n_samples * 2:\n",
    "            background, _ = train_test_split(\n",
    "                X_scaled,\n",
    "                train_size=n_samples,\n",
    "                stratify=y,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            background = X_scaled[np.random.choice(len(X_scaled), min(n_samples, len(X_scaled)), replace=False)]\n",
    "\n",
    "        joblib.dump(background, f'{RESULTS_DIR}/background_data_1.joblib')\n",
    "        print(f\"Saved background data ({background.shape[0]} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving background data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save background data and scaler\n",
    "    save_background_data(X_scaled, y)\n",
    "    joblib.dump(scaler, f'{RESULTS_DIR}/scaler_1.joblib')\n",
    "\n",
    "    # Cross-validation setup\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train model\n",
    "            model = create_model()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Save metrics\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "            save_evaluation_metrics(y_test, y_pred, y_probs, repeat, fold)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model_1.h5')\n",
    "\n",
    "            print(f\"Repeat {repeat+1}, Fold {fold+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Repeat statistics\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Final results\n",
    "    joblib.dump(all_accuracies, f\"{RESULTS_DIR}/all_accuracies_1.joblib\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"10×10-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc122c6bdf0e64a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:49:15.202543Z",
     "start_time": "2025-04-29T06:43:11.801643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Found 1800 fake test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fake test files: 100%|██████████| 1800/1800 [02:56<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 real test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Real test files: 100%|██████████| 1800/1800 [03:04<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total test samples loaded: 3600\n",
      "Class distribution: Real=1800, Fake=1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\n",
      "Test Accuracy: 0.7833\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.75      0.86      0.80      1800\n",
      "        Fake       0.83      0.71      0.77      1800\n",
      "\n",
      "    accuracy                           0.78      3600\n",
      "   macro avg       0.79      0.78      0.78      3600\n",
      "weighted avg       0.79      0.78      0.78      3600\n",
      "\n",
      "\n",
      "Test results saved in 'model_results/test_results' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Configuration\n",
    "TEST_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "RESULTS_DIR = \"model_results/test_results\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76  # Should match your training setup\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract 76 audio features from WAV file (same as training)\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # 1. MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # 2. Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # 3. Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        features.extend([\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ])\n",
    "\n",
    "        # 4. Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # 5. RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # 6. Spectral Contrast (12 features)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR, n_bands=6)\n",
    "        features.extend(np.mean(contrast[:6], axis=1))\n",
    "        features.extend(np.std(contrast[:6], axis=1))\n",
    "\n",
    "        # 7. Tonnetz (12 features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features.reshape(1, -1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_test_dataset():\n",
    "    \"\"\"Load test dataset from audio files\"\"\"\n",
    "    X_test, y_test = [], []\n",
    "    print(\"Loading test dataset...\")\n",
    "\n",
    "    # Process Fake audio files\n",
    "    fake_path = os.path.join(TEST_DIR, \"Fake\")\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_files = [f for f in os.listdir(fake_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(fake_files)} fake test samples\")\n",
    "\n",
    "        for file in tqdm(fake_files, desc=\"Loading Fake test files\"):\n",
    "            file_path = os.path.join(fake_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X_test.append(features[0])\n",
    "                y_test.append(1)  # Label 1 for fake\n",
    "\n",
    "    # Process Real audio files\n",
    "    real_path = os.path.join(TEST_DIR, \"Real\")\n",
    "    if os.path.exists(real_path):\n",
    "        real_files = [f for f in os.listdir(real_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(real_files)} real test samples\")\n",
    "\n",
    "        for file in tqdm(real_files, desc=\"Loading Real test files\"):\n",
    "            file_path = os.path.join(real_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X_test.append(features[0])\n",
    "                y_test.append(0)  # Label 0 for real\n",
    "\n",
    "    print(f\"\\nTotal test samples loaded: {len(X_test)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y_test)==0)}, Fake={sum(np.array(y_test)==1)}\")\n",
    "    return np.array(X_test), np.array(y_test)\n",
    "\n",
    "def evaluate_model(model, scaler, X_test, y_test):\n",
    "    \"\"\"Evaluate the model on test data\"\"\"\n",
    "    # Scale the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_probs = model.predict(X_test_scaled)\n",
    "    y_pred = y_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title('Test Confusion Matrix')\n",
    "    plt.savefig(f'{RESULTS_DIR}/test_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, y_pred, target_names=['Real', 'Fake'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    with open(f'{RESULTS_DIR}/test_classification_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('Test ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{RESULTS_DIR}/test_roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load test data\n",
    "    X_test, y_test = load_test_dataset()\n",
    "    if len(X_test) == 0:\n",
    "        print(\"No valid test samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Load trained model and scaler\n",
    "    try:\n",
    "        model = load_model(f'model_results/best_model_1.h5')\n",
    "        scaler = joblib.load(f'model_results/scaler_1.joblib')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or scaler: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(model, scaler, X_test, y_test)\n",
    "    print(f\"\\nTest results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3c6ab292ea62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_large\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76  # Number of audio features\n",
    "N_SPLITS = 10  # Number of folds for cross-validation\n",
    "N_REPEATS = 5  # Reduced repeats due to larger model\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# [Previous extract_features() and load_processed_dataset() functions remain the same]\n",
    "\n",
    "def create_large_model():\n",
    "    \"\"\"Create a larger neural network model with regularization\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(FEATURE_COUNT,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save background data and scaler\n",
    "    save_background_data(X_scaled, y)\n",
    "    joblib.dump(scaler, f'{RESULTS_DIR}/scaler_1.joblib')\n",
    "\n",
    "    # Cross-validation setup\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation with larger model...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train larger model\n",
    "            model = create_large_model()\n",
    "\n",
    "            # Additional callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,  # Increased epochs\n",
    "                batch_size=64,  # Larger batch size\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Save metrics\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "            save_evaluation_metrics(y_test, y_pred, y_probs, repeat, fold)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model_1.h5')\n",
    "\n",
    "            print(f\"Repeat {repeat+1}, Fold {fold+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Repeat statistics\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Final results\n",
    "    joblib.dump(all_accuracies, f\"{RESULTS_DIR}/all_accuracies_1.joblib\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"5×10-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d859abb45986c863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T18:34:18.303141Z",
     "start_time": "2025-04-29T18:09:46.595175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset...\n",
      "Found 3000 fake samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fake files: 100%|██████████| 3000/3000 [07:08<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 real samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Real files: 100%|██████████| 3000/3000 [11:52<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples loaded: 6000\n",
      "Class distribution: Real=3000, Fake=3000\n",
      "\n",
      "Class Distribution:\n",
      "Class 0: 3000 samples (50.0%)\n",
      "Class 1: 3000 samples (50.0%)\n",
      "\n",
      "Baseline (Random) Accuracy: 0.5000\n",
      "Baseline (Majority Class) Accuracy: 0.5000\n",
      "Saved background data (100 samples) to raw_background.joblib\n",
      "\n",
      "Starting 5×5-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 1: Accuracy = 0.9917\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 1, Fold 2: Accuracy = 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 3: Accuracy = 0.9950\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 4: Accuracy = 0.9967\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 5: Accuracy = 0.9950\n",
      "Repeat 1 complete. Mean accuracy: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 2, Fold 1: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 2, Fold 2: Accuracy = 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 2, Fold 3: Accuracy = 0.9983\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 2, Fold 4: Accuracy = 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step\n",
      "Repeat 2, Fold 5: Accuracy = 0.9950\n",
      "Repeat 2 complete. Mean accuracy: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step\n",
      "Repeat 3, Fold 1: Accuracy = 0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 3, Fold 2: Accuracy = 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step\n",
      "Repeat 3, Fold 3: Accuracy = 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 3, Fold 4: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Repeat 3, Fold 5: Accuracy = 0.9975\n",
      "Repeat 3 complete. Mean accuracy: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 4, Fold 1: Accuracy = 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 4, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 3: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 4, Fold 4: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 5: Accuracy = 0.9925\n",
      "Repeat 4 complete. Mean accuracy: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 5, Fold 1: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 5, Fold 2: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 5, Fold 3: Accuracy = 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Repeat 5, Fold 4: Accuracy = 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 5: Accuracy = 0.9933\n",
      "Repeat 5 complete. Mean accuracy: 0.9945\n",
      "Saved background data (200 samples) to final_background.joblib\n",
      "\n",
      "Final Results:\n",
      "Mean Accuracy: 0.9941 (±0.0022)\n",
      "Range: [0.9900, 0.9983]\n",
      "\n",
      "All results saved in 'model_results_cv_fold' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_cv_fold\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 5\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract audio features with error handling\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # MFCCs (40 features)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "\n",
    "        # Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR)\n",
    "        features.extend([np.mean(chroma), np.std(chroma)])\n",
    "\n",
    "        # Spectral Features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        features.extend([\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff)\n",
    "        ])\n",
    "\n",
    "        # Zero Crossing Rate (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        features.extend([np.mean(zcr), np.std(zcr)])\n",
    "\n",
    "        # RMS Energy (2 features)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "        # Spectral Contrast (12 features)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=SR, n_bands=6)\n",
    "        features.extend(np.mean(contrast[:6], axis=1))\n",
    "        features.extend(np.std(contrast[:6], axis=1))\n",
    "\n",
    "        # Tonnetz (12 features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        features = np.array(features)\n",
    "        if len(features) != FEATURE_COUNT:\n",
    "            raise ValueError(f\"Expected {FEATURE_COUNT} features, got {len(features)}\")\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load dataset from processed audio files\"\"\"\n",
    "    X, y = [], []\n",
    "    print(\"Loading processed dataset...\")\n",
    "\n",
    "    # Process Fake audio files\n",
    "    fake_path = os.path.join(PROCESSED_DIR, \"Fake\")\n",
    "    if os.path.exists(fake_path):\n",
    "        fake_files = [f for f in os.listdir(fake_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(fake_files)} fake samples\")\n",
    "\n",
    "        for file in tqdm(fake_files, desc=\"Loading Fake files\"):\n",
    "            file_path = os.path.join(fake_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(1)  # Label 1 for fake\n",
    "\n",
    "    # Process Real audio files\n",
    "    real_path = os.path.join(PROCESSED_DIR, \"Real\")\n",
    "    if os.path.exists(real_path):\n",
    "        real_files = [f for f in os.listdir(real_path) if f.endswith(\".wav\")]\n",
    "        print(f\"Found {len(real_files)} real samples\")\n",
    "\n",
    "        for file in tqdm(real_files, desc=\"Loading Real files\"):\n",
    "            file_path = os.path.join(real_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(0)  # Label 0 for real\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Class distribution: Real={sum(np.array(y)==0)}, Fake={sum(np.array(y)==1)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def analyze_class_balance(y):\n",
    "    \"\"\"Check and visualize class distribution\"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"Class {cls}: {count} samples ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x=y)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.savefig(f\"{RESULTS_DIR}/class_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_baseline(X, y):\n",
    "    \"\"\"Evaluate simple baseline models\"\"\"\n",
    "    dummy = DummyClassifier(strategy='stratified')\n",
    "    dummy_acc = np.mean(cross_val_score(dummy, X, y, cv=5))\n",
    "    print(f\"\\nBaseline (Random) Accuracy: {dummy_acc:.4f}\")\n",
    "\n",
    "    dummy = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_acc = np.mean(cross_val_score(dummy, X, y, cv=5))\n",
    "    print(f\"Baseline (Majority Class) Accuracy: {dummy_acc:.4f}\")\n",
    "\n",
    "def create_simpler_model():\n",
    "    \"\"\"Create neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(FEATURE_COUNT,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_evaluation_metrics(y_true, y_pred, y_probs, repeat, fold):\n",
    "    \"\"\"Save evaluation metrics for each fold\"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.title(f'Confusion Matrix (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/confusion_matrix_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Real', 'Fake'])\n",
    "    with open(f'{RESULTS_DIR}/classification_report_r{repeat+1}_f{fold+1}.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'ROC Curve (Repeat {repeat+1}, Fold {fold+1})')\n",
    "    plt.savefig(f'{RESULTS_DIR}/roc_curve_r{repeat+1}_f{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_background_data(X, y, n_samples=100, filename='background_data.joblib'):\n",
    "    \"\"\"Save representative background data for SHAP analysis\"\"\"\n",
    "    try:\n",
    "        if len(X) < n_samples:\n",
    "            print(f\"Warning: Not enough samples ({len(X)}) for requested background size ({n_samples})\")\n",
    "            n_samples = len(X)\n",
    "\n",
    "        if len(np.unique(y)) > 1:  # Only stratify if we have multiple classes\n",
    "            background, _, _, _ = train_test_split(\n",
    "                X, y,\n",
    "                train_size=n_samples,\n",
    "                stratify=y,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            background = X[np.random.choice(len(X), n_samples, replace=False)]\n",
    "\n",
    "        joblib.dump(background, os.path.join(RESULTS_DIR, filename))\n",
    "        print(f\"Saved background data ({background.shape[0]} samples) to {filename}\")\n",
    "        return background\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving background data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Data analysis\n",
    "    analyze_class_balance(y)\n",
    "    evaluate_baseline(X, y)\n",
    "\n",
    "    # Save initial background data (before scaling)\n",
    "    raw_background = save_background_data(X, y, n_samples=100, filename='raw_background.joblib')\n",
    "\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            # Scale inside CV loop to prevent data leakage\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X[train_idx])\n",
    "            X_test = scaler.transform(X[test_idx])\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Save scaled background data for the first fold of each repeat\n",
    "            if fold == 0:\n",
    "                scaled_background = scaler.transform(raw_background)\n",
    "                joblib.dump(\n",
    "                    scaled_background,\n",
    "                    os.path.join(RESULTS_DIR, f'scaled_background_repeat{repeat+1}.joblib')\n",
    "                )\n",
    "\n",
    "            # Train model\n",
    "            model = create_simpler_model()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=30,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Save metrics\n",
    "            y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "            y_probs = model.predict(X_test)\n",
    "            save_evaluation_metrics(y_test, y_pred, y_probs, repeat, fold)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model.h5')\n",
    "                joblib.dump(scaler, f'{RESULTS_DIR}/best_scaler.joblib')\n",
    "\n",
    "                # Save the best background data (scaled)\n",
    "                best_background = scaler.transform(raw_background)\n",
    "                joblib.dump(\n",
    "                    best_background,\n",
    "                    f'{RESULTS_DIR}/best_scaled_background.joblib'\n",
    "                )\n",
    "\n",
    "            print(f\"Repeat {repeat+1}, Fold {fold+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Save final background data\n",
    "    final_background = save_background_data(X, y, n_samples=200, filename='final_background.joblib')\n",
    "\n",
    "    # Final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Range: [{np.min(all_accuracies):.4f}, {np.max(all_accuracies):.4f}]\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"{N_REPEATS}×{N_SPLITS}-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {np.max(all_accuracies):.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "        f.write(f\"\\nBackground Data Information:\\n\")\n",
    "        f.write(f\"- Raw background samples: {raw_background.shape[0]}\\n\")\n",
    "        f.write(f\"- Final background samples: {final_background.shape[0] if final_background is not None else 0}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e96e30abf85ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T05:48:04.035590Z",
     "start_time": "2025-04-27T04:19:48.129511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset...\n",
      "Found 3000 fake samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fake files: 100%|██████████| 3000/3000 [08:01<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 real samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Real files: 100%|██████████| 3000/3000 [07:26<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples loaded: 6000\n",
      "Class distribution: Real=3000, Fake=3000\n",
      "\n",
      "Saved background data (100 samples)\n",
      "\n",
      "Starting 5×10-fold cross-validation with larger model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1, Fold 1: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 1, Fold 2: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 3: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 4: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 5: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 1, Fold 7: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 8: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 1, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Repeat 1, Fold 10: Accuracy = 0.9967\n",
      "Repeat 1 complete. Mean accuracy: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 2, Fold 2: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 2, Fold 3: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 2, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 6: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 2, Fold 7: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 2, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 2, Fold 9: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 2, Fold 10: Accuracy = 0.9983\n",
      "Repeat 2 complete. Mean accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Repeat 3, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 3: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 3, Fold 4: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 3, Fold 6: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Repeat 3, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Repeat 3, Fold 8: Accuracy = 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Repeat 3, Fold 9: Accuracy = 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Repeat 3, Fold 10: Accuracy = 1.0000\n",
      "Repeat 3 complete. Mean accuracy: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Repeat 4, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Repeat 4, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Repeat 4, Fold 3: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Repeat 4, Fold 4: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 5: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 6: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 4, Fold 7: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 4, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 4, Fold 9: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 4, Fold 10: Accuracy = 0.9983\n",
      "Repeat 4 complete. Mean accuracy: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Repeat 5, Fold 1: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 2: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 3: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 4: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Repeat 5, Fold 5: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Repeat 5, Fold 6: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 7: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Repeat 5, Fold 8: Accuracy = 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Repeat 5, Fold 9: Accuracy = 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Repeat 5, Fold 10: Accuracy = 0.9967\n",
      "Repeat 5 complete. Mean accuracy: 0.9985\n",
      "\n",
      "Generating SHAP explanations for best model...\n",
      "\n",
      "Generating SHAP explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:94: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_600\n",
      "Received: inputs=['Tensor(shape=(100, 76))']\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_600\n",
      "Received: inputs=['Tensor(shape=(200, 76))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in SHAP explanation: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\n",
      "Final Results:\n",
      "Mean Accuracy: 0.9983 (±0.0016)\n",
      "Best Accuracy: 1.0000\n",
      "Worst Accuracy: 0.9933\n",
      "\n",
      "All results saved in 'model_results_large_with_shap' directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_large_with_shap\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76  # Number of audio features\n",
    "N_SPLITS = 10  # Number of folds for cross-validation\n",
    "N_REPEATS = 5  # Reduced repeats due to larger model\n",
    "BACKGROUND_SAMPLES = 100  # Number of samples for SHAP background\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# [Previous extract_features() and load_processed_dataset() functions remain the same]\n",
    "\n",
    "def create_large_model():\n",
    "    \"\"\"Create a larger neural network model with regularization\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(FEATURE_COUNT,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_shap_explanations(model, background_data, X_test, feature_names=None):\n",
    "    \"\"\"Generate and save SHAP explanations for model predictions\"\"\"\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP explanations...\")\n",
    "\n",
    "        # Create explainer\n",
    "        explainer = shap.DeepExplainer(model, background_data)\n",
    "\n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "        # Plot summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_DIR}/shap_summary.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save SHAP values\n",
    "        joblib.dump(shap_values, f'{RESULTS_DIR}/shap_values.joblib')\n",
    "        print(\"SHAP analysis completed and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP explanation: {str(e)}\")\n",
    "\n",
    "def save_background_data(X_scaled, y, n_samples=BACKGROUND_SAMPLES):\n",
    "    \"\"\"Save representative background data for SHAP analysis\"\"\"\n",
    "    try:\n",
    "        # Create stratified background data\n",
    "        if len(X_scaled) > n_samples * 2:\n",
    "            background, _ = train_test_split(\n",
    "                X_scaled,\n",
    "                train_size=n_samples,\n",
    "                stratify=y,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # If dataset is small, use all available data\n",
    "            background = X_scaled\n",
    "\n",
    "        joblib.dump(background, f'{RESULTS_DIR}/background_data.joblib')\n",
    "        print(f\"\\nSaved background data ({background.shape[0]} samples)\")\n",
    "\n",
    "        # Generate feature names for SHAP plots\n",
    "        feature_names = [\n",
    "            *[f\"MFCC_mean_{i}\" for i in range(20)],\n",
    "            *[f\"MFCC_std_{i}\" for i in range(20)],\n",
    "            \"Chroma_mean\", \"Chroma_std\",\n",
    "            \"SpectralCentroid_mean\", \"SpectralCentroid_std\",\n",
    "            \"SpectralBandwidth_mean\", \"SpectralBandwidth_std\",\n",
    "            \"SpectralRolloff_mean\", \"SpectralRolloff_std\",\n",
    "            \"ZCR_mean\", \"ZCR_std\",\n",
    "            \"RMS_mean\", \"RMS_std\",\n",
    "            *[f\"SpectralContrast_mean_{i}\" for i in range(6)],\n",
    "            *[f\"SpectralContrast_std_{i}\" for i in range(6)],\n",
    "            *[f\"Tonnetz_mean_{i}\" for i in range(6)],\n",
    "            *[f\"Tonnetz_std_{i}\" for i in range(6)]\n",
    "        ]\n",
    "\n",
    "        joblib.dump(feature_names, f'{RESULTS_DIR}/feature_names.joblib')\n",
    "\n",
    "        return background, feature_names\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving background data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save background data and scaler\n",
    "    background_data, feature_names = save_background_data(X_scaled, y)\n",
    "    joblib.dump(scaler, f'{RESULTS_DIR}/scaler.joblib')\n",
    "\n",
    "    # Cross-validation setup\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    best_X_test = None\n",
    "    best_y_test = None\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation with larger model...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X_scaled, y)):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train larger model\n",
    "            model = create_large_model()\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Save metrics\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "            save_evaluation_metrics(y_test, y_pred, y_probs, repeat, fold)\n",
    "\n",
    "            # Track best model and corresponding test data\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                best_X_test = X_test\n",
    "                best_y_test = y_test\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model.h5')\n",
    "\n",
    "            print(f\"Repeat {repeat+1}, Fold {fold+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Repeat statistics\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Generate SHAP explanations for best model\n",
    "    if best_model is not None and background_data is not None:\n",
    "        print(\"\\nGenerating SHAP explanations for best model...\")\n",
    "        generate_shap_explanations(best_model, background_data, best_X_test[:100], feature_names)\n",
    "\n",
    "        # Save example predictions with explanations\n",
    "        example_idx = np.random.choice(len(best_X_test), size=min(10, len(best_X_test)), replace=False)\n",
    "        example_data = {\n",
    "            'X': best_X_test[example_idx],\n",
    "            'y_true': best_y_test[example_idx],\n",
    "            'y_pred': best_model.predict(best_X_test[example_idx]).argmax(axis=1),\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "        joblib.dump(example_data, f'{RESULTS_DIR}/example_predictions.joblib')\n",
    "\n",
    "    # Final results\n",
    "    joblib.dump(all_accuracies, f\"{RESULTS_DIR}/all_accuracies.joblib\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "   ` with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"5×10-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f699b45de8fc484",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_corrected\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 5  # Changed from 10 to 5 folds\n",
    "N_REPEATS = 3  # Changed from 5 to 3 repeats for more reliable estimation\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load preprocessed audio features and labels\"\"\"\n",
    "    try:\n",
    "        features = []\n",
    "        labels = []\n",
    "        for label in [\"real\", \"fake\"]:\n",
    "            folder = os.path.join(PROCESSED_DIR, label)\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(\".npy\"):\n",
    "                    data = np.load(os.path.join(folder, file))\n",
    "                    features.append(data)\n",
    "                    labels.append(0 if label == \"real\" else 1)\n",
    "        return np.array(features), np.array(labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create neural network model with regularization\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(FEATURE_COUNT,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Initialize metrics\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42+repeat)\n",
    "        repeat_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            print(f\"\\nRepeat {repeat+1}, Fold {fold+1}:\")\n",
    "\n",
    "            # Scale data within each fold to prevent leakage\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X[train_idx])\n",
    "            X_test = scaler.transform(X[test_idx])\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train model\n",
    "            model = create_model()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[\n",
    "                    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "                    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            repeat_accuracies.append(accuracy)\n",
    "\n",
    "            # Print classification report\n",
    "            print(classification_report(y_test, y_pred))\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                save_model(model, f'{RESULTS_DIR}/best_model.h5')\n",
    "                joblib.dump(scaler, f'{RESULTS_DIR}/best_scaler.joblib')\n",
    "\n",
    "        # Repeat statistics\n",
    "        mean_acc = np.mean(repeat_accuracies)\n",
    "        all_accuracies.extend(repeat_accuracies)\n",
    "        print(f\"Repeat {repeat+1} complete. Mean accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    # Final results\n",
    "    joblib.dump(all_accuracies, f\"{RESULTS_DIR}/all_accuracies.joblib\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Cross Validation Results ({N_REPEATS}×{N_SPLITS}-fold)\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9954306f55a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                           confusion_matrix, classification_report)\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"processed_audio/\"\n",
    "RESULTS_DIR = \"result_robust\"  # Changed to your requested directory name\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 3\n",
    "BACKGROUND_SAMPLES = 100\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load preprocessed audio features and labels\"\"\"\n",
    "    try:\n",
    "        features = []\n",
    "        labels = []\n",
    "        for label in [\"real\", \"fake\"]:\n",
    "            folder = os.path.join(PROCESSED_DIR, label)\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(\".npy\"):\n",
    "                    data = np.load(os.path.join(folder, file))\n",
    "                    features.append(data)\n",
    "                    labels.append(0 if label == \"real\" else 1)\n",
    "        return np.array(features), np.array(labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "def create_robust_model(input_shape):\n",
    "    \"\"\"Create optimized model architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "\n",
    "def generate_shap_explanations(model, background, X_test, feature_names):\n",
    "    \"\"\"Generate SHAP feature importance analysis\"\"\"\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP explanations...\")\n",
    "        explainer = shap.DeepExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(X_test[:100])  # Use subset for efficiency\n",
    "\n",
    "        # Save summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test[:100], feature_names=feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_DIR}/shap_summary.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save raw SHAP values\n",
    "        joblib.dump({\n",
    "            'shap_values': shap_values,\n",
    "            'expected_value': explainer.expected_value,\n",
    "            'feature_names': feature_names,\n",
    "            'sample_indices': np.arange(100)  # Save which samples were used\n",
    "        }, f'{RESULTS_DIR}/shap_results.joblib')\n",
    "\n",
    "        print(f\"SHAP analysis saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f687e2b9e9baf365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:39:25.791472Z",
     "start_time": "2025-04-29T05:39:19.438679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\adity\\x-ai for music classification\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\x-ai for music classification\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc5f4ad3c3ffad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:44:19.377165Z",
     "start_time": "2025-04-29T12:44:19.338528Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: closing parenthesis '}' does not match opening parenthesis '(' (748076904.py, line 35)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"- Class distribution: {dict(zip(*np.unique(y, return_counts=True))}\")\u001b[39m\n                                                                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m f-string: closing parenthesis '}' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_full_verified\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 10\n",
    "N_REPEATS = 5\n",
    "BACKGROUND_SAMPLES = 100\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load preprocessed features and labels\"\"\"\n",
    "    try:\n",
    "        X = np.load(os.path.join(PROCESSED_DIR, \"features.npy\"))\n",
    "        y = np.load(os.path.join(PROCESSED_DIR, \"labels.npy\"))\n",
    "\n",
    "        print(f\"\\nDataset loaded successfully:\")\n",
    "        print(f\"- Total samples: {len(X)}\")\n",
    "        print(f\"- Features per sample: {X.shape[1]}\")\n",
    "        print(f\"- Class distribution: {dict(zip(*np.unique(y, return_counts=True))}\")\n",
    "\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading dataset: {str(e)}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create optimized neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_training_plots(history, repeat, fold):\n",
    "    \"\"\"Save training history plots\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Repeat {repeat} Fold {fold} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Repeat {repeat} Fold {fold} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/repeat_{repeat}_fold_{fold}_training.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, repeat, fold):\n",
    "    \"\"\"Save confusion matrix visualization\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Fake', 'Real'],\n",
    "               yticklabels=['Fake', 'Real'])\n",
    "    plt.title(f'Repeat {repeat} Fold {fold}\\nAccuracy: {np.sum(np.diag(cm))/np.sum(cm):.4f}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f\"{RESULTS_DIR}/repeat_{repeat}_fold_{fold}_cm.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_fold_results(repeat, fold, y_true, y_pred, y_probs, test_indices):\n",
    "    \"\"\"Save detailed fold results\"\"\"\n",
    "    fold_dir = f\"{RESULTS_DIR}/repeat_{repeat}/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    # Save indices and raw predictions\n",
    "    np.save(f\"{fold_dir}/test_indices.npy\", test_indices)\n",
    "    np.save(f\"{fold_dir}/y_true.npy\", y_true)\n",
    "    np.save(f\"{fold_dir}/y_pred.npy\", y_pred)\n",
    "    np.save(f\"{fold_dir}/y_probs.npy\", y_probs)\n",
    "\n",
    "    # Save classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Fake', 'Real'], output_dict=True)\n",
    "    joblib.dump(report, f\"{fold_dir}/classification_report.joblib\")\n",
    "\n",
    "    # Save ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{fold_dir}/roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_shap_explanations(model, X_train, X_test, feature_names):\n",
    "    \"\"\"Generate SHAP explanations for model predictions\"\"\"\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP explanations...\")\n",
    "\n",
    "        # Create explainer with background data\n",
    "        background = shap.sample(X_train, BACKGROUND_SAMPLES)\n",
    "        explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "        # Calculate SHAP values for test samples\n",
    "        test_samples = X_test[:50]  # Use first 50 test samples for efficiency\n",
    "        shap_values = explainer.shap_values(test_samples)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[1], test_samples, feature_names=feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_DIR}/shap_summary.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save SHAP values\n",
    "        joblib.dump(shap_values, f'{RESULTS_DIR}/shap_values.joblib')\n",
    "        print(\"SHAP analysis completed and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP explanation: {str(e)}\")\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\"Return descriptive feature names\"\"\"\n",
    "    return [\n",
    "        *[f\"MFCC_mean_{i}\" for i in range(20)],\n",
    "        *[f\"MFCC_std_{i}\" for i in range(20)],\n",
    "        \"Chroma_mean\", \"Chroma_std\",\n",
    "        \"SpectralCentroid_mean\", \"SpectralCentroid_std\",\n",
    "        \"SpectralBandwidth_mean\", \"SpectralBandwidth_std\",\n",
    "        \"SpectralRolloff_mean\", \"SpectralRolloff_std\",\n",
    "        \"ZCR_mean\", \"ZCR_std\",\n",
    "        \"RMS_mean\", \"RMS_std\",\n",
    "        *[f\"SpectralContrast_mean_{i}\" for i in range(6)],\n",
    "        *[f\"SpectralContrast_std_{i}\" for i in range(6)],\n",
    "        *[f\"Tonnetz_mean_{i}\" for i in range(6)],\n",
    "        *[f\"Tonnetz_std_{i}\" for i in range(6)]\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    # Load and verify dataset\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Initialize results tracking\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model_path = f\"{RESULTS_DIR}/best_model.h5\"\n",
    "    feature_names = get_feature_names()\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        print(f\"\\n\\n=== REPEAT {repeat+1}/{N_REPEATS} ===\")\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            print(f\"\\nFold {fold+1}/{N_SPLITS}\")\n",
    "            print(f\"Train samples: {len(train_idx)} (Fake: {sum(y[train_idx]==0)}, Real: {sum(y[train_idx]==1)})\")\n",
    "            print(f\"Test samples:  {len(test_idx)} (Fake: {sum(y[test_idx]==0)}, Real: {sum(y[test_idx]==1)})\")\n",
    "\n",
    "            # Scale data inside the fold\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X[train_idx])\n",
    "            X_test = scaler.transform(X[test_idx])\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train model\n",
    "            model = create_model()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[\n",
    "                    EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            all_accuracies.append(accuracy)\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "\n",
    "            # Save results\n",
    "            save_training_plots(history, repeat+1, fold+1)\n",
    "            save_confusion_matrix(y_test, y_pred, repeat+1, fold+1)\n",
    "            save_fold_results(repeat+1, fold+1, y_test, y_pred, y_probs, test_idx)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                save_model(model, best_model_path)\n",
    "                joblib.dump(scaler, f\"{RESULTS_DIR}/best_scaler.joblib\")\n",
    "                print(f\"New best model saved (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "            print(f\"Fold accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Mean accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"{N_REPEATS}×{N_SPLITS}-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    # Generate SHAP explanations for best model\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(\"\\nLoading best model for SHAP explanations...\")\n",
    "        best_model = load_model(best_model_path)\n",
    "        best_scaler = joblib.load(f\"{RESULTS_DIR}/best_scaler.joblib\")\n",
    "\n",
    "        # Get background data from all training folds\n",
    "        background_indices = []\n",
    "        for repeat in range(N_REPEATS):\n",
    "            for fold in range(N_SPLITS):\n",
    "                try:\n",
    "                    test_indices = np.load(f\"{RESULTS_DIR}/repeat_{repeat+1}/fold_{fold+1}/test_indices.npy\")\n",
    "                    background_indices.extend([i for i in range(len(X)) if i not in test_indices])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        background_data = best_scaler.transform(X[np.random.choice(background_indices,\n",
    "                                                min(1000, len(background_indices)), replace=False)])\n",
    "        test_data = best_scaler.transform(X[:50])  # Explain first 50 samples\n",
    "\n",
    "        generate_shap_explanations(best_model, background_data, test_data, feature_names)\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87b1c365cc8c110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:09:51.537350Z",
     "start_time": "2025-04-28T14:09:49.449676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# After training the model:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m explainer = shap.DeepExplainer(\u001b[43mmodel\u001b[49m, background_data[:\u001b[32m100\u001b[39m])  \u001b[38;5;66;03m# Sample of background data\u001b[39;00m\n\u001b[32m      5\u001b[39m shap_values = explainer.shap_values(background_data[:\u001b[32m100\u001b[39m])  \u001b[38;5;66;03m# Can use more data if needed\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save for later use\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# After training the model:\n",
    "explainer = shap.DeepExplainer(model, background_data[:100])  # Sample of background data\n",
    "shap_values = explainer.shap_values(background_data[:100])  # Can use more data if needed\n",
    "\n",
    "# Save for later use\n",
    "joblib.dump(shap_values, \"model_results_large_with_shap/shap_values.joblib\")\n",
    "joblib.dump(background_data, \"model_results_large_with_shap/background_data.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c400a13a5aa087ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:25:20.325555Z",
     "start_time": "2025-04-29T06:23:24.969401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20.0% of test data...\n",
      "\n",
      "Loaded 720 samples (360 real, 360 fake)\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Test Accuracy: 0.2167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.25      0.29      0.27       360\n",
      "        Real       0.16      0.14      0.15       360\n",
      "\n",
      "    accuracy                           0.22       720\n",
      "   macro avg       0.21      0.22      0.21       720\n",
      "weighted avg       0.21      0.22      0.21       720\n",
      "\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "File: real_1180.wav\n",
      "True: Real\n",
      "Pred: Fake\n",
      "Confidence: 1.0000\n",
      "Probabilities: Fake=1.0000, Real=0.0000\n",
      "\n",
      "File: real_1178.wav\n",
      "True: Real\n",
      "Pred: Fake\n",
      "Confidence: 1.0000\n",
      "Probabilities: Fake=1.0000, Real=0.0000\n",
      "\n",
      "File: fake_1065.wav\n",
      "True: Fake\n",
      "Pred: Real\n",
      "Confidence: 0.9987\n",
      "Probabilities: Fake=0.0013, Real=0.9987\n",
      "\n",
      "File: real_1072.wav\n",
      "True: Real\n",
      "Pred: Fake\n",
      "Confidence: 0.9999\n",
      "Probabilities: Fake=0.9999, Real=0.0001\n",
      "\n",
      "File: fake_1184.wav\n",
      "True: Fake\n",
      "Pred: Fake\n",
      "Confidence: 0.9998\n",
      "Probabilities: Fake=0.9998, Real=0.0002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Configuration\n",
    "TEST_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "MODEL_DIR = \"model_results_large_with_shap\"\n",
    "SR = 22050  # Sample rate\n",
    "TEST_SIZE =0.2 # 1.0 means 100% of data, adjust if needed\n",
    "RANDOM_STATE = 42\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Consistent labeling (0 = Fake, 1 = Real)\n",
    "LABELS = {0: \"Fake\", 1: \"Real\"}\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract exactly 76 features matching the training setup\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR, mono=True)\n",
    "\n",
    "        # Feature extraction\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=SR, n_bands=6)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "\n",
    "        # Construct features (ensure this matches your training exactly)\n",
    "        features = [\n",
    "            *np.mean(mfcc, axis=1), *np.std(mfcc, axis=1),\n",
    "            np.mean(chroma), np.std(chroma),\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
    "            np.mean(zcr), np.std(zcr),\n",
    "            np.mean(rms), np.std(rms),\n",
    "            *np.mean(spectral_contrast, axis=1)[:6],\n",
    "            *np.std(spectral_contrast, axis=1)[:6],\n",
    "            *np.mean(tonnetz, axis=1)[:6],\n",
    "            *np.std(tonnetz, axis=1)[:6]\n",
    "        ]\n",
    "\n",
    "        if len(features) != 76:\n",
    "            raise ValueError(f\"Expected 76 features, got {len(features)}\")\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_test_data(test_dir, test_size=1.0):\n",
    "    \"\"\"Load and process test data\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    file_paths = []\n",
    "\n",
    "    # Load Fake files (label 0)\n",
    "    fake_dir = os.path.join(test_dir, \"Fake\")\n",
    "    fake_files = [f for f in os.listdir(fake_dir) if f.endswith(\".wav\")]\n",
    "    if test_size < 1.0:\n",
    "        fake_files = sorted(fake_files)[:int(len(fake_files)*test_size)]\n",
    "\n",
    "    for filename in fake_files:\n",
    "        file_path = os.path.join(fake_dir, filename)\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(0)  # Fake = 0\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    # Load Real files (label 1)\n",
    "    real_dir = os.path.join(test_dir, \"Real\")\n",
    "    real_files = [f for f in os.listdir(real_dir) if f.endswith(\".wav\")]\n",
    "    if test_size < 1.0:\n",
    "        real_files = sorted(real_files)[:int(len(real_files)*test_size)]\n",
    "\n",
    "    for filename in real_files:\n",
    "        file_path = os.path.join(real_dir, filename)\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(1)  # Real = 1\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    return np.array(X), np.array(y), file_paths\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[LABELS[0], LABELS[1]],\n",
    "                yticklabels=[LABELS[0], LABELS[1]])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'{MODEL_DIR}/confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'{MODEL_DIR}/roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall(y_true, y_probs):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs[:, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, color='blue', lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.savefig(f'{MODEL_DIR}/precision_recall.png')\n",
    "    plt.close()\n",
    "\n",
    "def test_model():\n",
    "    print(f\"Loading {TEST_SIZE*100}% of test data...\")\n",
    "\n",
    "    # Load model and scaler\n",
    "    model = load_model(f\"{MODEL_DIR}/best_model.h5\")\n",
    "    scaler = joblib.load(f\"{MODEL_DIR}/scaler.joblib\")\n",
    "\n",
    "    # Load test data\n",
    "    X_test, y_test, file_paths = load_test_data(TEST_DIR, test_size=TEST_SIZE)\n",
    "\n",
    "    if X_test is None:\n",
    "        print(\"No valid test samples found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoaded {len(X_test)} samples ({len(y_test[y_test==1])} real, {len(y_test[y_test==0])} fake)\")\n",
    "\n",
    "    # Scale features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled).argmax(axis=1)\n",
    "    y_probs = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[LABELS[0], LABELS[1]]))\n",
    "\n",
    "    # Save plots and results\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "    plot_roc_curve(y_test, y_probs)\n",
    "    plot_precision_recall(y_test, y_probs)\n",
    "\n",
    "    results = {\n",
    "        'file_paths': file_paths,\n",
    "        'y_true': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_probs': y_probs,\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': classification_report(y_test, y_pred, target_names=[LABELS[0], LABELS[1]], output_dict=True)\n",
    "    }\n",
    "    joblib.dump(results, f\"{MODEL_DIR}/test_results.joblib\")\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    import pandas as pd\n",
    "    df_results = pd.DataFrame({\n",
    "        'file_path': file_paths,\n",
    "        'true_label': [LABELS[y] for y in y_test],\n",
    "        'pred_label': [LABELS[y] for y in y_pred],\n",
    "        'prob_fake': y_probs[:, 0],\n",
    "        'prob_real': y_probs[:, 1],\n",
    "        'correct': y_pred == y_test\n",
    "    })\n",
    "    df_results.to_csv(f\"{MODEL_DIR}/detailed_predictions.csv\", index=False)\n",
    "\n",
    "    print(\"\\nSample predictions:\")\n",
    "    sample_indices = np.random.choice(len(file_paths), size=min(5, len(file_paths)), replace=False)\n",
    "    for i in sample_indices:\n",
    "        print(f\"\\nFile: {os.path.basename(file_paths[i])}\")\n",
    "        print(f\"True: {LABELS[y_test[i]]}\")\n",
    "        print(f\"Pred: {LABELS[y_pred[i]]}\")\n",
    "        print(f\"Confidence: {y_probs[i][y_pred[i]]:.4f}\")\n",
    "        print(f\"Probabilities: Fake={y_probs[i][0]:.4f}, Real={y_probs[i][1]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12410fd7d5b6dc8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T05:55:01.056783Z",
     "start_time": "2025-04-29T05:53:41.385410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20.0% of test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 720 samples (360 real, 360 fake)\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "Test Accuracy: 0.7833\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real (0)       0.75      0.86      0.80       360\n",
      "    Fake (1)       0.84      0.71      0.77       360\n",
      "\n",
      "    accuracy                           0.78       720\n",
      "   macro avg       0.79      0.78      0.78       720\n",
      "weighted avg       0.79      0.78      0.78       720\n",
      "\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "File: fake_1077.wav\n",
      "True: Fake (1)\n",
      "Pred: Fake (1)\n",
      "Confidence: 1.0000\n",
      "\n",
      "File: fake_1262.wav\n",
      "True: Fake (1)\n",
      "Pred: Fake (1)\n",
      "Confidence: 0.9850\n",
      "\n",
      "File: fake_1170.wav\n",
      "True: Fake (1)\n",
      "Pred: Real (0)\n",
      "Confidence: 0.9996\n",
      "\n",
      "File: real_1144.wav\n",
      "True: Real (0)\n",
      "Pred: Fake (1)\n",
      "Confidence: 0.9832\n",
      "\n",
      "File: fake_1258.wav\n",
      "True: Fake (1)\n",
      "Pred: Fake (1)\n",
      "Confidence: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "TEST_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/Data/\"\n",
    "MODEL_DIR = \"model_results_large_with_shap\"\n",
    "SR = 22050\n",
    "TEST_SIZE = 0.20  # Load only 20% of data\n",
    "RANDOM_STATE = 42\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract exactly 76 features matching the training setup\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SR, mono=True)\n",
    "\n",
    "        # Feature extraction (same as training)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SR, n_mfcc=20)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SR)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=SR)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=SR)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=SR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=SR, n_bands=6)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=SR)\n",
    "\n",
    "        # Construct features\n",
    "        features = [\n",
    "            *np.mean(mfcc, axis=1), *np.std(mfcc, axis=1),\n",
    "            np.mean(chroma), np.std(chroma),\n",
    "            np.mean(spectral_centroid), np.std(spectral_centroid),\n",
    "            np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
    "            np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
    "            np.mean(zcr), np.std(zcr),\n",
    "            np.mean(rms), np.std(rms),\n",
    "            *np.mean(spectral_contrast, axis=1)[:6],\n",
    "            *np.std(spectral_contrast, axis=1)[:6],\n",
    "            *np.mean(tonnetz, axis=1)[:6],\n",
    "            *np.std(tonnetz, axis=1)[:6]\n",
    "        ]\n",
    "\n",
    "        if len(features) != 76:\n",
    "            raise ValueError(f\"Expected 76 features, got {len(features)}\")\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_test_data(test_dir, test_size=0.2):\n",
    "    \"\"\"Load and process test data, returning only a portion\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    file_paths = []\n",
    "\n",
    "    # Load Real files (label = 0)\n",
    "    real_dir = os.path.join(test_dir, \"Real\")\n",
    "    real_files = [f for f in os.listdir(real_dir) if f.endswith(\".wav\")]\n",
    "    real_files = sorted(real_files)[:int(len(real_files)*test_size)]  # Take portion\n",
    "\n",
    "    for filename in real_files:\n",
    "        file_path = os.path.join(real_dir, filename)\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(0)  # Real = 0 (matching training)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    # Load Fake files (label = 1)\n",
    "    fake_dir = os.path.join(test_dir, \"Fake\")\n",
    "    fake_files = [f for f in os.listdir(fake_dir) if f.endswith(\".wav\")]\n",
    "    fake_files = sorted(fake_files)[:int(len(fake_files)*test_size)]  # Take portion\n",
    "\n",
    "    for filename in fake_files:\n",
    "        file_path = os.path.join(fake_dir, filename)\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            X.append(features)\n",
    "            y.append(1)  # Fake = 1 (matching training)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    return np.array(X), np.array(y), file_paths\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real (0)', 'Fake (1)'],  # Updated to match training\n",
    "                yticklabels=['Real (0)', 'Fake (1)'])  # Updated to match training\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix (Real=0, Fake=1)')\n",
    "    plt.savefig(f'{MODEL_DIR}/confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def test_model():\n",
    "    print(f\"Loading {TEST_SIZE*100}% of test data...\")\n",
    "\n",
    "    # Load model and scaler\n",
    "    model = load_model(f\"{MODEL_DIR}/best_model.h5\")\n",
    "    scaler = joblib.load(f\"{MODEL_DIR}/scaler.joblib\")\n",
    "\n",
    "    # Load test data\n",
    "    X_test, y_test, file_paths = load_test_data(TEST_DIR, test_size=TEST_SIZE)\n",
    "\n",
    "    if X_test is None:\n",
    "        print(\"No valid test samples found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoaded {len(X_test)} samples ({len(y_test[y_test==0])} real, {len(y_test[y_test==1])} fake)\")\n",
    "\n",
    "    # Scale features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled).argmax(axis=1)\n",
    "    y_probs = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Real (0)\", \"Fake (1)\"]))\n",
    "\n",
    "    # Save results\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "    results = {\n",
    "        'file_paths': file_paths,\n",
    "        'y_true': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_probs': y_probs,\n",
    "        'accuracy': accuracy,\n",
    "        'test_size': TEST_SIZE\n",
    "    }\n",
    "    joblib.dump(results, f\"{MODEL_DIR}/test_results_20percent.joblib\")\n",
    "\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i in np.random.choice(len(file_paths), size=min(5, len(file_paths)), replace=False):\n",
    "        print(f\"\\nFile: {os.path.basename(file_paths[i])}\")\n",
    "        print(f\"True: {'Fake (1)' if y_test[i] else 'Real (0)'}\")\n",
    "        print(f\"Pred: {'Fake (1)' if y_pred[i] else 'Real (0)'}\")\n",
    "        print(f\"Confidence: {max(y_probs[i]):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390862afbe2e2163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:43:19.109992Z",
     "start_time": "2025-04-27T07:43:15.951129Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_100\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_100\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_300 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">39,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_200 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_301 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_302 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_202 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_303 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_203 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_304 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_300 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m39,424\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_200 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_301 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_201 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_302 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_202 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_303 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_203 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_304 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,876</span> (843.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m215,876\u001b[0m (843.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,954</span> (835.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m213,954\u001b[0m (835.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model_path = \"model_results_large_with_shap/best_model.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ac77663a45d2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:11:19.127194Z",
     "start_time": "2025-04-28T14:11:08.799966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mKey Model Details:\u001b[0m\n",
      "• Input shape: (76,)\n",
      "• Output shape: (2,)\n",
      "• Total params: 215,874\n",
      "• Trainable layers: 18\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"model_results_large_with_shap/best_model.h5\")\n",
    "\n",
    "# Generate architecture diagram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_model(model,\n",
    "           to_file='model_architecture.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           dpi=96,\n",
    "           rankdir='TB')  # TB = top to bottom layout\n",
    "plt.title('Model Architecture Summary', pad=20)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print concise summary\n",
    "print(\"\\033[1m\" + \"Key Model Details:\" + \"\\033[0m\")\n",
    "print(f\"• Input shape: {model.input_shape[1:]}\")\n",
    "print(f\"• Output shape: {model.output_shape[1:]}\")\n",
    "print(f\"• Total params: {model.count_params():,}\")\n",
    "print(f\"• Trainable layers: {len(model.trainable_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5b237031bd89b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T14:15:03.131372Z",
     "start_time": "2025-04-28T14:15:01.869578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m=== MODEL ARCHITECTURE ===\u001b[0m\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'model_arch.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1100\u001b[39m, in \u001b[36mImage._data_and_metadata\u001b[39m\u001b[34m(self, always_both)\u001b[39m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     b64_data = \u001b[43mb2a_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mTypeError\u001b[39m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1090\u001b[39m, in \u001b[36mImage._repr_mimebundle_\u001b[39m\u001b[34m(self, include, exclude)\u001b[39m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed:\n\u001b[32m   1089\u001b[39m     mimetype = \u001b[38;5;28mself\u001b[39m._mimetype\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     data, metadata = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43malways_both\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[32m   1092\u001b[39m         metadata = {mimetype: metadata}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1102\u001b[39m, in \u001b[36mImage._data_and_metadata\u001b[39m\u001b[34m(self, always_both)\u001b[39m\n\u001b[32m   1100\u001b[39m     b64_data = b2a_base64(\u001b[38;5;28mself\u001b[39m.data, newline=\u001b[38;5;28;01mFalse\u001b[39;00m).decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m.data)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1104\u001b[39m md = {}\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory: 'model_arch.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'model_arch.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1100\u001b[39m, in \u001b[36mImage._data_and_metadata\u001b[39m\u001b[34m(self, always_both)\u001b[39m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     b64_data = \u001b[43mb2a_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mTypeError\u001b[39m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\formatters.py:406\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    404\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1122\u001b[39m, in \u001b[36mImage._repr_png_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_png_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format == \u001b[38;5;28mself\u001b[39m._FMT_PNG:\n\u001b[32m-> \u001b[39m\u001b[32m1122\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:1102\u001b[39m, in \u001b[36mImage._data_and_metadata\u001b[39m\u001b[34m(self, always_both)\u001b[39m\n\u001b[32m   1100\u001b[39m     b64_data = b2a_base64(\u001b[38;5;28mself\u001b[39m.data, newline=\u001b[38;5;28;01mFalse\u001b[39;00m).decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m.data)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1104\u001b[39m md = {}\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory: 'model_arch.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m=== MODEL SUMMARY ===\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_100\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_100\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                </span>┃<span style=\"font-weight: bold\"> Output Shape               </span>┃<span style=\"font-weight: bold\">           Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_300 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">39,424</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_200 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_301 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_302 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_202 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_303 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_203 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_304 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────┴────────────────────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_300 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                │            \u001b[38;5;34m39,424\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                │             \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_200 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                │                 \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_301 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                │           \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                │             \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_201 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                │                 \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_302 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                │            \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                │               \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_202 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                │                 \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_303 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 │             \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ batch_normalization_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 │               \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)        │                            │                   │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dropout_203 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 │                 \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────┼────────────────────────────┼───────────────────┤\n",
       "│ dense_304 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                  │               \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────┴────────────────────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">215,876</span> (843.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m215,876\u001b[0m (843.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,954</span> (835.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m213,954\u001b[0m (835.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m=== KEY STATS ===\u001b[0m\n",
      "• Input Shape: (76,)\n",
      "• Output Shape: (2,)\n",
      "• Total Parameters: 215,874\n",
      "• Trainable Parameters: 213,954\n",
      "• Non-Trainable Parameters: 1,920\n",
      "\n",
      "\u001b[1m=== LAYER TYPES ===\u001b[0m\n",
      "1. Dense\n",
      "2. Dropout\n",
      "3. BatchNormalization\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display  # For Jupyter notebooks\n",
    "import numpy as np\n",
    "\n",
    "# Load your model\n",
    "model = load_model(\"model_results_large_with_shap/best_model.h5\")\n",
    "\n",
    "# 1. Generate and display visual architecture\n",
    "print(\"\\n\\033[1m=== MODEL ARCHITECTURE ===\\033[0m\")\n",
    "plot_model(model,\n",
    "           to_file='model_arch.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           show_layer_activations=True,\n",
    "           dpi=70,\n",
    "           rankdir='TB')  # Top-to-bottom layout\n",
    "\n",
    "try:\n",
    "    display(Image('model_arch.png', width=600))  # For Jupyter\n",
    "except:\n",
    "    img = plt.imread('model_arch.png')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Print enhanced text summary\n",
    "print(\"\\n\\033[1m=== MODEL SUMMARY ===\\033[0m\")\n",
    "model.summary(line_length=80, positions=[0.3, 0.6, 0.8, 1.0])\n",
    "\n",
    "# 3. Key metrics extraction\n",
    "print(\"\\n\\033[1m=== KEY STATS ===\\033[0m\")\n",
    "print(f\"• Input Shape: {model.input_shape[1:]}\")\n",
    "print(f\"• Output Shape: {model.output_shape[1:]}\")\n",
    "print(f\"• Total Parameters: {model.count_params():,}\")\n",
    "print(f\"• Trainable Parameters: {sum([np.prod(w.shape) for w in model.trainable_weights]):,}\")\n",
    "print(f\"• Non-Trainable Parameters: {sum([np.prod(w.shape) for w in model.non_trainable_weights]):,}\")\n",
    "\n",
    "# 4. Layer-type breakdown\n",
    "print(\"\\n\\033[1m=== LAYER TYPES ===\\033[0m\")\n",
    "layer_types = set([layer.__class__.__name__ for layer in model.layers])\n",
    "for i, lt in enumerate(layer_types, 1):\n",
    "    print(f\"{i}. {lt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a71dc0e8b7b8086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T16:03:07.653885Z",
     "start_time": "2025-04-27T16:03:00.070736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:94: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: input_layer_100\n",
      "Received: inputs=['Tensor(shape=(100, 76))']\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\adity\\X-AI for music classification\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: input_layer_100\n",
      "Received: inputs=['Tensor(shape=(200, 76))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values saved successfully! Note: Explainers must be recreated at runtime\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your pre-existing files\n",
    "model = load_model(\"model_results_large_with_shap/best_model.h5\")\n",
    "background_data = joblib.load(\"model_results_large_with_shap/background_data.joblib\")\n",
    "scaler = joblib.load(\"model_results_large_with_shap/scaler.joblib\")\n",
    "\n",
    "# Scale the background data\n",
    "background_data_scaled = scaler.transform(background_data)\n",
    "\n",
    "# Create and compute SHAP values (but don't try to save the explainer)\n",
    "explainer = shap.DeepExplainer(\n",
    "    model,\n",
    "    background_data_scaled[:100]  # Using first 100 samples as background\n",
    ")\n",
    "\n",
    "# Calculate SHAP values for the same background samples\n",
    "shap_values = explainer.shap_values(background_data_scaled[:100])\n",
    "\n",
    "# Save just the SHAP values and background data\n",
    "joblib.dump(shap_values, \"model_results_large_with_shap/shap_values.joblib\")\n",
    "\n",
    "# For your app, you'll need to recreate the explainer each time\n",
    "print(\"SHAP values saved successfully! Note: Explainers must be recreated at runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40ec014e1685a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0ccf8fce55408f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bc948a1d071722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T12:58:22.012002Z",
     "start_time": "2025-04-29T12:58:20.328632Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_curve, auc\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, save_model, load_model\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, BatchNormalization\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msix\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:99\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mThis module contains:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m \u001b[33;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils_exp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompress\u001b[39m(element):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m internal\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\__init__.py:28\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Ragged Tensors.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mThis package defines ops for manipulating ragged tensors (`tf.RaggedTensor`),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03mAPI docstring: tensorflow.ragged\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mragged\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:2320\u001b[39m\n\u001b[32m   2313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n\u001b[32m   2316\u001b[39m \u001b[38;5;66;03m# ===============================================================================\u001b[39;00m\n\u001b[32m   2317\u001b[39m \u001b[38;5;66;03m# RaggedTensorSpec\u001b[39;00m\n\u001b[32m   2318\u001b[39m \u001b[38;5;66;03m# ===============================================================================\u001b[39;00m\n\u001b[32m   2319\u001b[39m \u001b[38;5;129;43m@tf_export\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRaggedTensorSpec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m \u001b[38;5;129;43m@type_spec_registry\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtf.RaggedTensorSpec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2321\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mRaggedTensorSpec\u001b[39;49;00m\u001b[43m(\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBatchableTypeSpec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_types\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRaggedTensorSpec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2323\u001b[39m \u001b[38;5;250;43m  \u001b[39;49m\u001b[33;43;03m\"\"\"Type specification for a `tf.RaggedTensor`.\"\"\"\u001b[39;49;00m\n\u001b[32m   2325\u001b[39m \u001b[43m  \u001b[49m\u001b[34;43m__slots__\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_shape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_ragged_rank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_row_splits_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_flat_values_spec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2328\u001b[39m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\X-AI for music classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\type_spec_registry.py:59\u001b[39m, in \u001b[36mregister.<locals>.decorator_fn\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mClass \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m has already been registered with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m %\n\u001b[32m     57\u001b[39m                    (\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m, \u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m, _TYPE_SPEC_TO_NAME[\u001b[38;5;28mcls\u001b[39m]))\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _NAME_TO_TYPE_SPEC:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mName \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m has already been registered for class \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m %\n\u001b[32m     60\u001b[39m                    (name, _NAME_TO_TYPE_SPEC[name].\u001b[34m__module__\u001b[39m,\n\u001b[32m     61\u001b[39m                     _NAME_TO_TYPE_SPEC[name].\u001b[34m__name__\u001b[39m))\n\u001b[32m     62\u001b[39m _TYPE_SPEC_TO_NAME[\u001b[38;5;28mcls\u001b[39m] = name\n\u001b[32m     63\u001b[39m _NAME_TO_TYPE_SPEC[name] = \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"c:/Users/adity/Downloads/X_AI_for_fake_real_audio_detection/processed_audio/\"\n",
    "RESULTS_DIR = \"model_results_full_verified\"\n",
    "SR = 22050\n",
    "FEATURE_COUNT = 76\n",
    "N_SPLITS = 10\n",
    "N_REPEATS = 5\n",
    "BACKGROUND_SAMPLES = 100\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_processed_dataset():\n",
    "    \"\"\"Load preprocessed features and labels\"\"\"\n",
    "    try:\n",
    "        X = np.load(os.path.join(PROCESSED_DIR, \"features.npy\"))\n",
    "        y = np.load(os.path.join(PROCESSED_DIR, \"labels.npy\"))\n",
    "\n",
    "        print(f\"\\nDataset loaded successfully:\")\n",
    "        print(f\"- Total samples: {len(X)}\")\n",
    "        print(f\"- Features per sample: {X.shape[1]}\")\n",
    "        print(f\"- Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading dataset: {str(e)}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create optimized neural network model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(FEATURE_COUNT,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_training_plots(history, repeat, fold):\n",
    "    \"\"\"Save training history plots\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Repeat {repeat} Fold {fold} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Repeat {repeat} Fold {fold} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/repeat_{repeat}_fold_{fold}_training.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, repeat, fold):\n",
    "    \"\"\"Save confusion matrix visualization\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Fake', 'Real'],\n",
    "               yticklabels=['Fake', 'Real'])\n",
    "    plt.title(f'Repeat {repeat} Fold {fold}\\nAccuracy: {np.sum(np.diag(cm))/np.sum(cm):.4f}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f\"{RESULTS_DIR}/repeat_{repeat}_fold_{fold}_cm.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_fold_results(repeat, fold, y_true, y_pred, y_probs, test_indices):\n",
    "    \"\"\"Save detailed fold results\"\"\"\n",
    "    fold_dir = f\"{RESULTS_DIR}/repeat_{repeat}/fold_{fold}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    # Save indices and raw predictions\n",
    "    np.save(f\"{fold_dir}/test_indices.npy\", test_indices)\n",
    "    np.save(f\"{fold_dir}/y_true.npy\", y_true)\n",
    "    np.save(f\"{fold_dir}/y_pred.npy\", y_pred)\n",
    "    np.save(f\"{fold_dir}/y_probs.npy\", y_probs)\n",
    "\n",
    "    # Save classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Fake', 'Real'], output_dict=True)\n",
    "    joblib.dump(report, f\"{fold_dir}/classification_report.joblib\")\n",
    "\n",
    "    # Save ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{fold_dir}/roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_shap_explanations(model, X_train, X_test, feature_names):\n",
    "    \"\"\"Generate SHAP explanations for model predictions\"\"\"\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP explanations...\")\n",
    "\n",
    "        # Create explainer with background data\n",
    "        background = shap.sample(X_train, BACKGROUND_SAMPLES)\n",
    "        explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "        # Calculate SHAP values for test samples\n",
    "        test_samples = X_test[:50]  # Use first 50 test samples for efficiency\n",
    "        shap_values = explainer.shap_values(test_samples)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[1], test_samples, feature_names=feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_DIR}/shap_summary.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Save SHAP values\n",
    "        joblib.dump(shap_values, f'{RESULTS_DIR}/shap_values.joblib')\n",
    "        print(\"SHAP analysis completed and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP explanation: {str(e)}\")\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\"Return descriptive feature names\"\"\"\n",
    "    return [\n",
    "        *[f\"MFCC_mean_{i}\" for i in range(20)],\n",
    "        *[f\"MFCC_std_{i}\" for i in range(20)],\n",
    "        \"Chroma_mean\", \"Chroma_std\",\n",
    "        \"SpectralCentroid_mean\", \"SpectralCentroid_std\",\n",
    "        \"SpectralBandwidth_mean\", \"SpectralBandwidth_std\",\n",
    "        \"SpectralRolloff_mean\", \"SpectralRolloff_std\",\n",
    "        \"ZCR_mean\", \"ZCR_std\",\n",
    "        \"RMS_mean\", \"RMS_std\",\n",
    "        *[f\"SpectralContrast_mean_{i}\" for i in range(6)],\n",
    "        *[f\"SpectralContrast_std_{i}\" for i in range(6)],\n",
    "        *[f\"Tonnetz_mean_{i}\" for i in range(6)],\n",
    "        *[f\"Tonnetz_std_{i}\" for i in range(6)]\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    # Load and verify dataset\n",
    "    X, y = load_processed_dataset()\n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Initialize results tracking\n",
    "    all_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_model_path = f\"{RESULTS_DIR}/best_model.h5\"\n",
    "    feature_names = get_feature_names()\n",
    "\n",
    "    print(f\"\\nStarting {N_REPEATS}×{N_SPLITS}-fold cross-validation...\")\n",
    "\n",
    "    for repeat in range(N_REPEATS):\n",
    "        print(f\"\\n\\n=== REPEAT {repeat+1}/{N_REPEATS} ===\")\n",
    "        kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=repeat)\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "            print(f\"\\nFold {fold+1}/{N_SPLITS}\")\n",
    "            print(f\"Train samples: {len(train_idx)} (Fake: {sum(y[train_idx]==0)}, Real: {sum(y[train_idx]==1)})\")\n",
    "            print(f\"Test samples:  {len(test_idx)} (Fake: {sum(y[test_idx]==0)}, Real: {sum(y[test_idx]==1)})\")\n",
    "\n",
    "            # Scale data inside the fold\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X[train_idx])\n",
    "            X_test = scaler.transform(X[test_idx])\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Train model\n",
    "            model = create_model()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[\n",
    "                    EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            all_accuracies.append(accuracy)\n",
    "            y_pred = model.predict(X_test).argmax(axis=1)\n",
    "            y_probs = model.predict(X_test)\n",
    "\n",
    "            # Save results\n",
    "            save_training_plots(history, repeat+1, fold+1)\n",
    "            save_confusion_matrix(y_test, y_pred, repeat+1, fold+1)\n",
    "            save_fold_results(repeat+1, fold+1, y_test, y_pred, y_probs, test_idx)\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                save_model(model, best_model_path)\n",
    "                joblib.dump(scaler, f\"{RESULTS_DIR}/best_scaler.joblib\")\n",
    "                print(f\"New best model saved (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "            print(f\"Fold accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Mean accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\")\n",
    "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Worst accuracy: {np.min(all_accuracies):.4f}\")\n",
    "\n",
    "    # Save comprehensive report\n",
    "    with open(f\"{RESULTS_DIR}/final_report.txt\", \"w\") as f:\n",
    "        f.write(f\"{N_REPEATS}×{N_SPLITS}-Fold Cross Validation Results\\n\")\n",
    "        f.write(f\"Mean Accuracy: {np.mean(all_accuracies):.4f} (±{np.std(all_accuracies):.4f})\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Worst Accuracy: {np.min(all_accuracies):.4f}\\n\")\n",
    "\n",
    "    # Generate SHAP explanations for best model\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(\"\\nLoading best model for SHAP explanations...\")\n",
    "        best_model = load_model(best_model_path)\n",
    "        best_scaler = joblib.load(f\"{RESULTS_DIR}/best_scaler.joblib\")\n",
    "\n",
    "        # Get background data from all training folds\n",
    "        background_indices = []\n",
    "        for repeat in range(N_REPEATS):\n",
    "            for fold in range(N_SPLITS):\n",
    "                try:\n",
    "                    test_indices = np.load(f\"{RESULTS_DIR}/repeat_{repeat+1}/fold_{fold+1}/test_indices.npy\")\n",
    "                    background_indices.extend([i for i in range(len(X)) if i not in test_indices])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        background_data = best_scaler.transform(X[np.random.choice(background_indices,\n",
    "                                                min(1000, len(background_indices)), replace=False)])\n",
    "        test_data = best_scaler.transform(X[:50])  # Explain first 50 samples\n",
    "\n",
    "        generate_shap_explanations(best_model, background_data, test_data, feature_names)\n",
    "\n",
    "    print(f\"\\nAll results saved in '{RESULTS_DIR}' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cda0912447510b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:50:34.198800Z",
     "start_time": "2025-04-26T09:50:34.048351Z"
    }
   },
   "outputs": [],
   "source": [
    "import  tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b990a407aa994730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:48:47.547436Z",
     "start_time": "2025-04-26T09:48:47.539365Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
